<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Big Data</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Big Data</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="chapter-1"><a class="header" href="#chapter-1">Chapter 1</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hadoop"><a class="header" href="#hadoop">Hadoop</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs"><a class="header" href="#hdfs">HDFS</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs-概述"><a class="header" href="#hdfs-概述">HDFS 概述</a></h1>
<ul>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#hdfs-%E4%BA%A7%E5%87%BA%E8%83%8C%E6%99%AF%E5%8F%8A%E5%AE%9A%E4%B9%89">HDFS 产出背景及定义</a>
<ul>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#1-hdfs-%E4%BA%A7%E7%94%9F%E8%83%8C%E6%99%AF">1 ）HDFS 产生背景</a></li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#2-hdfs-%E5%AE%9A%E4%B9%89">2 ）HDFS 定义</a></li>
</ul>
</li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#hdfs-%E4%BC%98%E7%BC%BA%E7%82%B9">HDFS 优缺点</a></li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#hdfs-%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84">HDFS 组成架构</a></li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#hdfs-%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F-%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9">HDFS 文件块大小 （面试重点）</a></li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#namenode-%E5%92%8C-secondarynamenode">NameNode 和 SecondaryNameNode</a>
<ul>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#nn-%E5%92%8C-2nn-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">NN 和 2NN 工作机制</a></li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#namenode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">NameNode工作机制</a></li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#fsimage-%E5%92%8C-edits-%E8%A7%A3%E6%9E%90">Fsimage 和 Edits 解析</a></li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#checkpoint-%E6%97%B6%E9%97%B4%E8%AE%BE%E7%BD%AE">CheckPoint 时间设置</a></li>
</ul>
</li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#datanode">DataNode</a>
<ul>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#datanode-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">DataNode 工作机制</a></li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7">数据完整性</a></li>
<li><a href="HDFS%E6%A6%82%E8%BF%B0.html#datanode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE">DataNode掉线时限参数设置</a></li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027142704.png" alt="" /></p>
<h2 id="hdfs-产出背景及定义"><a class="header" href="#hdfs-产出背景及定义">HDFS 产出背景及定义</a></h2>
<h3 id="1-hdfs-产生背景"><a class="header" href="#1-hdfs-产生背景">1 ）HDFS 产生背景</a></h3>
<p>随着数据量越来越大， 在一个操作系统存不下所有的数据， 那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS 只是分布式文件管理系统中的一种。</p>
<h3 id="2-hdfs-定义"><a class="header" href="#2-hdfs-定义">2 ）HDFS 定义</a></h3>
<p>HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>
<p>HDFS 的使用场景：<strong>适合一次写入，多次读出的场景。 一个文件经过创建、写入和关闭之后就不需要改变</strong>。</p>
<h2 id="hdfs-优缺点"><a class="header" href="#hdfs-优缺点">HDFS 优缺点</a></h2>
<p>HDFS优点</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027144347.png" alt="" /></p>
<p>HDFS缺点</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027144402.png" alt="" /></p>
<h2 id="hdfs-组成架构"><a class="header" href="#hdfs-组成架构">HDFS 组成架构</a></h2>
<p>HDFS组成架构示意图</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027145753.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027145821.png" alt="" /></p>
<p>简单形容：NameNode是老板，DataNode是打工仔，Client是客户，Secondary NameNode是秘书</p>
<h2 id="hdfs-文件块大小-面试重点"><a class="header" href="#hdfs-文件块大小-面试重点">HDFS 文件块大小 （面试重点）</a></h2>
<p>HDFS 文件块大小</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027145939.png" alt="" /></p>
<p>思考：为什么块的大小不能设置太小，也不能设置太大？</p>
<p>（1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；
（2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</p>
<p>总结：<strong>HDFS块的大小设置主要取决于磁盘传输速率</strong>。</p>
<p>e.g.</p>
<p>普通的机械硬盘，传输速率100M/s →HDFS块设置为128M</p>
<p>固态硬盘，传输速率200-300M/s →HDFS块设置为256M</p>
<h2 id="namenode-和-secondarynamenode"><a class="header" href="#namenode-和-secondarynamenode">NameNode 和 SecondaryNameNode</a></h2>
<h3 id="nn-和-2nn-工作机制"><a class="header" href="#nn-和-2nn-工作机制">NN 和 2NN 工作机制</a></h3>
<p>思考：NameNode 中的元数据是存储在哪里的？</p>
<p>首先，我们做个假设，如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。</p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数据丢失。因此，引入 Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits 中。这样，一旦 NameNode 节点断电，可以通过 FsImage 和 Edits 的合并，合成元数据。 但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这个操作由NameNode节点完成， 又会效率过低。 因此， <strong>引入一个新的节点SecondaryNamenode，专门用于 FsImage 和 Edits 的合并。</strong></p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/d99676a51208185f6f4c380d6f1e9354.png" alt="a" /></p>
<h3 id="namenode工作机制"><a class="header" href="#namenode工作机制">NameNode工作机制</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102132550.png" alt="" /></p>
<p>1 ） 第一阶段：NameNode 启动</p>
<p>（1）第一次启动 NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</p>
<p>（2）客户端对元数据进行增删改的请求。</p>
<p>（3）NameNode 记录操作日志，更新滚动日志。</p>
<p>（4）NameNode 在内存中对元数据进行增删改。</p>
<p>2 ） 第二阶段：Secondary NameNode 工作</p>
<p>（1）Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode是否检查结果。</p>
<p>（2）Secondary NameNode 请求执行 CheckPoint。</p>
<p>（3）NameNode 滚动正在写的 Edits 日志。</p>
<p>（4）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。</p>
<p>（5）Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。</p>
<p>（6）生成新的镜像文件 fsimage.chkpoint。</p>
<p>（7）拷贝 fsimage.chkpoint 到 NameNode。</p>
<p>（8）NameNode 将 fsimage.chkpoint 重新命名成 fsimage。</p>
<h3 id="fsimage-和-edits-解析"><a class="header" href="#fsimage-和-edits-解析">Fsimage 和 Edits 解析</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102132707.png" alt="" /></p>
<p>查看 oiv 和 oev 命令说明</p>
<pre><code>oiv            apply the offline fsimage viewer to an fsimage 
oev            apply the offline edits viewer to an edits file 
</code></pre>
<p>1 ）oiv 查看 Fsimage 文件</p>
<p>（1）基本语法</p>
<pre><code>hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径
</code></pre>
<p>（2）案例实操</p>
<pre><code>hdfs oiv -p XML -i fsimage_0000000000000000261 -o /opt/module/hadoop-3.1.3/fsimage.xml
</code></pre>
<p>思考：Fsimage 中没有记录块所对应 DataNode，为什么？</p>
<p>在集群启动后，要求 DataNode 上报数据块信息，并间隔一段时间后再次上报。</p>
<p>2 ）oev 查看 Edits 文件</p>
<p>（1）基本语法</p>
<pre><code>hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径
</code></pre>
<p>（2）案例实操</p>
<pre><code>hdfs oev -p XML -i edits_inprogress_0000000000000000262 -o/opt/module/hadoop-3.1.3/edits.xml
</code></pre>
<p>思考：NameNode 如何确定下次开机启动的时候合并哪些 Edits？</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102133225.png" alt="" /></p>
<p>注意时间，看到每间隔1h进行一次合并</p>
<p>集群一停止、开关机也要合并一次</p>
<h3 id="checkpoint-时间设置"><a class="header" href="#checkpoint-时间设置">CheckPoint 时间设置</a></h3>
<p>1 ） 通常情况下，SecondaryNameNode 每隔一小时执行一次</p>
<p>参照：hdfs-default.xml</p>
<pre><code class="language-xml">&lt;property&gt; 
  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; 
  &lt;value&gt;3600s&lt;/value&gt; 
&lt;/property&gt; 
</code></pre>
<p>2 ） 一分钟检查一次操作次数，当操作次数达到 1 百万时，SecondaryNameNode执行一次</p>
<pre><code class="language-xml">&lt;property&gt; 
  &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; 
  &lt;value&gt;1000000&lt;/value&gt; 
&lt;description&gt;操作动作次数&lt;/description&gt; 
&lt;/property&gt; 
 
&lt;property&gt; 
  &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; 
  &lt;value&gt;60s&lt;/value&gt; 
&lt;description&gt; 1 分钟检查一次操作次数&lt;/description&gt; 
&lt;/property&gt; 
</code></pre>
<h2 id="datanode"><a class="header" href="#datanode">DataNode</a></h2>
<h3 id="datanode-工作机制"><a class="header" href="#datanode-工作机制">DataNode 工作机制</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102130953.png" alt="" /></p>
<p>（1）一个数据块在 DataNode 上以文件形式存储在磁盘上，包括两个文件：一个是数据本身，一个是元数据包括数据块的长度、块数据的校验和以及时间戳。</p>
<p>（2）DataNode 启动后向 NameNode 注册，通过后，周期性（6 小时）的向 NameNode 上报所有的块信息。</p>
<p>DN 向 NN 汇报当前解读信息的时间间隔，默认 6 小时。</p>
<p>相关配置参数如下：</p>
<pre><code class="language-xml">&lt;property&gt; 
  &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; 
  &lt;value&gt;21600000&lt;/value&gt; 
  &lt;description&gt;Determines block reporting interval in 
milliseconds.&lt;/description&gt; 
&lt;/property&gt; 
</code></pre>
<p>注意：这里value值单位是毫秒</p>
<p>DN 扫描自己节点块信息列表的时间，默认 6 小时</p>
<p>相关配置参数如下：</p>
<pre><code class="language-xml">&lt;property&gt; 
  &lt;name&gt;dfs.datanode.directoryscan.interval&lt;/name&gt; 
  &lt;value&gt;21600s&lt;/value&gt; 
  &lt;description&gt;Interval in seconds for Datanode to scan data 
  directories and reconcile the difference between blocks in memory and on 
the disk. 
  Support multiple time unit suffix(case insensitive), as described 
  in dfs.heartbeat.interval. 
  &lt;/description&gt; 
&lt;/property&gt; 
</code></pre>
<p>注意：这里value值单位是秒</p>
<p>（3）心跳是每 3 秒一次，心跳返回结果带有 NameNode 给该 DataNode 的命令如复制块数据到另一台机器 或删除某个数据块。 如果超过 10 分钟+30 秒没有收到某个 DataNode 的心跳，则认为该节点不可用（挂了），不会再向其传输信息。</p>
<p>（4）集群运行中可以安全加入和删除一些机器。</p>
<h3 id="数据完整性"><a class="header" href="#数据完整性">数据完整性</a></h3>
<p>思考：DataNode 节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？</p>
<p>如下是 DataNode 节点保证数据完整性的方法。</p>
<p>（1）当 DataNode 读取 Block 的时候，它会计算 CheckSum。</p>
<p>（2）如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。</p>
<p>（3）Client 读取其他 DataNode 上的 Block。</p>
<p>（4）常见的校验算法 crc（32），md5（128），sha1（160）</p>
<p>（5）DataNode 在其文件创建后周期验证 CheckSum。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102131756.png" alt="" /></p>
<h3 id="datanode掉线时限参数设置"><a class="header" href="#datanode掉线时限参数设置">DataNode掉线时限参数设置</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102131954.png" alt="" /></p>
<p>相关配置参数如下：</p>
<pre><code class="language-xml">&lt;property&gt; 
    &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; 
    &lt;value&gt;300000&lt;/value&gt; 
&lt;/property&gt; 
 
&lt;property&gt; 
    &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; 
    &lt;value&gt;3&lt;/value&gt; 
&lt;/property&gt; 
</code></pre>
<p>需要注意的是 hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval 的单位为秒。</p>
<p>DataNode 被中止之后，可以执行<code>hdfs --daemon start datanode</code>重启</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs读写流程"><a class="header" href="#hdfs读写流程">HDFS读写流程</a></h1>
<ul>
<li><a href="HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E5%86%99%E6%95%B0%E6%8D%AE">写数据</a>
<ul>
<li><a href="HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E6%96%87%E4%BB%B6%E5%86%99%E5%85%A5">文件写入</a></li>
<li><a href="HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97">网络拓扑-节点距离计算</a></li>
<li><a href="HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5%E5%89%AF%E6%9C%AC%E5%AD%98%E5%82%A8%E8%8A%82%E7%82%B9%E9%80%89%E6%8B%A9">机架感知（副本存储节点选择）</a></li>
</ul>
</li>
<li><a href="HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B">读数据流程</a></li>
</ul>
<h2 id="写数据"><a class="header" href="#写数据">写数据</a></h2>
<h3 id="文件写入"><a class="header" href="#文件写入">文件写入</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102124812.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102124951.png" alt="" /></p>
<p>（1）客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。</p>
<p>（2）NameNode 返回是否可以上传。</p>
<p>（3）客户端请求第一个 Block 上传到哪几个 DataNode 服务器上。</p>
<p>（4）NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。</p>
<p>（5） 客户端通过 FSDataOutputStream 模块请求 dn1 上传数据， dn1 收到请求会继续调用
dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。</p>
<p>（6）dn1、dn2、dn3 逐级应答客户端。</p>
<p>（7） 客户端开始往 dn1 上传第一个 Block （先从磁盘读取数据放到一个本地内存缓存） ，
以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个 packet
会放入一个应答队列等待应答。</p>
<p>（8）当一个 Block 传输完成之后， 客户端再次请求 NameNode 上传第二个 Block 的服务
器。（重复执行 3-7 步）。</p>
<h3 id="网络拓扑-节点距离计算"><a class="header" href="#网络拓扑-节点距离计算">网络拓扑-节点距离计算</a></h3>
<p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接
收数据。那么这个最近距离怎么计算呢？</p>
<p><strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p>
<p>例如，假设有数据中心 d1 机架 r1 中的节点 n1。该节点可以表示为/d1/r1/n1。利用这种
标记，这里给出四种距离描述。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102125235.png" alt="" /></p>
<h3 id="机架感知副本存储节点选择"><a class="header" href="#机架感知副本存储节点选择">机架感知（副本存储节点选择）</a></h3>
<p>1 ）机架感知说明</p>
<p>（1）官方说明</p>
<p>http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication</p>
<blockquote>
<p>For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on the local machine if the writer is on a datanode, otherwise on a random datanode, another replica on a node in a different (remote) rack, and the last on a different node in the same remote rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance.</p>
</blockquote>
<p>（2）源码说明</p>
<p>Crtl + n 查找 BlockPlacementPolicyDefault，在该类中查找 chooseTargetInOrder 方法。</p>
<p>2 ）Hadoop3.1.3 副本节点选择</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102125546.png" alt="" /></p>
<p>第一个选择最近的节点</p>
<p>第二个节点跨机架保证副本的可靠性</p>
<p>第三个节点还是兼顾效率</p>
<h2 id="读数据流程"><a class="header" href="#读数据流程">读数据流程</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102125848.png" alt="" /></p>
<p>（1）客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。</p>
<p>（2）挑选一台 DataNode服务器，请求读取数据。挑选过程遵循就近原则，然后随机（会考虑当前节点的负载能力）。</p>
<p>（3）DataNode 开始传输数据给客户端，从磁盘里面读取数据输入流，以 Packet 为单位来做校验。</p>
<p>（4）客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs的shell操作"><a class="header" href="#hdfs的shell操作">HDFS的Shell操作</a></h1>
<ul>
<li><a href="HDFS-Shell%E6%93%8D%E4%BD%9C.html#21-%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95">2.1 基本语法</a></li>
<li><a href="HDFS-Shell%E6%93%8D%E4%BD%9C.html#22-%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8">2.2 命令大全</a></li>
<li><a href="HDFS-Shell%E6%93%8D%E4%BD%9C.html#23-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AE%9E%E6%93%8D">2.3 常用命令实操</a>
<ul>
<li><a href="HDFS-Shell%E6%93%8D%E4%BD%9C.html#231-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C">2.3.1 准备工作</a></li>
<li><a href="HDFS-Shell%E6%93%8D%E4%BD%9C.html#232-%E4%B8%8A%E4%BC%A0">2.3.2 上传</a></li>
<li><a href="HDFS-Shell%E6%93%8D%E4%BD%9C.html#233-%E4%B8%8B%E8%BD%BD">2.3.3 下载</a></li>
<li><a href="HDFS-Shell%E6%93%8D%E4%BD%9C.html#234-hdfs-%E7%9B%B4%E6%8E%A5%E6%93%8D%E4%BD%9C">2.3.4 HDFS 直接操作</a></li>
</ul>
</li>
</ul>
<h2 id="21-基本语法"><a class="header" href="#21-基本语法">2.1 基本语法</a></h2>
<p><code>hadoop fs 具体命令</code> 和 <code>hdfs dfs 具体命令</code>两者等价。</p>
<h2 id="22-命令大全"><a class="header" href="#22-命令大全">2.2 命令大全</a></h2>
<p>执行<code>bin/hadoop fs</code>或<code>hdfs dfs</code>查看所有相关命令。</p>
<h2 id="23-常用命令实操"><a class="header" href="#23-常用命令实操">2.3 常用命令实操</a></h2>
<h3 id="231-准备工作"><a class="header" href="#231-准备工作">2.3.1 准备工作</a></h3>
<p>1）启动 Hadoop 集群（方便后续的测试）</p>
<p>其实如果看了之前的Hadoop入门(十三)——集群常用知识(面试题)与技巧总结里面写了一个快速启动集群的脚本，只需要一个命令即可启动集群</p>
<pre><code>[leokadia@hadoop102 bin]$ myhadoop.sh start
</code></pre>
<p>或者也可以像之前一样分别在102，103上使用以下两个命令：</p>
<pre><code>[leokadia@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
[leokadia@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
</code></pre>
<p>2）-help：输出这个命令参数</p>
<p>如果对哪一个命令的用法不是特别清楚，用以下指令查看该命令如何使用的：</p>
<p><code>hadoop fs -help rm</code></p>
<p>3）创建/Marvel 文件夹</p>
<p><code>hadoop fs -mkdir /Marvel</code></p>
<p>打开之后就有了一个漫威的文件夹</p>
<p>后续的命令就在漫威的文件夹里面开启我们的漫威之旅</p>
<p>一共分三大类命令：上传、下载、HDFS直接操作</p>
<h3 id="232-上传"><a class="header" href="#232-上传">2.3.2 上传</a></h3>
<p>注意以下命令执行目录都是<code>hadoop-3.1.3</code>，（这里其实可以是任何路径）</p>
<p>1）<code>-moveFromLocal</code>：从本地移动到 HDFS</p>
<p>在虚拟机中执行<code>vim Avengers.txt</code>创建一个txt文件</p>
<p>输入：The Avengers</p>
<p>执行<code>hadoop fs -moveFromLocal ./Avengers.txt /Marvel</code>将其上传到HDFS</p>
<p>源文件：Avengers.txt</p>
<p>目的目录：Marvel</p>
<p>2）<code>-copyFromLocal</code>：从本地文件系统中拷贝文件到 HDFS 路径去</p>
<p><code>hadoop fs -copyFromLocal X-Men.txt /Marvel</code></p>
<p>3）<code>-put</code>：等同于 -<code>copyFromLocal</code>，生产环境更习惯用 <code>put</code></p>
<p><code>hadoop fs -put ./Fantastic_Four.txt /Marvel</code></p>
<p>4）<code>-appendToFile</code>：追加一个文件到已经存在的文件末尾</p>
<p>HDFS只能追加，不允许随机修改，而且只能在文件的末尾进行追加</p>
<p><code>hadoop fs -appendToFile Iron_Man.txt /Marvel/Avengers.txt</code></p>
<h3 id="233-下载"><a class="header" href="#233-下载">2.3.3 下载</a></h3>
<p>注意以下命令执行目录都是<code>hadoop-3.1.3</code></p>
<p>1）<code>-copyToLocal</code>：从 HDFS 拷贝到本地</p>
<p>将Marvel中的Averagers.txt拷贝到当前文件夹</p>
<p><code>hadoop fs -copyToLocal /Marvel/Avengers.txt ./</code></p>
<p>2）<code>-get</code>：等同于 <code>-copyToLocal</code>，生产环境更习惯用 <code>get</code></p>
<p>将Marvel中的Averagers.txt拷贝到当前文件夹，并将拷贝的文件名更改为The_Avengers.txt</p>
<p><code>hadoop fs -get /Marvel/Avengers.txt ./The_Avengers.txt</code></p>
<h3 id="234-hdfs-直接操作"><a class="header" href="#234-hdfs-直接操作">2.3.4 HDFS 直接操作</a></h3>
<p>注意以下命令执行目录都是<code>hadoop-3.1.3</code>（任何路径都可以）</p>
<p>1）-ls: 显示目录信息</p>
<p>查询根目录</p>
<p><code>hadoop fs -ls /</code></p>
<p>查询Marvel目录</p>
<p><code>hadoop fs -ls /Marvel</code></p>
<p>2）-cat：显示文件内容</p>
<p><code>hadoop fs -cat /Marvel/Avengers.txt</code></p>
<p>3）-chgrp、-chmod、-chown：Linux 文件系统中的用法一样，修改文件所属权限</p>
<p><code>hadoop fs -chmod 666 /Marvel/Avengers.txt</code></p>
<p><code>hadoop fs -chown leokadia:leokadia /Marvel/Avengers.txt</code></p>
<p>4）-mkdir：创建路径</p>
<p><code>hadoop fs -mkdir /DC</code></p>
<p>再建一个Disney文件夹（因为后来漫威被迪士尼收购了）</p>
<p><code>hadoop fs -mkdir /Disney</code></p>
<p>5）-cp：从 HDFS 的一个路径拷贝到 HDFS 的另一个路径</p>
<p><code>hadoop fs -cp /Marvel/Avengers.txt /Disney</code></p>
<p>这个命令是拷贝，也就是说复仇者联盟在漫威中还有一份</p>
<p>6）-mv：在 HDFS 目录中移动文件</p>
<p><code>hadoop fs -mv /Marvel/Fantastic_Four.txt /Disney</code></p>
<p><code>hadoop fs -mv /Marvel/X-Men.txt /Disney</code></p>
<p>成功移动，注意，这个是移动，所以原来的文件夹里这两个文件没有了。</p>
<p>7）-tail：显示一个文件的末尾 1kb 的数据</p>
<p><code>hadoop fs -tail /Marvel/Avengers.txt</code></p>
<p>8）-rm：删除文件或文件夹</p>
<p><code>hadoop fs -rm /Marvel/Avengers.txt</code></p>
<p>9）-rm -r：递归删除目录及目录里面内容</p>
<p>删除文件夹及里面的内容</p>
<p><code>hadoop fs -rm -r /Marvel</code></p>
<p>没有Marvel文件夹了，呜呜呜。。。</p>
<p>注意：用 rm-r 之类的删除命令一定要慎重，慎重，再慎重！！！</p>
<p>10）-du 统计文件夹的大小信息</p>
<p><code>hadoop fs -du -s -h /Disney</code></p>
<p>说明：43 表示文件大小；129 表示 43*3 个副本；/Disney 表示查看的目录</p>
<p><code>hadoop fs -du -h /Disney</code></p>
<p>22+15+6=43 所以Disney文件总大小为43</p>
<p>11）-setrep：设置 HDFS 中文件的副本数量</p>
<p>给Avengers.txt设置10个副本：</p>
<p><code>hadoop fs -setrep 10 /Disney/Avengers.txt</code></p>
<p>这里设置的副本数只是记录在 NameNode 的元数据中， 是否真的会有这么多副本， 还得看DataNode 的数量。因为目前只有 3 台设备，最多也就 3 个副本，只有节点数的增加到 10台时，副本数才能达到 10。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs的api操作"><a class="header" href="#hdfs的api操作">HDFS的API操作</a></h1>
<ul>
<li><a href="HDFS-API%E6%93%8D%E4%BD%9C.html#31-%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87">3.1 客户端环境准备</a></li>
<li><a href="HDFS-API%E6%93%8D%E4%BD%9C.html#32-hdfs-%E7%9A%84-api-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.2 HDFS 的 API 案例实操</a>
<ul>
<li><a href="HDFS-API%E6%93%8D%E4%BD%9C.html#321-%E7%94%A8%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9C%E7%A8%8B%E5%88%9B%E5%BB%BA%E7%9B%AE%E5%BD%95">3.2.1 用客户端远程创建目录</a></li>
<li><a href="HDFS-API%E6%93%8D%E4%BD%9C.html#322-hdfs-%E7%94%A8%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E6%B5%8B%E8%AF%95-%E5%8F%82%E6%95%B0%E4%BC%98%E5%85%88%E7%BA%A7-">3.2.2 HDFS 用客户端上传文件（测试 参数优先级 ）</a></li>
<li><a href="HDFS-API%E6%93%8D%E4%BD%9C.html#323-hdfs-%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD">3.2.3 HDFS 文件下载</a></li>
<li><a href="HDFS-API%E6%93%8D%E4%BD%9C.html#324-hdfs-%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E5%92%8C%E7%9B%AE%E5%BD%95">3.2.4 HDFS 删除文件和目录</a></li>
<li><a href="HDFS-API%E6%93%8D%E4%BD%9C.html#325-%E6%96%87%E4%BB%B6%E7%9A%84%E6%9B%B4%E5%90%8D%E5%92%8C%E7%A7%BB%E5%8A%A8">3.2.5 文件的更名和移动</a></li>
<li><a href="HDFS-API%E6%93%8D%E4%BD%9C.html#326-%E9%80%9A%E8%BF%87%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E6%96%B9%E5%BC%8F%E8%8E%B7%E5%8F%96-hdfs-%E6%96%87%E4%BB%B6-%E8%AF%A6%E6%83%85%E4%BF%A1%E6%81%AF">3.2.6 通过客户端的方式获取 HDFS 文件 详情信息</a></li>
<li><a href="HDFS-API%E6%93%8D%E4%BD%9C.html#327-hdfs-%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%B9%E5%88%A4%E6%96%AD">3.2.7 HDFS 文件和文件夹判断</a></li>
</ul>
</li>
</ul>
<p>刚刚（二）讲的是用Shell/Hadoop fs/HDFS/dfs的一些相关操作，相当于是在集群内部，跟集群的一些客户端打交道，这章讲的是：我们希望在Windows环境（办公环境）对远程的集群进行一个客户端访问，于是现在就在Windows环境上写代码，写HDFS客户端代码，远程连接上集群，对它们进行增删改查相关操作。</p>
<h2 id="31-客户端环境准备"><a class="header" href="#31-客户端环境准备">3.1 客户端环境准备</a></h2>
<p>想让我们的windows能够连接上远程的Hadoop集群，windows里面也得有相关的环境变量</p>
<p>1） 下载 hadoop-3.1.0 （windows版）到非中文路径 （比如E:\Sofware）</p>
<p>2 ） 配置 HADOOP_HOME 环境 变量</p>
<p>3 ） 配置 Path 环境 变量。</p>
<p>将HADOOP_HOME目录添加到对应的PATH目录</p>
<p>注意： 如果环境变量不起作用，可以 重启电脑 试试。</p>
<p>验证 Hadoop 环境变量是否正常。双击 winutils.exe，如果报如下错误。</p>
<p>说明缺少微软运行库 （正版系统往往有这个问题） 。 下载微软运行库安装包双击安装即可。</p>
<p>4 ） 在 IDEA 中 创建一个 Maven 工程 HdfsClientDemo ，并导入相应的依赖坐标+ 日志</p>
<p>创建Maven工程以及进行相关配置</p>
<p>5 ） 创建包名 ：com.leokadia.hdfs</p>
<p>6 ） 创建 HdfsClient 类</p>
<p>创建好了客户端类，接下来写代码操作远程的服务器集群集群</p>
<p>7 ） 执行 程序</p>
<p>客户端去操作 HDFS 时，是有一个用户身份的。默认情况下，HDFS 客户端 API 会从采用 Windows 默认用户访问 HDFS，会报权限异常错误。所以在访问 HDFS 时，一定要配置用户。</p>
<h2 id="32-hdfs-的-api-案例实操"><a class="header" href="#32-hdfs-的-api-案例实操">3.2 HDFS 的 API 案例实操</a></h2>
<h3 id="321-用客户端远程创建目录"><a class="header" href="#321-用客户端远程创建目录">3.2.1 用客户端远程创建目录</a></h3>
<pre><code class="language-java">package com.leokadia.hdfs;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.Configuration;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * @author sa
 * @create 2021-05-04 16:37
 *
 *
 * 客户端代码常用套路
 * 1、获取一个客户端对象
 * 2、执行相关的操作命令
 * 3、关闭资源
 * HDFS zookeeper
 */
public class HdfsClient {

    private FileSystem fs;

    @Before
    public void init() throws IOException, URISyntaxException, InterruptedException {

        //连接集群的nn地址
        URI uri = new URI("hdfs://hadoop102:8020");

        //创建一个配置文件
        Configuration configuration = new Configuration();

        //用户
        String user = "leokadia";

        // 1 获取客户端对象
        fs = FileSystem.get(uri, configuration, user);
    }

    @After
    public void close() throws IOException {
        // 3 关闭资源
        fs.close();
    }

    @Test
    public void testmkdir() throws URISyntaxException,IOException,InterruptedException {
        // 2 创建一个文件夹
        fs.mkdirs(new Path("/Marvel/Avengers"));

    }
}
</code></pre>
<p>运行@Test</p>
<p>成功创建文件夹</p>
<h3 id="322-hdfs-用客户端上传文件测试-参数优先级-"><a class="header" href="#322-hdfs-用客户端上传文件测试-参数优先级-">3.2.2 HDFS 用客户端上传文件（测试 参数优先级 ）</a></h3>
<p>1）上传文件</p>
<p>先在D盘根目录下创建一个待上传的文件</p>
<p>在刚刚的代码中加入如下代码</p>
<pre><code class="language-java"> // 上传
    @Test
    public void testPut() throws IOException {
        //参数解读：参数一：表示删除原数据；参数二：是否允许覆盖；参数三：原数据路径；参数四：目的地路径
        fs.copyFromLocalFile(false,false,new Path("D:\\Iron_Man.txt"),new Path("hdfs://hadoop102/Marvel/Avengers"));
    }

</code></pre>
<p>点击运行</p>
<p>2 ） 将 hdfs-site.xml 拷贝到项目的 resources 资源目录下</p>
<p>已知服务器的默认配置 （xxx-default.xml） 中的副本数是3，现在resources下新建一个file——hdfs-site.xml修改副本数，测试二者的优先级</p>
<p>在resources下新建一个file——hdfs-site.xml</p>
<p>将下面代码粘贴到里面：（修改副本数为1）</p>
<pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt; 
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt; 
 
&lt;configuration&gt; 
  &lt;property&gt; 
   &lt;name&gt;dfs.replication&lt;/name&gt;       
    &lt;value&gt;1&lt;/value&gt; 
  &lt;/property&gt; 
&lt;/configuration&gt; 
</code></pre>
<p>如果再上传一个文件，它的副本数为1，说明 resources 资源目录下的hdfs-site.xml 优先级高
再创建一个测试文件</p>
<p>执行上传，发现副本数为1</p>
<p>说明在项目资源目录下用户自定义的配置文件高</p>
<p>再测试客户端代码中配置副本的值的优先级：</p>
<p>在源代码中加上：</p>
<pre><code class="language-java">configuration.set("dfs.replication","2");
</code></pre>
<p>再运行一遍刚刚的代码</p>
<p>发现Spider_Man.txt副本数变为2了</p>
<p>说明客户端代码中设置的值 &gt;ClassPath 下的用户自定义配置文件</p>
<p>3 ）总结： 参数 优先级</p>
<p>参数优先级排序：</p>
<p>（1）客户端代码中设置的值 &gt;
（2）ClassPath 下的用户自定义配置文件 &gt;
（3） 然后是服务器的自定义配置 （xxx-site.xml） &gt;
（4） 服务器的默认配置 （xxx-default.xml）</p>
<h3 id="323-hdfs-文件下载"><a class="header" href="#323-hdfs-文件下载">3.2.3 HDFS 文件下载</a></h3>
<pre><code class="language-java">// 文件下载
    @Test
    public void testGet() throws IOException {
        //参数解读：参数一： boolean delSrc 指是否将原文件删除；参数二：Path src 指要下载的原文件路径
        // 参数三：Path dst 指将文件下载到的目标地址路径；参数四：boolean useRawLocalFileSystem 是否开启文件校验
        fs.copyToLocalFile(false, new Path("hdfs://hadoop102/Marvel/Avengers/Iron_Man.txt"), new Path("D:\\Robert.txt"), false);
    }

</code></pre>
<h3 id="324-hdfs-删除文件和目录"><a class="header" href="#324-hdfs-删除文件和目录">3.2.4 HDFS 删除文件和目录</a></h3>
<pre><code class="language-java"> // 删除
    @Test
    public void testRm() throws IOException {

        // 参数解读：参数1：要删除的路径； 参数2 ： 是否递归删除
        // 删除文件(不再演示了)
        fs.delete(new Path("/jdk-8u212-linux-x64.tar.gz"),false);

        // 删除空目录
        fs.delete(new Path("/delete_test_empty"), false);

        // 删除非空目录
        fs.delete(new Path("/Marvel"), true);
    }

</code></pre>
<h3 id="325-文件的更名和移动"><a class="header" href="#325-文件的更名和移动">3.2.5 文件的更名和移动</a></h3>
<p>新建一个测试文件夹和相应的测试文件</p>
<pre><code class="language-java">// 文件的更名和移动
    @Test
    public void testmv() throws IOException {
        // 参数解读：参数1 ：原文件路径； 参数2 ：目标文件路径
        // 对文件名称的修改
        fs.rename(new Path("/move/from.txt"), new Path("/move/new.txt"));

        // 文件的移动和更名
        fs.rename(new Path("/move/new.txt"),new Path("/to.txt"));

        // 目录更名
        fs.rename(new Path("/move"), new Path("/shift"));

    }

</code></pre>
<h3 id="326-通过客户端的方式获取-hdfs-文件-详情信息"><a class="header" href="#326-通过客户端的方式获取-hdfs-文件-详情信息">3.2.6 通过客户端的方式获取 HDFS 文件 详情信息</a></h3>
<p>查看文件名称、权限、长度、块信息</p>
<pre><code class="language-java">// 获取文件详细信息
    @Test
    public void fileDetail() throws IOException {

        // 获取所有文件信息
        RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path("/"), true);

        // 遍历文件
        while (listFiles.hasNext()) {
            LocatedFileStatus fileStatus = listFiles.next();

            System.out.println("==========" + fileStatus.getPath() + "=========");
            System.out.println(fileStatus.getPermission());
            System.out.println(fileStatus.getOwner());
            System.out.println(fileStatus.getGroup());
            System.out.println(fileStatus.getLen());
            System.out.println(fileStatus.getModificationTime());
            System.out.println(fileStatus.getReplication());
            System.out.println(fileStatus.getBlockSize());
            System.out.println(fileStatus.getPath().getName());

            // 获取块信息
            BlockLocation[] blockLocations = fileStatus.getBlockLocations();

            System.out.println(Arrays.toString(blockLocations));

        }
    }

</code></pre>
<h3 id="327-hdfs-文件和文件夹判断"><a class="header" href="#327-hdfs-文件和文件夹判断">3.2.7 HDFS 文件和文件夹判断</a></h3>
<pre><code class="language-java">// 判断是文件夹还是文件
    @Test
    public void testFile() throws IOException {

        FileStatus[] listStatus = fs.listStatus(new Path("/"));

        for (FileStatus status : listStatus) {

            if (status.isFile()) {
                System.out.println("文件：" + status.getPath().getName());
            } else {
                System.out.println("目录：" + status.getPath().getName());
            }
        }
    }

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="mapreduce"><a class="header" href="#mapreduce">MapReduce</a></h1>
<ul>
<li><a href="MapReduce.html#%E6%A6%82%E8%A7%88">概览</a></li>
<li><a href="MapReduce.html#mapreduce-%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93">MapReduce 开发总结</a></li>
<li><a href="MapReduce.html#%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">常见错误及解决方案</a></li>
</ul>
<h2 id="概览"><a class="header" href="#概览">概览</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102144251.png" alt="" /></p>
<p><a href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/04_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88MapReduce%EF%BC%89V3.3/">完整博文来源</a></p>
<h2 id="mapreduce-开发总结"><a class="header" href="#mapreduce-开发总结">MapReduce 开发总结</a></h2>
<p><strong>1</strong>）输入数据接口：InputFormat</p>
<p>（1）默认使用的实现类是：TextInputFormat</p>
<p>（2）TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为 key，行内容作为 value 返回。</p>
<p>（3）CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</p>
<p><strong>2</strong>）逻辑处理接口：Mapper</p>
<p>用户根据业务需求实现其中三个方法：map ()、setup () 和 cleanup ()</p>
<p><strong>3</strong>）Partitioner 分区</p>
<p>（1）有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；<code>key.hashCode ()&amp;Integer.MAXVALUE % numReduces</code></p>
<p>（2）如果业务上有特别的需求，可以自定义分区。</p>
<p><strong>4</strong>）Comparable 排序</p>
<p>（1）当我们用自定义的对象作为 key 来输出时，就必须要实现 <code>WritableComparable</code> 接口，重写其中的 <code>compareTo ()</code> 方法。</p>
<p>（2）部分排序：对最终输出的每一个文件进行内部排序。</p>
<p>（3）全排序：对所有数据进行排序，通常只有一个 Reduce。</p>
<p>（4）二次排序：排序的条件有两个。</p>
<p><strong>5</strong>）Combiner 合并</p>
<p>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。</p>
<p><strong>6</strong>）逻辑处理接口：Reducer</p>
<p>用户根据业务需求实现其中三个方法：reduce ()、setup () 和 cleanup ()</p>
<p><strong>7</strong>）输出数据接口：OutputFormat</p>
<p>（1）默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</p>
<p>（2）用户还可以自定义 OutputFormat。</p>
<h2 id="常见错误及解决方案"><a class="header" href="#常见错误及解决方案">常见错误及解决方案</a></h2>
<p>1）导包容易出错。尤其 Text 和 CombineTextInputFormat。</p>
<p>2）Mapper 中第一个输入的参数必须是 LongWritable 或者 NullWritable，不可以是 IntWritable. 报的错误是类型转换异常。</p>
<p>3）java.lang.Exception: java.io.IOException: Illegal partition for 13926435656 (4)，说明 Partition 和 ReduceTask 个数没对上，调整 ReduceTask 个数。</p>
<p>4）如果分区数不是 1，但是 reducetask 为 1，是否执行分区过程。答案是：不执行分区过程。因为在 MapTask 的源码中，执行分区的前提是先判断 ReduceNum 个数是否大于 1。不大于 1 肯定不执行。</p>
<p>5）在 Windows 环境编译的 jar 包导入到 Linux 环境中运行，</p>
<pre><code>hadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver/user/atguigu//user/atguigu/output
</code></pre>
<p>报如下错误：</p>
<pre><code>Exception in thread “main” java.lang.UnsupportedClassVersionError: com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version 52.0
</code></pre>
<p>原因是 Windows 环境用的 jdk1.7，Linux 环境用的 jdk1.8。</p>
<p>解决方案：统一 jdk 版本。</p>
<p>6）缓存 <code>pd.txt</code> 小文件案例中，报找不到 <code>pd.txt</code> 文件</p>
<p>原因：大部分为路径书写错误。还有就是要检查 <code>pd.txt.txt</code> 的问题。还有个别电脑写相对路径找不到 <code>pd.txt</code>，可以修改为绝对路径。</p>
<p>7）报类型转换异常。</p>
<p>通常都是在驱动函数中设置 <code>Map</code> 输出和最终输出时编写错误。</p>
<p>Map 输出的 key 如果没有排序，也会报类型转换异常。</p>
<p>8）集群中运行 <code>wc.jar</code> 时出现了无法获得输入文件。</p>
<p>原因：<code>WordCount</code> 案例的输入文件不能放用 <code>HDFS</code> 集群的根目录。</p>
<p>9）出现了如下相关异常</p>
<pre><code>Exception in thread "main" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0 (Ljava/lang/String;I) Z  
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0 (Native Method)  
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access (NativeIO.java:609)  
	at org.apache.hadoop.fs.FileUtil.canRead (FileUtil.java:977)  
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.  
	at org.apache.hadoop.util.Shell.getQualifiedBinPath (Shell.java:356)  
	at org.apache.hadoop.util.Shell.getWinUtilsPath (Shell.java:371)  
	at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:364)
</code></pre>
<p>解决方案：拷贝 <code>hadoop.dll</code> 文件到 <code>Windows</code> 目录 <code>C:\Windows\System32</code>。个别同学电脑还需要修改 <code>Hadoop</code> 源码。</p>
<p>方案二：创建包 <code>org.apache.hadoop.io.nativeio</code>，并将 NativeIO.java 拷贝到该包名下</p>
<p><a href="https://blog.csdn.net/syl_ccc/article/details/105946007">详情参考 org.apache.hadoop.io.nativeio.NativeIO$Windows.access0 (Ljava/lang/String;I) Z 的解决办法</a></p>
<p>10）自定义 Outputformat 时，注意在 RecordWirter 中的 close 方法必须关闭流资源。否则输出的文件内容中数据为空。</p>
<pre><code class="language-java">@Override  
public void close (TaskAttemptContext context) throws IOException, InterruptedException {  
		if (atguigufos != null) {  
			atguigufos.close ();  
		}  
		if (otherfos != null) {  
			otherfos.close ();  
		}  
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="mapreduce-概述"><a class="header" href="#mapreduce-概述">MapReduce 概述</a></h1>
<ul>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E5%AE%9A%E4%B9%89">MapReduce 定义</a></li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E4%BC%98%E7%BC%BA%E7%82%B9">MapReduce 优缺点</a>
<ul>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#%E4%BC%98%E7%82%B9">优点</a></li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#%E7%BC%BA%E7%82%B9">缺点</a></li>
</ul>
</li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3">MapReduce 核心编程思想</a></li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E8%BF%9B%E7%A8%8B">MapReduce 进程</a></li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#%E5%AE%98%E6%96%B9-wordcount-%E6%BA%90%E7%A0%81">官方 WordCount 源码</a></li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96%E7%B1%BB%E5%9E%8B">常用数据序列化类型</a></li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83">MapReduce 编程规范</a></li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#wordcount-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">WordCount 案例实操</a>
<ul>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#%E6%9C%AC%E5%9C%B0%E6%B5%8B%E8%AF%95">本地测试</a></li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#wordcount-%E6%A1%88%E4%BE%8B-debug-%E8%B0%83%E8%AF%95">WordCount 案例 Debug 调试</a></li>
<li><a href="MapReduce%E6%A6%82%E8%BF%B0.html#%E6%8F%90%E4%BA%A4%E5%88%B0%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95">提交到集群测试</a></li>
</ul>
</li>
</ul>
<h2 id="mapreduce-定义"><a class="header" href="#mapreduce-定义">MapReduce 定义</a></h2>
<p>MapReduce 是一个分布式运算程序的编程框架，是用户开发 “基于 Hadoop 的数据分析应用” 的核心框架。</p>
<p>MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。</p>
<h2 id="mapreduce-优缺点"><a class="header" href="#mapreduce-优缺点">MapReduce 优缺点</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102144435.png" alt="" /></p>
<h3 id="优点"><a class="header" href="#优点">优点</a></h3>
<p>1 ）MapReduce 易于编程</p>
<p>它简单的实现一些接口， 就可以完成一个分布式程序， 这个分布式程序可以分布到大量廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得 MapReduce 编程变得非常流行。</p>
<p>2 ） 良好的扩展性</p>
<p>当你的计算资源不能得到满足的时候， 你可以通过简单的增加机器来扩展它的计算能力。</p>
<p>3 ） 高容错性</p>
<p>MapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上， 这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败， 而且这个过程不需要人工参与， 而完全是由 Hadoop 内部完成的。</p>
<p>4 ） 适合 PB 级以上海量数据的离线处理</p>
<p>可以实现上千台服务器集群并发工作，提供数据处理能力。</p>
<h3 id="缺点"><a class="header" href="#缺点">缺点</a></h3>
<p>1 ） 不擅长实时计算</p>
<p>MapReduce 无法像 MySQL 一样，在毫秒或者秒级内返回结果。</p>
<p>2 ） 不擅长流式计算</p>
<p>流式计算的输入数据是动态的， 而 MapReduce 的输入数据集是静态的， 不能动态变化。
这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。</p>
<p>3 ） 不擅长 DAG （有向无环图）计算</p>
<p>多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce 并不是不能做， 而是使用后， 每个 MapReduce 作业的输出结果都会写入到磁盘，会造成大量的磁盘 IO，导致性能非常的低下。</p>
<h2 id="mapreduce-核心编程思想"><a class="header" href="#mapreduce-核心编程思想">MapReduce 核心编程思想</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102144500.png" alt="" /></p>
<p>（1）分布式的运算程序往往需要分成至少 2 个阶段。</p>
<p>（2）第一个阶段的 MapTask 并发实例，完全并行运行，互不相干。</p>
<p>（3）第二个阶段的 ReduceTask 并发实例互不相干，但是他们的数据依赖于上一个阶段的所有 MapTask 并发实例的输出。</p>
<p>（4）MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行。</p>
<p>总结：分析 WordCount 数据流走向深入理解 MapReduce 核心思想。</p>
<h2 id="mapreduce-进程"><a class="header" href="#mapreduce-进程">MapReduce 进程</a></h2>
<p>任务、job、MR 都表示任务。</p>
<p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程：</p>
<p>（1）MrAppMaster：负责整个程序的过程调度及状态协调。</p>
<p>（2）MapTask：负责 Map 阶段的整个数据处理流程。</p>
<p>（3）ReduceTask：负责 Reduce 阶段的整个数据处理流程。</p>
<h2 id="官方-wordcount-源码"><a class="header" href="#官方-wordcount-源码">官方 WordCount 源码</a></h2>
<p>采用反编译工具反编译源码，发现 WordCount 案例有 Map 类、Reduce 类和驱动类，且数据的类型是 Hadoop 自身封装的序列化类型。</p>
<p>如何查看里面的代码程序呢？使用反编译工具 jd-gui</p>
<p><a href="https://blog.csdn.net/zlbdmm/article/details/104653823">java 反编译工具 jd-gui 下载与使用</a></p>
<p>查看 <code>hadoop-mapreduce-examples-3.3.6.jar</code> 得到如下的源代码：</p>
<pre><code class="language-java">/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.examples;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class WordCount {

  public static class TokenizerMapper 
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{
    
    private final static IntWritable one = new IntWritable (1);
    private Text word = new Text ();
      
    public void map (Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer (value.toString ());
      while (itr.hasMoreTokens ()) {
        word.set (itr.nextToken ());
        context.write (word, one);
      }
    }
  }
  
  public static class IntSumReducer 
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
    private IntWritable result = new IntWritable ();

    public void reduce (Text key, Iterable&lt;IntWritable&gt; values, 
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get ();
      }
      result.set (sum);
      context.write (key, result);
    }
  }

  public static void main (String [] args) throws Exception {
    Configuration conf = new Configuration ();
    String [] otherArgs = new GenericOptionsParser (conf, args).getRemainingArgs ();
    if (otherArgs.length &lt; 2) {
      System.err.println ("Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;");
      System.exit (2);
    }
    Job job = Job.getInstance (conf, "word count");
    job.setJarByClass (WordCount.class);
    job.setMapperClass (TokenizerMapper.class);
    job.setCombinerClass (IntSumReducer.class);
    job.setReducerClass (IntSumReducer.class);
    job.setOutputKeyClass (Text.class);
    job.setOutputValueClass (IntWritable.class);
    for (int i = 0; i &lt; otherArgs.length - 1; ++i) {
      FileInputFormat.addInputPath (job, new Path (otherArgs [i]));
    }
    FileOutputFormat.setOutputPath (job,
      new Path (otherArgs [otherArgs.length - 1]));
    System.exit (job.waitForCompletion (true) ? 0 : 1);
  }
}

</code></pre>
<h2 id="常用数据序列化类型"><a class="header" href="#常用数据序列化类型">常用数据序列化类型</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102145930.png" alt="" /></p>
<div class="table-wrapper"><table><thead><tr><th>Java 类型</th><th>Hadoop Writable 类型</th></tr></thead><tbody>
<tr><td>Boolean</td><td>BooleanWritable</td></tr>
<tr><td>Byte</td><td>ByteWritable</td></tr>
<tr><td>Int</td><td>IntWritable</td></tr>
<tr><td>Float</td><td>FloatWritable</td></tr>
<tr><td>Long</td><td>LongWritable</td></tr>
<tr><td>Double</td><td>DoubleWritable</td></tr>
<tr><td>String</td><td>Text</td></tr>
<tr><td>Map</td><td>MapWritable</td></tr>
<tr><td>Array</td><td>ArrayWritable</td></tr>
<tr><td>Null</td><td>NullWritable</td></tr>
</tbody></table>
</div>
<h2 id="mapreduce-编程规范"><a class="header" href="#mapreduce-编程规范">MapReduce 编程规范</a></h2>
<p>用户编写的程序分成三个部分：Mapper、Reducer 和 Driver。</p>
<p>1．Mapper 阶段</p>
<p>（1）用户自定义的 Mapper 要继承自己的父类</p>
<pre><code class="language-java">public static class TokenizerMapper 
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{}
</code></pre>
<p>（2）Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义）</p>
<p>p.s. K 是这一行的首字符偏移量，V 是这一行的内容。</p>
<p>（3）Mapper 中的业务逻辑写在 map () 方法中</p>
<pre><code class="language-java">public void map (Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer (value.toString ());
      while (itr.hasMoreTokens ()) {
        word.set (itr.nextToken ());
        context.write (word, one);
      }
    }
</code></pre>
<p>（4）Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义）</p>
<p>（5）map () 方法（MapTask 进程）对每一个 &lt; K,V &gt; 调用一次</p>
<p>2．Reducer 阶段</p>
<p>（1）用户自定义的 Reducer 要继承自己的父类</p>
<pre><code class="language-java">public static class IntSumReducer 
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt;{}
</code></pre>
<p>（2）Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV</p>
<p>（3）Reducer 的业务逻辑写在 reduce () 方法中</p>
<pre><code class="language-java">public void reduce (Text key, Iterable&lt;IntWritable&gt; values, 
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get ();
      }
      result.set (sum);
      context.write (key, result);
    }
</code></pre>
<p>（4）ReduceTask 进程对每一组相同 k 的 &lt; k,v &gt; 组调用一次 reduce () 方法</p>
<p>3．Driver 阶段</p>
<p>相当于 YARN 集群的客户端，用于提交我们整个程序到 YARN 集群，提交的是封装了 MapReduce 程序相关运行参数的 job 对象</p>
<h2 id="wordcount-案例实操"><a class="header" href="#wordcount-案例实操">WordCount 案例实操</a></h2>
<h3 id="本地测试"><a class="header" href="#本地测试">本地测试</a></h3>
<p>1 ） 需求</p>
<p>在给定的文本文件中统计输出每一个单词出现的总次数，输出结果默认按照 utf-8 编码顺序排列</p>
<p>（1）输入数据</p>
<p>创建一个文件并写入想要测试的数据，例如：</p>
<pre><code>Avengers Avengers
DC DC
Mavel Mavel
Iron_Man
Captain_America
Thor
Hulk
Black_Widow
Hawkeye
Black_Panther
Spider_Man
Doctor_Strange
Ant_Man
Vision
Scarlet_Witch
Winter_Soldier
Loki
Star_Lord
Gamora
Rocket_Raccoon
Groot
</code></pre>
<p>（2）期望输出数据（涉及输入的排序问题）</p>
<pre><code>Ant_Man	1
Avengers	2
Black_Panther	1
Black_Widow	1
Captain_America	1
DC	2
Doctor_Strange	1
Gamora	1
Groot	1
Hawkeye	1
Hulk	1
Iron_Man	1
Loki	1
Mavel	2
Rocket_Raccoon	1
Scarlet_Witch	1
Spider_Man	1
Star_Lord	1
Thor	1
Vision	1
Winter_Soldier	1
</code></pre>
<p>2 ） 需求分析</p>
<p>按照 MapReduce 编程规范，分别编写 Mapper、Reducer、Driver 类。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102150832.png" alt="" /></p>
<p>3 ） 环境准备</p>
<p>（1）创建 maven 工程 MapReduceDemo</p>
<p>（2）在 pom.xml 文件中添加版本信息以及相关依赖</p>
<pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.TianHan&lt;/groupId&gt;
    &lt;artifactId&gt;wordcount&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;properties&gt;
        &lt;maven.compiler.source&gt;21&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;21&lt;/maven.compiler.target&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;3.3.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;
            &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt;
            &lt;version&gt;5.11.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;version&gt;2.0.16&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

&lt;/project&gt;
</code></pre>
<p>（2）在项目的 src/main/resources 目录下，新建一个文件，命名为 “log4j.properties” 用于打印相关日志。在该文件中填入：</p>
<pre><code>log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=% d % p [% c] - % m% n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=% d % p [% c] - % m% n
</code></pre>
<p>（3）创建包 mapreduce.wordcount 然后创建三个类</p>
<p>4 ） 编写程序</p>
<p>（1）编写 Mapper 类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.wordcount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

/**
 * KEYIN, map 阶段输入的 key 的类型：LongWritable
 * VALUEIN,map 阶段输入 value 类型：Text
 * KEYOUT,map 阶段输出的 Key 类型：Text
 * VALUEOUT,map 阶段输出的 value 类型：IntWritable
 */
public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    private Text outK = new Text ();
    private IntWritable outV = new IntWritable (1);  //map 阶段不进行聚合

    @Override
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {

        // 1 获取一行
        //xxxxxx xxxxxx
        String line = value.toString ();

        // 2 切割 (取决于原始数据的中间分隔符)
        //xxxxxxx
        //xxxxxxx
        String [] words = line.split (" ");

        // 3 循环写出
        for (String word : words) {
            // 封装 outk
            outK.set (word);

            // 写出
            context.write (outK, outV);
        }
    }
}
</code></pre>
<p>（2）编写 Reducer 类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.wordcount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * KEYIN, reduce 阶段输入的 key 的类型：Text
 * VALUEIN,reduce 阶段输入 value 类型：IntWritable
 * KEYOUT,reduce 阶段输出的 Key 类型：Text
 * VALUEOUT,reduce 阶段输出的 value 类型：IntWritable
 */
public class WordCountReducer extends Reducer&lt;Text, IntWritable,Text,IntWritable&gt; {
    private IntWritable outV = new IntWritable ();

    @Override
    protected void reduce (Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {

        int sum = 0;
        //xxxxxxx xxxxxxx -&gt;(xxxxxxx,1),(xxxxxxx,1)
        //xxxxxxx, (1,1)
        // 将 values 进行累加
        for (IntWritable value : values) {
            sum += value.get ();
        }

        outV.set (sum);

        // 写出
        context.write (key,outV);
    }
}
</code></pre>
<p>（3）编写 Driver 驱动类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.wordcount;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class WordCountDriver {

    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {

        // 1 获取 job
        Configuration conf = new Configuration ();
        Job job = Job.getInstance (conf);

        // 2 设置 jar 包路径
        job.setJarByClass (WordCountDriver.class);

        // 3 关联 mapper 和 reducer
        job.setMapperClass (WordCountMapper.class);
        job.setReducerClass (WordCountReducer.class);

        // 4 设置 map 输出的 kv 类型
        job.setMapOutputKeyClass (Text.class);
        job.setMapOutputValueClass (IntWritable.class);

        // 5 设置最终输出的 kV 类型
        job.setOutputKeyClass (Text.class);
        job.setOutputValueClass (IntWritable.class);

        // 6 设置输入路径和输出路径
        FileInputFormat.setInputPaths (job, new Path ("E:\\BigData\\hadoop\\input"));
        FileOutputFormat.setOutputPath (job, new Path ("E:\\BigData\\hadoop\\output"));

        // 7 提交 job
        boolean result = job.waitForCompletion (true);

        System.exit (result ? 0 : 1);
    }
}
</code></pre>
<p>5 ） 本地测试</p>
<p>（1）由于这里通过 Maven 安装了 hadoop-client，所以不需要配置 HADOOP_HOME 变量以及 Windows 运行依赖即可成功运行程序。</p>
<p>（2）在 IDEA 上运行程序</p>
<p>注意：此时如果再运行一遍，会报错。在 mapreduce 中，如果输出路径存在会报错。</p>
<h3 id="wordcount-案例-debug-调试"><a class="header" href="#wordcount-案例-debug-调试">WordCount 案例 Debug 调试</a></h3>
<p>在以下几个地方打好断点：Mapper 类中 map 函数第一行、开始 setup、结束 cleanup</p>
<p>通过调试可以更清楚的理解机制。至少三遍。</p>
<h3 id="提交到集群测试"><a class="header" href="#提交到集群测试">提交到集群测试</a></h3>
<p>刚刚上面的代码是在本地运行的，是通过下载了 hadoop 相关的依赖，运用本地模式运行的。</p>
<p>我们还需要把程序推送到生产环境（Linux 环境）中。</p>
<p>（1）用 Maven 打包，需要添加的打包插件依赖</p>
<p>将下面的代码放在之前配置的依赖后面（对应 pom.xml 文件）</p>
<pre><code class="language-xml">&lt;build&gt; 
    &lt;plugins&gt; 
        &lt;plugin&gt; 
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; 
            &lt;version&gt;3.6.1&lt;/version&gt; 
            &lt;configuration&gt; 
                &lt;source&gt;1.8&lt;/source&gt; 
                &lt;target&gt;1.8&lt;/target&gt; 
            &lt;/configuration&gt; 
        &lt;/plugin&gt; 
        &lt;plugin&gt; 
            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; 
            &lt;configuration&gt; 
                &lt;descriptorRefs&gt; 
                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; 
                &lt;/descriptorRefs&gt; 
            &lt;/configuration&gt; 
            &lt;executions&gt; 
                &lt;execution&gt; 
                    &lt;id&gt;make-assembly&lt;/id&gt; 
                    &lt;phase&gt;package&lt;/phase&gt; 
                    &lt;goals&gt; 
                        &lt;goal&gt;single&lt;/goal&gt; 
                    &lt;/goals&gt; 
                &lt;/execution&gt; 
            &lt;/executions&gt; 
        &lt;/plugin&gt; 
    &lt;/plugins&gt; 
&lt;/build&gt; 
</code></pre>
<p><code>maven-compiler-plugin</code>：：打包但不带有所需 jar 包，jar 包小</p>
<p><code>maven-assembly-plugin</code>：打包并带有所需 jar 包，jar 包大</p>
<p>这需要根据需求使用。</p>
<p>（2）将程序打成 jar 包</p>
<p>打包完毕，生成 jar 包，去文件夹里查看一下</p>
<p>（3）修改不带依赖的 jar 包名称为 wc.jar，并拷贝该 jar 包到 Hadoop 集群的 /opt/module/hadoop-3.3.6 路径</p>
<p>思考：刚刚的程序中，我们写的路径是本地 Windows 系统中的路径，上传到 Linux 环境后它其实没有这个路径，输入输出路径不存在，于是我们需要对它进行修改，改成对应的集群路径。</p>
<p>如果想更灵活一点 —— 根据传入的路径来确定输入的路径，需要使用 <code>args [0]</code> 和 <code>args [1]</code></p>
<p>我们再创建一个 wordcount2 包，跟 wordcount 内容一致，就将输入输出路径修改了一下。</p>
<p>对于新改的程序，先点 clean 把前面的删掉，再点 package 进行打包。</p>
<p>将新的包按上面的操作更名 wc.jar</p>
<p>根据命令行设定输入输出路径，使用 <code>args [0]，args [1]</code>。</p>
<pre><code class="language-java">// 6 设置输入路径和输出路径
FileInputFormat.setInputPaths (job, new Path (args [0]));
FileOutputFormat.setOutputPath (job, new Path (args [1]));
</code></pre>
<p>（4）启动 Hadoop 集群</p>
<pre><code>start-dfs.sh
</code></pre>
<p>先在 HDFS 集群中设置刚刚要处理的源文件：</p>
<p>在集群中建一个 Marvel 文件夹，然后在文件夹中上传我们之前要处理的 Marvel.txt 源文件</p>
<p>（5）执行 WordCount 程序</p>
<p>生成 jar 包，导入 jar 包到集群，再重新运行程序</p>
<pre><code>hadoop jar wc.jar com.TianHan.mapreduce.wordcount.WordCountDriver mapreduce/input mapreduce/output
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="序列化"><a class="header" href="#序列化">序列化</a></h1>
<ul>
<li><a href="%E5%BA%8F%E5%88%97%E5%8C%96.html#21-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A6%82%E8%BF%B0">2.1 序列化概述</a></li>
<li><a href="%E5%BA%8F%E5%88%97%E5%8C%96.html#22-%E8%87%AA%E5%AE%9A%E4%B9%89-bean-%E5%AF%B9%E8%B1%A1%E5%AE%9E%E7%8E%B0%E5%BA%8F%E5%88%97%E5%8C%96%E6%8E%A5%E5%8F%A3writable">2.2 自定义 bean 对象实现序列化接口（Writable）</a></li>
<li><a href="%E5%BA%8F%E5%88%97%E5%8C%96.html#23-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">2.3 序列化案例实操</a></li>
</ul>
<h2 id="21-序列化概述"><a class="header" href="#21-序列化概述">2.1 序列化概述</a></h2>
<p>1）<strong>什么是序列化</strong></p>
<p>​ <strong>序列化</strong>就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。</p>
<p>​ <strong>反序列化</strong>就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</p>
<p>2）<strong>为什么要序列化</strong></p>
<p>​ 一般来说，“活的” 对象只生存在内存里，关机断电就没有了。而且 “活的” 对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储 “活的” 对象，可以将 “活的” 对象发送到远程计算机。</p>
<p>3）为什么不用 Java 的序列化</p>
<p>​ Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop 自己开发了一套序列化机制（Writable）。</p>
<p>4）Hadoop 序列化特点：</p>
<p><strong>（1</strong>）<strong>紧凑</strong>：高效使用存储空间。</p>
<p><strong>（2</strong>）快速：读写数据的额外开销小。</p>
<p><strong>（3</strong>）互操作：支持多语言的交互</p>
<h2 id="22-自定义-bean-对象实现序列化接口writable"><a class="header" href="#22-自定义-bean-对象实现序列化接口writable">2.2 自定义 bean 对象实现序列化接口（Writable）</a></h2>
<p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在 Hadoop 框架内部传递一个 bean 对象，那么该对象就需要实现序列化接口。</p>
<p>具体实现 bean 对象序列化步骤如下 7 步。</p>
<p>（1）必须实现 Writable 接口</p>
<p>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造</p>
<pre><code class="language-java">public FlowBean () {
	super ();
}
</code></pre>
<p>（3）重写序列化方法</p>
<pre><code class="language-java">@Override
public void write (DataOutput out) throws IOException {
	out.writeLong (upFlow);
	out.writeLong (downFlow);
	out.writeLong (sumFlow);
}
</code></pre>
<p>（4）重写反序列化方法</p>
<pre><code class="language-java">@Override
public void readFields (DataInput in) throws IOException {
	upFlow = in.readLong ();
	downFlow = in.readLong ();
	sumFlow = in.readLong ();
}

</code></pre>
<p>（5）<strong>注意</strong>反序列化的顺序和序列化的顺序完全一致</p>
<p>（6）要想把结果显示在文件中，需要重写 toString ()，可用”<code>\t</code>” 分开，方便后续用。</p>
<p>（7）如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口，因为 MapReduce 框中的 Shuffle 过程要求对 key 必须能排序。详见后面排序案例。</p>
<pre><code class="language-java">@Override
public int compareTo (FlowBean o) {
	// 倒序排列，从大到小
	return this.sumFlow &gt; o.getSumFlow () ? -1 : 1;
}
</code></pre>
<h2 id="23-序列化案例实操"><a class="header" href="#23-序列化案例实操">2.3 序列化案例实操</a></h2>
<p><strong>1</strong>）需求</p>
<p>统计每一个手机号耗费的总上行流量、总下行流量、总流量</p>
<p>（1）输入数据</p>
<pre><code>vim phone_data.txt
</code></pre>
<p>相应数据为：</p>
<pre><code>1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
</code></pre>
<p>（2）输入数据格式：</p>
<pre><code>7    13560436666   120.196.100.99      1116       954           200 
id   手机号码      网络 ip             上行流量  下行流量   网络状态码  
</code></pre>
<p>（3）期望输出数据格式</p>
<pre><code>13560436666         1116         954               2070  
手机号码        上行流量     下行流量          总流量  
</code></pre>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241105160111.png" alt="" /></p>
<p><strong>3</strong>）编写 MapReduce 程序</p>
<p>（1）编写流量统计的 Bean 对象</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writable;

import org.apache.hadoop.io.Writable;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

//1 继承 Writable 接口
public class FlowBean implements Writable {

    private long upFlow; // 上行流量
    private long downFlow; // 下行流量
    private long sumFlow; // 总流量

    //2 提供无参构造
    public FlowBean () {
    }

    //3 提供三个参数的 getter 和 setter 方法
    public long getUpFlow () {
        return upFlow;
    }

    public void setUpFlow (long upFlow) {
        this.upFlow = upFlow;
    }

    public long getDownFlow () {
        return downFlow;
    }

    public void setDownFlow (long downFlow) {
        this.downFlow = downFlow;
    }

    public long getSumFlow () {
        return sumFlow;
    }

    public void setSumFlow (long sumFlow) {
        this.sumFlow = sumFlow;
    }

    public void setSumFlow () {
        this.sumFlow = this.upFlow + this.downFlow;
    }

    //4 实现序列化和反序列化方法，注意顺序一定要保持一致
    @Override
    public void write (DataOutput dataOutput) throws IOException {
        dataOutput.writeLong (upFlow);
        dataOutput.writeLong (downFlow);
        dataOutput.writeLong (sumFlow);
    }

    @Override
    public void readFields (DataInput dataInput) throws IOException {
        this.upFlow = dataInput.readLong ();
        this.downFlow = dataInput.readLong ();
        this.sumFlow = dataInput.readLong ();
    }

    //5 重写 ToString
    @Override
    public String toString () {
        return upFlow + "\t" + downFlow + "\t" + sumFlow;
    }
}

</code></pre>
<p>（2）编写 Mapper 类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writable;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

public class FlowMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; {
    private Text outK = new Text ();
    private FlowBean outV = new FlowBean ();

    @Override
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {

        //1 获取一行数据，转成字符串
        String line = value.toString ();

        //2 切割数据
        String [] split = line.split ("\t");

        //3 抓取我们需要的数据：手机号，上行流量，下行流量
        String phone = split [1];
        String up = split [split.length - 3];
        String down = split [split.length - 2];

        //4 封装 outK outV
        outK.set (phone);
        outV.setUpFlow (Long.parseLong (up));
        outV.setDownFlow (Long.parseLong (down));
        outV.setSumFlow ();

        //5 写出 outK outV
        context.write (outK, outV);
    }
}

</code></pre>
<p>（3）编写 Reducer 类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writable;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

public class FlowReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; {
    private FlowBean outV = new FlowBean ();
    @Override
    protected void reduce (Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException {

        long totalUp = 0;
        long totalDown = 0;

        //1 遍历 values, 将其中的上行流量，下行流量分别累加
        for (FlowBean flowBean : values) {
            totalUp += flowBean.getUpFlow ();
            totalDown += flowBean.getDownFlow ();
        }

        //2 封装 outKV
        outV.setUpFlow (totalUp);
        outV.setDownFlow (totalDown);
        outV.setSumFlow ();

        //3 写出 outK outV
        context.write (key,outV);
    }
}
</code></pre>
<p>（4）编写 Driver 驱动类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writable;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

public class FlowDriver {
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {

        //1 获取 job 对象
        Configuration conf = new Configuration ();
        Job job = Job.getInstance (conf);

        //2 关联本 Driver 类
        job.setJarByClass (FlowDriver.class);

        //3 关联 Mapper 和 Reducer
        job.setMapperClass (FlowMapper.class);
        job.setReducerClass (FlowReducer.class);
        
		//4 设置 Map 端输出 KV 类型
        job.setMapOutputKeyClass (Text.class);
        job.setMapOutputValueClass (FlowBean.class);
        
		//5 设置程序最终输出的 KV 类型
        job.setOutputKeyClass (Text.class);
        job.setOutputValueClass (FlowBean.class);
        
		//6 设置程序的输入输出路径
        FileInputFormat.setInputPaths (job, new Path ("D:\\inputflow"));
        FileOutputFormat.setOutputPath (job, new Path ("D:\\flowoutput"));
        
		//7 提交 Job
        boolean b = job.waitForCompletion (true);
        System.exit (b ? 0 : 1);
    }
}
</code></pre>
<p>4）输出结果</p>
<pre><code>13470253144	180	180	360
13509468723	7335	110349	117684
13560439638	918	4938	5856
13568436656	3597	25635	29232
13590439668	1116	954	2070
13630577991	6960	690	7650
13682846555	1938	2910	4848
13729199489	240	0	240
13736230513	2481	24681	27162
13768778790	120	120	240
13846544121	264	0	264
13956435636	132	1512	1644
13966251146	240	0	240
13975057813	11058	48243	59301
13992314666	3008	3720	6728
15043685818	3659	3538	7197
15910133277	3156	2936	6092
15959002129	1938	180	2118
18271575951	1527	2106	3633
18390173782	9531	2412	11943
84188413	4116	1432	5548
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="mapreduce框架原理"><a class="header" href="#mapreduce框架原理">MapReduce框架原理</a></h1>
<ul>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#31-inputformat-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5">3.1 InputFormat 数据输入</a>
<ul>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#311-%E5%88%87%E7%89%87%E4%B8%8E-maptask-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6">3.1.1 切片与 MapTask 并行度决定机制</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#312-job-%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%92%8C%E5%88%87%E7%89%87%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3">3.1.2 Job 提交流程源码和切片源码详解</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#313-fileinputformat-%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6">3.1.3 FileInputFormat 切片机制</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#314-textinputformat">3.1.4 TextInputFormat</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#315-combinetextinputformat-%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6">3.1.5 CombineTextInputFormat 切片机制</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#316-combinetextinputformat-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.1.6 CombineTextInputFormat 案例实操</a></li>
</ul>
</li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#32-mapreduce-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B">3.2 MapReduce 工作流程</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#33-shuffle-%E6%9C%BA%E5%88%B6">3.3 Shuffle 机制</a>
<ul>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#331-shuffle-%E6%9C%BA%E5%88%B6">3.3.1 Shuffle 机制</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#332-partition-%E5%88%86%E5%8C%BA">3.3.2 Partition 分区</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#333-partition-%E5%88%86%E5%8C%BA%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.3.3 Partition 分区案例实操</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#334-writablecomparable-%E6%8E%92%E5%BA%8F">3.3.4 WritableComparable 排序</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#335-writablecomparable-%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E5%85%A8%E6%8E%92%E5%BA%8F">3.3.5 WritableComparable 排序案例实操（全排序）</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#336-writablecomparable-%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E5%8C%BA%E5%86%85%E6%8E%92%E5%BA%8F">3.3.6 WritableComparable 排序案例实操（区内排序）</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#337-combiner-%E5%90%88%E5%B9%B6">3.3.7 Combiner 合并</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#338-combiner-%E5%90%88%E5%B9%B6%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.3.8 Combiner 合并案例实操</a></li>
</ul>
</li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#34-outputformat-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA">3.4 OutputFormat 数据输出</a>
<ul>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#341-outputformat-%E6%8E%A5%E5%8F%A3%E5%AE%9E%E7%8E%B0%E7%B1%BB">3.4.1 OutputFormat 接口实现类</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#342-%E8%87%AA%E5%AE%9A%E4%B9%89-outputformat-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.4.2 自定义 OutputFormat 案例实操</a></li>
</ul>
</li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#35-mapreduce-%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90">3.5 MapReduce 内核源码解析</a>
<ul>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#351-maptask-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">3.5.1 MapTask 工作机制</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#352-reducetask-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">3.5.2 ReduceTask 工作机制</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#353-reducetask-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6">3.5.3 ReduceTask 并行度决定机制</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#354-maptask--reducetask-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90">3.5.4 MapTask &amp; ReduceTask 源码解析</a></li>
</ul>
</li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#36-join-%E5%BA%94%E7%94%A8">3.6 Join 应用</a>
<ul>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#361-reduce-join">3.6.1 Reduce Join</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#362-reduce-join-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.6.2 Reduce Join 案例实操</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#363-map-join">3.6.3 Map Join</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#364-map-join-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.6.4 Map Join 案例实操</a></li>
</ul>
</li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C">程序运行结果</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#37-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97etl">3.7 数据清洗（ETL）</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E9%99%84%E5%BD%95etl-%E6%B8%85%E6%B4%97%E8%A7%84%E5%88%99">附录：ETL 清洗规则</a>
<ul>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E9%97%AE%E9%A2%98">解决数据质量问题</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E4%BE%9B%E5%BA%94%E7%AE%97%E6%B3%95%E5%8E%9F%E6%96%99">供应算法原料</a></li>
</ul>
</li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E9%99%84%E5%BD%95%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F">附录常用正则表达式</a>
<ul>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E4%B8%80%E6%A0%A1%E9%AA%8C%E6%95%B0%E5%AD%97%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F">一、校验数字的表达式</a></li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E4%BA%8C%E6%A0%A1%E9%AA%8C%E5%AD%97%E7%AC%A6%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F">二、校验字符的表达式</a></li>
</ul>
</li>
<li><a href="MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#38-mapreduce-%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93">3.8 MapReduce 开发总结</a></li>
</ul>
<p>​<img src="https://image.3001.net/images/20221106/1667741555906.png" alt="image-20221106213218443" /></p>
<h2 id="31-inputformat-数据输入"><a class="header" href="#31-inputformat-数据输入">3.1 InputFormat 数据输入</a></h2>
<h3 id="311-切片与-maptask-并行度决定机制"><a class="header" href="#311-切片与-maptask-并行度决定机制">3.1.1 切片与 MapTask 并行度决定机制</a></h3>
<p><strong>1</strong>）问题引出</p>
<p>​MapTask 的并行度决定 Map 阶段的任务处理并发度，进而影响到整个 Job 的处理速度。</p>
<p>​思考：1G 的数据，启动 8 个 MapTask，可以提高集群的并发处理能力。那么 1K 的数据，也启动 8 个 MapTask，会提高集群性能吗？MapTask 并行任务是否越多越好呢？哪些因素影响了 MapTask 并行度？</p>
<p><strong>2</strong>）MapTask 并行度决定机制</p>
<p>​ <strong>数据块：</strong> Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。</p>
<p>​ <strong>数据切片：</strong> 数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</p>
<p><img src="https://image.3001.net/images/20221106/16677415962785.png" alt="image-20221106213259161" /></p>
<h3 id="312-job-提交流程源码和切片源码详解"><a class="header" href="#312-job-提交流程源码和切片源码详解">3.1.2 Job 提交流程源码和切片源码详解</a></h3>
<p>源码阅读三大要点：job.xml、xxx.jar、job.split</p>
<p>1）Job 提交流程源码详解</p>
<pre><code class="language-java">waitForCompletion ()

submit ();

// 1 建立连接
connect ();	
	// 1）创建提交 Job 的代理
	new Cluster (getConfiguration ());
		// （1）判断是本地运行环境还是 yarn 集群运行环境
		initialize (jobTrackAddr, conf); 

	// 2 提交 job
	submitter.submitJobInternal (Job.this, cluster)

// 1）创建给集群提交数据的 Stag 路径
Path jobStagingArea = JobSubmissionFiles.getStagingDir (cluster, conf);

// 2）获取 jobid ，并创建 Job 路径
JobID jobId = submitClient.getNewJobID ();

// 3）拷贝 jar 包到集群
copyAndConfigureFiles (job, submitJobDir);	
rUploader.uploadFiles (job, jobSubmitDir);

// 4）计算切片，生成切片规划文件
writeSplits (job, submitJobDir);
	maps = writeNewSplits (job, jobSubmitDir);
	input.getSplits (job);

// 5）向 Stag 路径写 XML 配置文件
writeConf (conf, submitJobFile);
conf.writeXml (out);

// 6）提交 Job, 返回提交状态
status = submitClient.submitJob (jobId, submitJobDir.toString (), job.getCredentials ());

</code></pre>
<p><img src="https://image.3001.net/images/20221106/16677416364405.png" alt="image-20221106213339571" /></p>
<p><strong>2</strong>）FileInputFormat 切片源码解析（input.getSplits (job)）</p>
<p><img src="https://image.3001.net/images/20221106/16677416493309.png" alt="image-20221106213352169" /></p>
<h3 id="313-fileinputformat-切片机制"><a class="header" href="#313-fileinputformat-切片机制">3.1.3 FileInputFormat 切片机制</a></h3>
<p><img src="https://image.3001.net/images/20221106/16677417044403.png" alt="image-20221106213447314" /></p>
<p><img src="https://image.3001.net/images/20221106/16677417312383.png" alt="image-20221106213514220" /></p>
<h3 id="314-textinputformat"><a class="header" href="#314-textinputformat">3.1.4 TextInputFormat</a></h3>
<p><strong>1</strong>）FileInputFormat 实现类</p>
<p>​思考：在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。那么，针对不同的数据类型，MapReduce 是如何读取这些数据的呢？</p>
<p>​FileInputFormat 常见的接口实现类包括：<strong>TextInputFormat</strong>、KeyValueTextInputFormat、NLineInputFormat、<strong>CombineTextInputFormat</strong>和自定义 InputFormat 等。</p>
<p><strong>2</strong>）TextInputFormat</p>
<p>​TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable 类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text 类型。</p>
<p>​以下是一个示例，比如，一个分片包含了如下 4 条文本记录。</p>
<pre><code>Rich learning form
Intelligent learning engine
Learning more convenient
From the real demand for more close to the enterprise
</code></pre>
<p>每条记录表示为以下键 / 值对：</p>
<pre><code>(0,Rich learning form)
(20,Intelligent learning engine)
(49,Learning more convenient)
(74,From the real demand for more close to the enterprise)
</code></pre>
<h3 id="315-combinetextinputformat-切片机制"><a class="header" href="#315-combinetextinputformat-切片机制">3.1.5 CombineTextInputFormat 切片机制</a></h3>
<p>框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask，这样如果有大量小文件，就会产生大量的 MapTask，处理效率极其低下。</p>
<p><strong>1</strong>）应用场景：</p>
<p>​<code>CombineTextInputFormat</code> 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。</p>
<p><strong>2</strong>）虚拟存储切片最大值设置</p>
<p><code>​CombineTextInputFormat.setMaxInputSplitSize (job, 4194304);// 4m</code></p>
<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
<p><strong>3</strong>）切片机制</p>
<p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>
<p><img src="https://image.3001.net/images/20221110/1668056403686.png" alt="image-20221110125959434" /></p>
<p>（1）虚拟存储过程：</p>
<p>​ 将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；<strong>当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片）。</strong></p>
<p>​ 例如 setMaxInputSplitSize 值为 4M，输入文件大小为 8.02M，则先逻辑上分成一个 4M。剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储文件，所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件。</p>
<p>（2）切片过程：</p>
<p>（a）判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独形成一个切片。</p>
<p>（b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</p>
<p>（c）测试举例：有 4 个小文件大小分别为 1.7M、5.1M、3.4M 以及 6.8M 这四个小文件，则虚拟存储之后形成 6 个文件块，大小分别为：</p>
<p>1.7M，（2.55M、2.55M），3.4M 以及（3.4M、3.4M）</p>
<p>最终会形成 3 个切片，大小分别为：</p>
<p>（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</p>
<h3 id="316-combinetextinputformat-案例实操"><a class="header" href="#316-combinetextinputformat-案例实操">3.1.6 CombineTextInputFormat 案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>将输入的大量小文件合并成一个切片统一处理。</p>
<p>（1）输入数据</p>
<p>准备 4 个小文件</p>
<p>（2）期望</p>
<p>期望一个切片处理 4 个文件</p>
<p><strong>2</strong>）实现过程</p>
<p>（1）不做任何处理，运行 1.8 节的 WordCount 案例程序，观察切片个数为 4。</p>
<pre><code>number of splits:4
</code></pre>
<p>（2）在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 3。</p>
<p>（a）驱动类中添加代码如下：</p>
<pre><code class="language-java">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.class
job.setInputFormatClass (CombineTextInputFormat.class);

// 虚拟存储切片最大值设置 4m
CombineTextInputFormat.setMaxInputSplitSize (job, 4194304);
</code></pre>
<p>（b）运行如果为 3 个切片。</p>
<pre><code>number of splits:3
</code></pre>
<p>（3）在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 1。</p>
<p>（a）驱动中添加代码如下：</p>
<pre><code class="language-java">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.class
job.setInputFormatClass (CombineTextInputFormat.class);

// 虚拟存储切片最大值设置 20m
CombineTextInputFormat.setMaxInputSplitSize (job, 20971520);
</code></pre>
<p>（b）运行如果为 1 个切片</p>
<pre><code>number of splits:1
</code></pre>
<h2 id="32-mapreduce-工作流程"><a class="header" href="#32-mapreduce-工作流程">3.2 MapReduce 工作流程</a></h2>
<p><img src="https://image.3001.net/images/20221110/16680566693739.png" alt="image-20221110130426443" /></p>
<p><img src="https://image.3001.net/images/20221110/16680566799674.png" alt="image-20221110130436072" /></p>
<p>​上面的流程是整个 MapReduce 最全工作流程，但是 Shuffle 过程只是从第 7 步开始到第 16 步结束，具体 Shuffle 过程详解，如下：</p>
<p>（1）MapTask 收集我们的 map () 方法输出的 kv 对，放到内存缓冲区中</p>
<p>（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p>
<p>（3）多个溢出文件会被合并成大的溢出文件</p>
<p>（4）在溢出过程及合并的过程中，都要调用 Partitioner 进行分区和针对 key 进行排序</p>
<p>（5）ReduceTask 根据自己的分区号，去各个 MapTask 机器上取相应的结果分区数据</p>
<p>（6）ReduceTask 会抓取到同一个分区的来自不同 MapTask 的结果文件，ReduceTask 会将这些文件再进行合并（归并排序）</p>
<p>（7）合并成大文件后，Shuffle 的过程也就结束了，后面进入 ReduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 Group，调用用户自定义的 reduce () 方法）</p>
<p><strong>注意：</strong></p>
<p>（1）Shuffle 中的缓冲区大小会影响到 MapReduce 程序的执行效率，原则上说，缓冲区越大，磁盘 IO 的次数越少，执行速度就越快。</p>
<p>（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认 100M。</p>
<h2 id="33-shuffle-机制"><a class="header" href="#33-shuffle-机制">3.3 Shuffle 机制</a></h2>
<h3 id="331-shuffle-机制"><a class="header" href="#331-shuffle-机制">3.3.1 Shuffle 机制</a></h3>
<p>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。</p>
<p><img src="https://image.3001.net/images/20221110/1668056711326.png" alt="image-20221110130507683" /></p>
<p>对 key 的索引按照字典顺序快速排序</p>
<p>分组格式：{key，（value1， value2， ……）}</p>
<h3 id="332-partition-分区"><a class="header" href="#332-partition-分区">3.3.2 Partition 分区</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680567231755.png" alt="image-20221110130520540" /></p>
<p>在 Driver 中添加 <code>job.setNumReduceTasks (2);</code>，然后 DeBug 逐步查找下面的代码：</p>
<pre><code class="language-java">public void write (K key, V value) throws IOException, InterruptedException {  
    this.collector.collect (key, value, this.partitioner.getPartition (key, value, this.partitions));  
}
</code></pre>
<pre><code class="language-java">//  
// Source code recreated from a .class file by IntelliJ IDEA  
// (powered by FernFlower decompiler)  
//  
  
package org.apache.hadoop.mapreduce.lib.partition;  
  
import org.apache.hadoop.classification.InterfaceAudience.Public;  
import org.apache.hadoop.classification.InterfaceStability.Stable;  
import org.apache.hadoop.mapreduce.Partitioner;  
  
@Public  
@Stable  
public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; {  
    public HashPartitioner () {  
    }  
  
    public int getPartition (K key, V value, int numReduceTasks) {  
        return (key.hashCode () &amp; Integer.MAX_VALUE) % numReduceTasks;  
    }  
}
</code></pre>
<p>在 Driver 中不设置 <code>job.setNumReduceTasks (2);</code>，然后 DeBug 逐步查找下面的代码：</p>
<pre><code class="language-java">NewOutputCollector (JobContext jobContext, JobConf job, TaskUmbilicalProtocol umbilical, Task.TaskReporter reporter) throws IOException, ClassNotFoundException {  
    this.collector = MapTask.this.createSortingCollector (job, reporter);  
    this.partitions = jobContext.getNumReduceTasks ();  
    if (this.partitions &gt; 1) {  
        this.partitioner = (Partitioner) ReflectionUtils.newInstance (jobContext.getPartitionerClass (), job);  
    } else {  
        this.partitioner = new Partitioner&lt;K, V&gt;() {  
            public int getPartition (K key, V value, int numPartitions) {  
                return NewOutputCollector.this.partitions - 1;  
            }  
        };  
    }  
  
}  
  
public void write (K key, V value) throws IOException, InterruptedException {  
    this.collector.collect (key, value, this.partitioner.getPartition (key, value, this.partitions));  
}
</code></pre>
<p><code>new Partitioner&lt;K, V&gt;(){}</code> 匿名内部类，<code>job.setNumReduceTasks</code> 默认是 1，只生成一个 0 号分区</p>
<p><img src="https://image.3001.net/images/20221110/16680567322654.png" alt="image-20221110130528974" /></p>
<p><img src="https://image.3001.net/images/20221110/16680567452392.png" alt="image-20221110130542610" /></p>
<h3 id="333-partition-分区案例实操"><a class="header" href="#333-partition-分区案例实操">3.3.3 Partition 分区案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
<p>（1）输入数据</p>
<p>​<code>phone_data.txt</code></p>
<pre><code>1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
</code></pre>
<p>（2）期望输出数据</p>
<p>​ 手机号 136、137、138、139 开头都分别放到一个独立的 4 个文件中，其他开头的放到一个文件中。</p>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://image.3001.net/images/20221110/1668056811486.png" alt="image-20221110130647783" /></p>
<p><strong>3</strong>）在案例<strong>2.3</strong>的基础上，增加一个分区类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.partitioner;  
  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Partitioner;  
  
public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; {  
  
    @Override  
    public int getPartition (Text text, FlowBean flowBean, int numPartitions) {  
        // 获取手机号前三位 prePhone  
        String phone = text.toString ();  
        String prePhone = phone.substring (0, 3);  
  
        // 定义一个分区号变量 partition, 根据 prePhone 设置分区号  
        int partition;  
  
        if ("136".equals (prePhone)){  
            partition = 0;  
        } else if ("137".equals (prePhone)){  
            partition = 1;  
        } else if ("138".equals (prePhone)){  
            partition = 2;  
        } else if ("139".equals (prePhone)){  
            partition = 3;  
        } else {  
            partition = 4;  
        }  
  
        // 最后返回分区号 partition  
        return partition;  
    }  
}
</code></pre>
<p>改进版写法：</p>
<pre><code class="language-java">package com.TianHan.mapreduce.partitioner2;  
  
import com.TianHan.mapreduce.partitioner2.FlowBean;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Partitioner;  
  
public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; {  
  
    @Override  
    public int getPartition (Text text, FlowBean flowBean, int numPartitions) {  
        // 获取手机号前三位 prePhone  
        String phone = text.toString ();  
        String prePhone = phone.substring (0, 3);  
  
        // 定义一个分区号变量 partition, 根据 prePhone 设置分区号  
  
        // 最后返回分区号 partition  
        return switch (prePhone) {  
            case "136" -&gt; 0;  
            case "137" -&gt; 1;  
            case "138" -&gt; 2;  
            case "139" -&gt; 3;  
            default -&gt; 4;  
        };  
    }  
}
</code></pre>
<p><strong>4</strong>）在驱动函数中增加自定义数据分区设置和 ReduceTask 设置</p>
<pre><code class="language-java">package com.TianHan.mapreduce.partitioner2;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import java.io.IOException;  
  
public class FlowDriver {  
  
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {  
  
        //1 获取 job 对象    
		Configuration conf = new Configuration ();  
        Job job = Job.getInstance (conf);  
  
        //2 关联本 Driver 类    
		job.setJarByClass (FlowDriver.class);  
  
        //3 关联 Mapper 和 Reducer    
		job.setMapperClass (FlowMapper.class);  
        job.setReducerClass (FlowReducer.class);  
  
        //4 设置 Map 端输出数据的 KV 类型    
		job.setMapOutputKeyClass (Text.class);  
        job.setMapOutputValueClass (FlowBean.class);  
  
        //5 设置程序最终输出的 KV 类型    
		job.setOutputKeyClass (Text.class);  
        job.setOutputValueClass (FlowBean.class);  
  
        //8 指定自定义分区器    
		job.setPartitionerClass (ProvincePartitioner.class);  
  
        //9 同时指定相应数量的 ReduceTask    
		job.setNumReduceTasks (5);  
  
        //6 设置输入输出路径    
		FileInputFormat.setInputPaths (job, new Path ("E:\\BigData\\hadoop\\input"));  
        FileOutputFormat.setOutputPath (job, new Path ("E:\\BigData\\hadoop\\output2"));  
  
        //7 提交 Job    
		boolean b = job.waitForCompletion (true);  
        System.exit (b ? 0 : 1);  
    }  
}
</code></pre>
<p>指定自定义分区器 <code>job.setPartitionerClass (ProvincePartitioner.class);</code></p>
<p>同时指定相应数量的 ReduceTask<code>job.setNumReduceTasks (5);</code></p>
<p>自定义分区器和 ReduceTask 数量要相等，否则可能报错 <code>Illegal partition ..</code>。</p>
<p>设置为 1 即 <code>job.setNumReduceTasks (1);</code> 走默认 partioner 方法，不会报错，但是无法达到想要的效果。</p>
<pre><code class="language-java">this.partitioner = new Partitioner&lt;K, V&gt;() {  
    public int getPartition (K key, V value, int numPartitions) {  
        return NewOutputCollector.this.partitions - 1;  
    }  
};
</code></pre>
<p>设置为 2 或 3 或 4 可能报错 <code>Illegal partition ..</code>。</p>
<p>设置为 5 刚刚好。</p>
<p>设置为 6 等大于 5 的数时不报错，但是多余文件中内容为空。</p>
<h3 id="334-writablecomparable-排序"><a class="header" href="#334-writablecomparable-排序">3.3.4 WritableComparable 排序</a></h3>
<p><img src="https://image.3001.net/images/20221110/1668056910753.png" alt="image-20221110130827331" /></p>
<p><img src="https://image.3001.net/images/20221110/1668056922989.png" alt="image-20221110130839547" /></p>
<p><img src="https://image.3001.net/images/20221110/16680569339822.png" alt="image-20221110130850481" /></p>
<p>自定义排序<strong>WritableComparable</strong>原理分析</p>
<p>​bean 对象做为 key 传输，需要实现 WritableComparable 接口重写 compareTo 方法，就可以实现排序。</p>
<pre><code class="language-java">@Override  
public int compareTo (FlowBean bean) {  
  
	int result;  
		  
	// 按照总流量大小，倒序排列  
	if (this.sumFlow &gt; bean.getSumFlow ()) {  
		result = -1;  
	} else if (this.sumFlow &lt; bean.getSumFlow ()) {  
		result = 1;  
	} else {  
		result = 0;  
	}  
  
	return result;  
}
</code></pre>
<h3 id="335-writablecomparable-排序案例实操全排序"><a class="header" href="#335-writablecomparable-排序案例实操全排序">3.3.5 WritableComparable 排序案例实操（全排序）</a></h3>
<p><strong>1</strong>）需求</p>
<p>根据案例 2.3 序列化案例产生的结果再次对总流量进行倒序排序。</p>
<p>（1）输入数据</p>
<p>​<code>phone_data.txt</code></p>
<pre><code>1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
</code></pre>
<p>第一次处理后的数据中只保留下面这个文件：</p>
<pre><code>part-r-00000
</code></pre>
<p>（2）期望输出数据</p>
<pre><code>13509468723 7335 110349 117684
13736230513 2481 24681 27162
13956435636 132 1512 1644
13846544121 264 0 264
。。。 。。。
</code></pre>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://image.3001.net/images/20221110/1668057054897.png" alt="image-20221110131050892" /></p>
<p><strong>3</strong>）代码实现</p>
<p>（1）FlowBean 对象在在需求 1 基础上增加了比较功能</p>
<pre><code class="language-java">import org.apache.hadoop.io.WritableComparable;  
import java.io.DataInput;  
import java.io.DataOutput;  
import java.io.IOException;  
  
public class FlowBean implements WritableComparable&lt;FlowBean&gt; {  
  
    private long upFlow; // 上行流量  
    private long downFlow; // 下行流量  
    private long sumFlow; // 总流量  
  
    // 提供无参构造  
    public FlowBean () {  
    }  
  
    // 生成三个属性的 getter 和 setter 方法  
    public long getUpFlow () {  
        return upFlow;  
    }  
  
    public void setUpFlow (long upFlow) {  
        this.upFlow = upFlow;  
    }  
  
    public long getDownFlow () {  
        return downFlow;  
    }  
  
    public void setDownFlow (long downFlow) {  
        this.downFlow = downFlow;  
    }  
  
    public long getSumFlow () {  
        return sumFlow;  
    }  
  
    public void setSumFlow (long sumFlow) {  
        this.sumFlow = sumFlow;  
    }  
  
    public void setSumFlow () {  
        this.sumFlow = this.upFlow + this.downFlow;  
    }  
  
    // 实现序列化和反序列化方法，注意顺序一定要一致  
    @Override  
    public void write (DataOutput out) throws IOException {  
        out.writeLong (this.upFlow);  
        out.writeLong (this.downFlow);  
        out.writeLong (this.sumFlow);  
  
    }  
  
    @Override  
    public void readFields (DataInput in) throws IOException {  
        this.upFlow = in.readLong ();  
        this.downFlow = in.readLong ();  
        this.sumFlow = in.readLong ();  
    }  
  
    // 重写 ToString, 最后要输出 FlowBean  
    @Override  
    public String toString () {  
        return upFlow + "\t" + downFlow + "\t" + sumFlow;  
    }  
  
    @Override  
    public int compareTo (FlowBean o) {  
  
        // 按照总流量比较，倒序排列  
        if (this.sumFlow &gt; o.sumFlow){  
            return -1;  
        } else if (this.sumFlow &lt; o.sumFlow){  
            return 1;  
        } else {  
            return 0;  
        }  
    }  
}
</code></pre>
<p>（2）编写 Mapper 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
import java.io.IOException;  
  
public class FlowMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; {  
    private FlowBean outK = new FlowBean ();  
    private Text outV = new Text ();  
  
    @Override  
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
  
        //1 获取一行数据  
        String line = value.toString ();  
  
        //2 按照 "\t", 切割数据  
        String [] split = line.split ("\t");  
  
        //3 封装 outK outV  
        outK.setUpFlow (Long.parseLong (split [1]));  
        outK.setDownFlow (Long.parseLong (split [2]));  
        outK.setSumFlow ();  
        outV.set (split [0]);  
  
        //4 写出 outK outV  
        context.write (outK,outV);  
    }  
}
</code></pre>
<p>（3）编写 Reducer 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
import java.io.IOException;  
  
public class FlowReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; {  
    @Override  
    protected void reduce (FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {  
  
        // 遍历 values 集合，循环写出，避免总流量相同的情况  
        for (Text value : values) {  
            // 调换 KV 位置，反向写出  
            context.write (value,key);  
        }  
    }  
}
</code></pre>
<p>总流量相同（key 相同）的键值对同时进入 reduce 方法。</p>
<p>（4）编写 Driver 类</p>
<pre><code class="language-java">import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import java.io.IOException;  
  
public class FlowDriver {  
  
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {  
  
        //1 获取 job 对象  
        Configuration conf = new Configuration ();  
        Job job = Job.getInstance (conf);  
  
        //2 关联本 Driver 类  
        job.setJarByClass (FlowDriver.class);  
  
        //3 关联 Mapper 和 Reducer  
        job.setMapperClass (FlowMapper.class);  
        job.setReducerClass (FlowReducer.class);  
  
        //4 设置 Map 端输出数据的 KV 类型  
        job.setMapOutputKeyClass (FlowBean.class);  
        job.setMapOutputValueClass (Text.class);  
  
        //5 设置程序最终输出的 KV 类型  
        job.setOutputKeyClass (Text.class);  
        job.setOutputValueClass (FlowBean.class);  
  
        //6 设置输入输出路径  
        FileInputFormat.setInputPaths (job, new Path ("D:\\inputflow2"));  
        FileOutputFormat.setOutputPath (job, new Path ("D:\\comparout"));  
  
        //7 提交 Job  
        boolean b = job.waitForCompletion (true);  
        System.exit (b ? 0 : 1);  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>13509468723	7335	110349	117684
13975057813	11058	48243	59301
13568436656	3597	25635	29232
13736230513	2481	24681	27162
18390173782	9531	2412	11943
13630577991	6960	690	7650
15043685818	3659	3538	7197
13992314666	3008	3720	6728
15910133277	3156	2936	6092
13560439638	918	4938	5856
84188413	4116	1432	5548
13682846555	1938	2910	4848
18271575951	1527	2106	3633
15959002129	1938	180	2118
13590439668	1116	954	2070
13956435636	132	1512	1644
13470253144	180	180	360
13846544121	264	0	264
13729199489	240	0	240
13768778790	120	120	240
13966251146	240	0	240
</code></pre>
<p>二次排序案例（总流量相同则按照上行流量升序排列）</p>
<p>修改 FlowBean.java 文件中的比较方式：</p>
<pre><code class="language-java">@Override  
public int compareTo (FlowBean o) {  
    if (this.sumFlow &gt; o.sumFlow) {  
        return -1;  
    } else if (this.sumFlow &lt; o.sumFlow) {  
        return 1;  
    } else {  
        if (this.upFlow &gt; o.upFlow){  
            return 1;  
        } else if (this.upFlow &lt; o.upFlow){  
            return -1;  
        } else {  
            return 0;  
        }  
    }  
}
</code></pre>
<p>简单写法：</p>
<pre><code class="language-java">@Override  
public int compareTo (FlowBean o) {  
    if (this.sumFlow &gt; o.sumFlow) {  
        return -1;  
    } else if (this.sumFlow &lt; o.sumFlow) {  
        return 1;  
    } else {  
        return this.upFlow.compareTo (o.upFlow);  
    }  
}
</code></pre>
<p>输出效果如下所示：</p>
<pre><code>13509468723	7335	110349	117684
13975057813	11058	48243	59301
13568436656	3597	25635	29232
13736230513	2481	24681	27162
18390173782	9531	2412	11943
13630577991	6960	690	7650
15043685818	3659	3538	7197
13992314666	3008	3720	6728
15910133277	3156	2936	6092
13560439638	918	4938	5856
84188413	4116	1432	5548
13682846555	1938	2910	4848
18271575951	1527	2106	3633
15959002129	1938	180	2118
13590439668	1116	954	2070
13956435636	132	1512	1644
13470253144	180	180	360
13846544121	264	0	264
13768778790	120	120	240
13729199489	240	0	240
13966251146	240	0	240
</code></pre>
<h3 id="336-writablecomparable-排序案例实操区内排序"><a class="header" href="#336-writablecomparable-排序案例实操区内排序">3.3.6 WritableComparable 排序案例实操（区内排序）</a></h3>
<p><strong>1</strong>）需求</p>
<p>要求每个省份手机号输出的文件中按照总流量内部排序。</p>
<p><strong>2</strong>）需求分析</p>
<p>​ 基于前一个需求，增加自定义分区类，分区按照省份手机号设置。</p>
<p><img src="https://image.3001.net/images/20221110/16680571604100.png" alt="image-20221110131236743" /></p>
<p><strong>3</strong>）案例实操</p>
<p>（1）增加自定义分区类</p>
<pre><code class="language-java">import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Partitioner;  
  
public class ProvincePartitioner extends Partitioner&lt;FlowBean, Text&gt; {  
  
    @Override  
    public int getPartition (FlowBean flowBean, Text text, int numPartitions) {  
        // 获取手机号前三位  
        String phone = text.toString ();  
        String prePhone = phone.substring (0, 3);  
  
        // 定义一个分区号变量 partition, 根据 prePhone 设置分区号  
        int partition;  
        if ("136".equals (prePhone)){  
            partition = 0;  
        } else if ("137".equals (prePhone)){  
            partition = 1;  
        } else if ("138".equals (prePhone)){  
            partition = 2;  
        } else if ("139".equals (prePhone)){  
            partition = 3;  
        } else {  
            partition = 4;  
        }  
  
        // 最后返回分区号 partition  
        return partition;  
    }  
}
</code></pre>
<p>（2）在驱动类中添加分区类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writableComparablePartitioner;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class FlowDriver {
    public static void main (String [] args) throws IOException, InterruptedException, ClassNotFoundException {
        Configuration conf = new Configuration ();
        Job job = Job.getInstance (conf);

        job.setJarByClass (FlowDriver.class);

        job.setMapperClass (FlowMapper.class);
        job.setReducerClass (FlowReducer.class);

        job.setMapOutputKeyClass (FlowBean.class);
        job.setMapOutputValueClass (Text.class);

        job.setOutputKeyClass (Text.class);
        job.setOutputValueClass (FlowBean.class);

        // 设置自定义分区器
        job.setPartitionerClass (ProvincePartitioner.class);
        // 设置对应的 ReduceTask 的个数
        job.setNumReduceTasks (5);

        FileInputFormat.setInputPaths (job, new Path ("E:\\BigData\\hadoop\\output"));
        FileOutputFormat.setOutputPath (job, new Path ("E:\\BigData\\hadoop\\output5"));

        boolean res = job.waitForCompletion (true);
        System.exit (res ? 0 : 1);
    }
}

</code></pre>
<h3 id="337-combiner-合并"><a class="header" href="#337-combiner-合并">3.3.7 Combiner 合并</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680572024854.png" alt="image-20221110131319588" /></p>
<p>（6）自定义 Combiner 实现步骤</p>
<p>（a）自定义一个 Combiner 继承 Reducer，重写 Reduce 方法</p>
<pre><code class="language-java">public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {  
  
    private IntWritable outV = new IntWritable ();  
  
    @Override  
    protected void reduce (Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {  
  
        int sum = 0;  
        for (IntWritable value : values) {  
            sum += value.get ();  
        }  
       
        outV.set (sum);  
       
        context.write (key,outV);  
    }  
}
</code></pre>
<p>（b）在 Job 驱动类中设置：</p>
<pre><code class="language-java">job.setCombinerClass (WordCountCombiner.class);
</code></pre>
<h3 id="338-combiner-合并案例实操"><a class="header" href="#338-combiner-合并案例实操">3.3.8 Combiner 合并案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>​ 统计过程中对每一个 MapTask 的输出进行局部汇总，以减小网络传输量即采用 Combiner 功能。</p>
<p>（1）数据输入</p>
<p><code>hello.txt</code></p>
<pre><code>banzhang ni hao  
xihuan hadoop banzhang  
banzhang ni hao  
xihuan hadoop banzhang
</code></pre>
<p>（2）期望输出数据</p>
<p>期望：Combine 输入数据多，输出时经过合并，输出数据减少。</p>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://image.3001.net/images/20221110/16680572608397.png" alt="image-20221110131417534" /></p>
<p><strong>3</strong>）案例实操 - 方案一</p>
<p>（1）增加一个 WordCountCombiner 类继承 Reducer</p>
<pre><code class="language-java">import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
import java.io.IOException;  
  
public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {  
  
private IntWritable outV = new IntWritable ();  
  
    @Override  
    protected void reduce (Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {  
  
        int sum = 0;  
        for (IntWritable value : values) {  
            sum += value.get ();  
        }  
  
        // 封装 outKV  
        outV.set (sum);  
  
        // 写出 outKV  
        context.write (key,outV);  
    }  
}
</code></pre>
<p>（2）在 WordcountDriver 驱动类中指定 Combiner</p>
<pre><code class="language-java">// 指定需要使用 combiner，以及用哪个类作为 combiner 的逻辑  
job.setCombinerClass (WordCountCombiner.class);
</code></pre>
<p><strong>4</strong>）案例实操 - 方案二（推荐）</p>
<p>（1）由于 Reducer 类中已经实现了相同的方法，下面将在 WordcountDriver 驱动类中指定 WordcountReducer 作为 Combiner</p>
<pre><code class="language-java">// 指定需要使用 Combiner，以及用哪个类作为 Combiner 的逻辑  
job.setCombinerClass (WordCountReducer.class);
</code></pre>
<p>运行程序，如下图所示</p>
<p><img src="https://image.3001.net/images/20221110/1668057397598.png" alt="image-20221110131634660" /></p>
<p>注意：没有 Reduce 阶段就没有 Shuffle 阶段。</p>
<h2 id="34-outputformat-数据输出"><a class="header" href="#34-outputformat-数据输出">3.4 OutputFormat 数据输出</a></h2>
<h3 id="341-outputformat-接口实现类"><a class="header" href="#341-outputformat-接口实现类">3.4.1 OutputFormat 接口实现类</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680574261360.png" alt="image-20221110131703273" /></p>
<h3 id="342-自定义-outputformat-案例实操"><a class="header" href="#342-自定义-outputformat-案例实操">3.4.2 自定义 OutputFormat 案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>过滤输入的 <code>log</code> 日志，包含 <code>atguigu</code> 的网站输出到 <code>e:/atguigu.log</code>，不包含 <code>atguigu</code> 的网站输出到 <code>e:/other.log</code>。</p>
<p>（1）输入数据</p>
<p><code>log.txt</code></p>
<pre><code>http://www.baidu.com  
http://www.google.com  
http://cn.bing.com  
http://www.atguigu.com  
http://www.sohu.com  
http://www.sina.com  
http://www.sin2a.com  
http://www.sin2desa.com  
http://www.sindsafa.com
</code></pre>
<p>（2）期望输出数据</p>
<p>​ atguigu.log</p>
<pre><code>http://www.atguigu.com
</code></pre>
<p>other.log</p>
<pre><code>http://cn.bing.com  
http://www.baidu.com  
http://www.google.com  
http://www.sin2a.com  
http://www.sin2desa.com  
http://www.sina.com  
http://www.sindsafa.com  
http://www.sohu.com
</code></pre>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://image.3001.net/images/20221110/16680577096922.png" alt="image-20221110132146534" /></p>
<p><strong>3</strong>）案例实操</p>
<p>（1）编写 LogMapper 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
  
import java.io.IOException;  
  
public class LogMapper extends Mapper&lt;LongWritable, Text,Text, NullWritable&gt; {  
    @Override  
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
        // 不做任何处理，直接写出一行 log 数据  
        context.write (value,NullWritable.get ());  
    }  
}
</code></pre>
<p>（2）编写 LogReducer 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
  
import java.io.IOException;  
  
public class LogReducer extends Reducer&lt;Text, NullWritable,Text, NullWritable&gt; {  
    @Override  
    protected void reduce (Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {  
        // 防止有相同的数据，迭代写出  
        for (NullWritable value : values) {  
            context.write (key,NullWritable.get ());  
        }  
    }  
}
</code></pre>
<p>（3）自定义一个 LogOutputFormat 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.RecordWriter;  
import org.apache.hadoop.mapreduce.TaskAttemptContext;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
import java.io.IOException;  
  
public class LogOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; {  
    @Override  
    public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter (TaskAttemptContext job) throws IOException, InterruptedException {  
        // 创建一个自定义的 RecordWriter 返回  
        LogRecordWriter logRecordWriter = new LogRecordWriter (job);  
        return logRecordWriter;  
    }  
}
</code></pre>
<p>（4）编写 LogRecordWriter 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.outputformat;  
  
import org.apache.hadoop.fs.FSDataOutputStream;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.IOUtils;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.RecordWriter;  
import org.apache.hadoop.mapreduce.TaskAttemptContext;  
  
import java.io.IOException;  
  
public class LogRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; {  
  
    private FSDataOutputStream atguiguOut;  
    private FSDataOutputStream otherOut;  
  
    public LogRecordWriter (TaskAttemptContext job) {  
        try {  
            // 获取文件系统对象  
            FileSystem fs = FileSystem.get (job.getConfiguration ());  
            // 用文件系统对象创建两个输出流对应不同的目录  
            atguiguOut = fs.create (new Path ("d:/hadoop/atguigu.log"));  
            otherOut = fs.create (new Path ("d:/hadoop/other.log"));  
        } catch (IOException e) {  
            e.printStackTrace ();  
        }  
    }  
  
    @Override  
    public void write (Text key, NullWritable value) throws IOException, InterruptedException {  
        String log = key.toString ();  
        // 根据一行的 log 数据是否包含 atguigu, 判断两条输出流输出的内容  
        if (log.contains ("atguigu")) {  
            atguiguOut.writeBytes (log + "\n");  
        } else {  
            otherOut.writeBytes (log + "\n");  
        }  
    }  
  
    @Override  
    public void close (TaskAttemptContext context) throws IOException, InterruptedException {  
        // 关流  
        IOUtils.closeStream (atguiguOut);  
        IOUtils.closeStream (otherOut);  
    }  
}
</code></pre>
<p>（5）编写 LogDriver 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.outputformat;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
import java.io.IOException;  
  
public class LogDriver {  
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {  
  
        Configuration conf = new Configuration ();  
        Job job = Job.getInstance (conf);  
  
        job.setJarByClass (LogDriver.class);  
        job.setMapperClass (LogMapper.class);  
        job.setReducerClass (LogReducer.class);  
  
        job.setMapOutputKeyClass (Text.class);  
        job.setMapOutputValueClass (NullWritable.class);  
  
        job.setOutputKeyClass (Text.class);  
        job.setOutputValueClass (NullWritable.class);  
  
        // 设置自定义的 outputformat  
        job.setOutputFormatClass (LogOutputFormat.class);  
  
        FileInputFormat.setInputPaths (job, new Path ("D:\\input"));  
        // 虽然我们自定义了 outputformat，但是因为我们的 outputformat 继承自 fileoutputformat  
        // 而 fileoutputformat 要输出一个_SUCCESS 文件，所以在这还得指定一个输出目录  
        FileOutputFormat.setOutputPath (job, new Path ("D:\\logoutput"));  
  
        boolean b = job.waitForCompletion (true);  
        System.exit (b ? 0 : 1);  
    }  
}
</code></pre>
<h2 id="35-mapreduce-内核源码解析"><a class="header" href="#35-mapreduce-内核源码解析">3.5 MapReduce 内核源码解析</a></h2>
<h3 id="351-maptask-工作机制"><a class="header" href="#351-maptask-工作机制">3.5.1 MapTask 工作机制</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680580349014.png" alt="image-20221110132710797" /></p>
<p>​ （1）Read 阶段：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中解析出一个个 key/value。</p>
<p>​ （2）Map 阶段：该节点主要是将解析出的 key/value 交给用户编写 map () 函数处理，并产生一系列新的 key/value。</p>
<p>​ （3）Collect 收集阶段：在用户编写 map () 函数中，当数据处理完成后，一般会调用 OutputCollector.collect () 输出结果。在该函数内部，它会将生成的 key/value 分区（调用 Partitioner），并写入一个环形内存缓冲区中。</p>
<p>​ （4）Spill 阶段：即 “溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>​ 溢写阶段详情：</p>
<p>​ 步骤 1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号 Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。</p>
<p>​ 步骤 2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件 output/spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
<p>​ 步骤 3：将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件 output/spillN.out.index 中。</p>
<p>​ （5）Merge 阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>​ 当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output/file.out 中，同时生成相应的索引文件 output/file.out.index。</p>
<p>​ 在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并 mapreduce.task.io.sort.factor（默认 10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>​ 让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<h3 id="352-reducetask-工作机制"><a class="header" href="#352-reducetask-工作机制">3.5.2 ReduceTask 工作机制</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680580777443.png" alt="image-20221110132754693" /></p>
<p>​ （1）Copy 阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>​ （2）Sort 阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照 MapReduce 语义，用户编写 reduce () 函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。</p>
<p>​ （3）Reduce 阶段：reduce () 函数将计算结果写到 HDFS 上。</p>
<h3 id="353-reducetask-并行度决定机制"><a class="header" href="#353-reducetask-并行度决定机制">3.5.3 ReduceTask 并行度决定机制</a></h3>
<p><strong>回顾：</strong> MapTask 并行度由切片个数决定，切片个数由输入文件和切片规则决定。</p>
<p><strong>思考：</strong> ReduceTask 并行度由谁决定？</p>
<p><strong>1</strong>）设置 ReduceTask 并行度（个数）</p>
<p>​ ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置：</p>
<pre><code class="language-java">// 默认值是 1，手动设置为 4  
job.setNumReduceTasks (4);
</code></pre>
<p><strong>2</strong>）实验：测试 ReduceTask 多少合适</p>
<p>（1）实验环境：1 个 Master 节点，16 个 Slave 节点：CPU:8GHZ，内存: 2G</p>
<p>（2）实验结论：</p>
<p>表 改变 ReduceTask（数据量为 1GB）</p>
<p>MapTask =16</p>
<div class="table-wrapper"><table><thead><tr><th>ReduceTask</th><th>1</th><th>5</th><th>10</th><th>15</th><th>16</th><th>20</th><th>25</th><th>30</th><th>45</th><th>60</th></tr></thead><tbody>
<tr><td>总时间</td><td>892</td><td>146</td><td>110</td><td>92</td><td>88</td><td>100</td><td>128</td><td>101</td><td>145</td><td>104</td></tr>
</tbody></table>
</div>
<p><strong>3</strong>）注意事项</p>
<p><img src="https://image.3001.net/images/20221110/16680581412729.png" alt="image-20221110132857785" /></p>
<h3 id="354-maptask--reducetask-源码解析"><a class="header" href="#354-maptask--reducetask-源码解析">3.5.4 MapTask &amp; ReduceTask 源码解析</a></h3>
<p><strong>1</strong>）MapTask 源码解析流程</p>
<p><img src="https://image.3001.net/images/20221110/16680581638498.png" alt="image-20221110132920449" /></p>
<p><strong>2</strong>）ReduceTask 源码解析流程</p>
<p><img src="https://image.3001.net/images/20221110/16680582064495.png" alt="image-20221110133003542" /></p>
<h2 id="36-join-应用"><a class="header" href="#36-join-应用">3.6 Join 应用</a></h2>
<h3 id="361-reduce-join"><a class="header" href="#361-reduce-join">3.6.1 Reduce Join</a></h3>
<p>​Map 端的主要工作：为来自不同表或文件的 key/value 对，打标签以区别不同来源的记录。然后用连接字段作为 key，其余部分和新加的标志作为 value，最后进行输出。</p>
<p>Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进行合并就 ok 了。</p>
<h3 id="362-reduce-join-案例实操"><a class="header" href="#362-reduce-join-案例实操">3.6.2 Reduce Join 案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>order.txt</p>
<pre><code>1001	01	1
1002	02	2
1003	03	3
1004	01	4
1005	02	5
1006	03	6
</code></pre>
<p>pd.txt</p>
<pre><code>01	小米
02	华为
03	格力
</code></pre>
<p>表 4-4 订单数据表 t_order</p>
<div class="table-wrapper"><table><thead><tr><th>id</th><th>pid</th><th>amount</th></tr></thead><tbody>
<tr><td>1001</td><td>01</td><td>1</td></tr>
<tr><td>1002</td><td>02</td><td>2</td></tr>
<tr><td>1003</td><td>03</td><td>3</td></tr>
<tr><td>1004</td><td>01</td><td>4</td></tr>
<tr><td>1005</td><td>02</td><td>5</td></tr>
<tr><td>1006</td><td>03</td><td>6</td></tr>
</tbody></table>
</div>
<p>表 4-5 商品信息表 t_product</p>
<div class="table-wrapper"><table><thead><tr><th>pid</th><th>pname</th></tr></thead><tbody>
<tr><td>01</td><td>小米</td></tr>
<tr><td>02</td><td>华为</td></tr>
<tr><td>03</td><td>格力</td></tr>
</tbody></table>
</div>
<p>​将商品信息表中数据根据商品 pid 合并到订单数据表中。</p>
<p>表 4-6 最终数据形式</p>
<div class="table-wrapper"><table><thead><tr><th>id</th><th>pname</th><th>amount</th></tr></thead><tbody>
<tr><td>1001</td><td>小米</td><td>1</td></tr>
<tr><td>1004</td><td>小米</td><td>4</td></tr>
<tr><td>1002</td><td>华为</td><td>2</td></tr>
<tr><td>1005</td><td>华为</td><td>5</td></tr>
<tr><td>1003</td><td>格力</td><td>3</td></tr>
<tr><td>1006</td><td>格力</td><td>6</td></tr>
</tbody></table>
</div>
<p><strong>2</strong>）需求分析</p>
<p>​ 通过将关联条件作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。</p>
<p><img src="https://image.3001.net/images/20221110/16680584431906.png" alt="image-20221110133359824" /></p>
<p><strong>3</strong>）代码实现</p>
<p>（1）创建商品和订单合并后的 TableBean 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.reducejoin;  
  
import org.apache.hadoop.io.Writable;  
  
import java.io.DataInput;  
import java.io.DataOutput;  
import java.io.IOException;  
  
public class TableBean implements Writable {  
  
    private String id; // 订单 id  
    private String pid; // 产品 id  
    private int amount; // 产品数量  
    private String pname; // 产品名称  
    private String flag; // 判断是 order 表还是 pd 表的标志字段  
  
    public TableBean () {  
    }  
  
    public String getId () {  
        return id;  
    }  
  
    public void setId (String id) {  
        this.id = id;  
    }  
  
    public String getPid () {  
        return pid;  
    }  
  
    public void setPid (String pid) {  
        this.pid = pid;  
    }  
  
    public int getAmount () {  
        return amount;  
    }  
  
    public void setAmount (int amount) {  
        this.amount = amount;  
    }  
  
    public String getPname () {  
        return pname;  
    }  
  
    public void setPname (String pname) {  
        this.pname = pname;  
    }  
  
    public String getFlag () {  
        return flag;  
    }  
  
    public void setFlag (String flag) {  
        this.flag = flag;  
    }  
  
    @Override  
    public String toString () {  
        return id + "\t" + pname + "\t" + amount;  
    }  
  
    @Override  
    public void write (DataOutput out) throws IOException {  
        out.writeUTF (id);  
        out.writeUTF (pid);  
        out.writeInt (amount);  
        out.writeUTF (pname);  
        out.writeUTF (flag);  
    }  
  
    @Override  
    public void readFields (DataInput in) throws IOException {  
        this.id = in.readUTF ();  
        this.pid = in.readUTF ();  
        this.amount = in.readInt ();  
        this.pname = in.readUTF ();  
        this.flag = in.readUTF ();  
    }  
}
</code></pre>
<p>（2）编写 TableMapper 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.reducejoin;  
  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.InputSplit;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.lib.input.FileSplit;  
  
import java.io.IOException;  
  
public class TableMapper extends Mapper&lt;LongWritable,Text,Text,TableBean&gt; {  
  
    private String filename;  
    private Text outK = new Text ();  
    private TableBean outV = new TableBean ();  
  
    @Override  
    protected void setup (Context context) throws IOException, InterruptedException {  
        // 获取对应文件名称  
        InputSplit split = context.getInputSplit ();  
        FileSplit fileSplit = (FileSplit) split;  
        filename = fileSplit.getPath ().getName ();  
    }  
  
    @Override  
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
  
        // 获取一行  
        String line = value.toString ();  
  
        // 判断是哪个文件，然后针对文件进行不同的操作  
        if (filename.contains ("order")){  // 订单表的处理  
            String [] split = line.split ("\t");  
            // 封装 outK  
            outK.set (split [1]);  
            // 封装 outV  
            outV.setId (split [0]);  
            outV.setPid (split [1]);  
            outV.setAmount (Integer.parseInt (split [2]));  
            outV.setPname ("");  
            outV.setFlag ("order");  
        } else {                             // 商品表的处理  
            String [] split = line.split ("\t");  
            // 封装 outK  
            outK.set (split [0]);  
            // 封装 outV  
            outV.setId ("");  
            outV.setPid (split [0]);  
            outV.setAmount (0);  
            outV.setPname (split [1]);  
            outV.setFlag ("pd");  
        }  
  
        // 写出 KV  
        context.write (outK,outV);  
    }  
}
</code></pre>
<p>（3）编写 TableReducer 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.reducejoin;  
  
import org.apache.commons.beanutils.BeanUtils;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
  
import java.io.IOException;  
import java.lang.reflect.InvocationTargetException;  
import java.util.ArrayList;  
  
public class TableReducer extends Reducer&lt;Text,TableBean,TableBean, NullWritable&gt; {  
  
    @Override  
    protected void reduce (Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException {  
  
        ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;();  
        TableBean pdBean = new TableBean ();  
  
        for (TableBean value : values) {  
  
            // 判断数据来自哪个表  
            if ("order".equals (value.getFlag ())){   // 订单表  
  
			  // 创建一个临时 TableBean 对象接收 value，防止数据被覆盖
                TableBean tmpOrderBean = new TableBean ();  
  
                try {  
                    BeanUtils.copyProperties (tmpOrderBean,value);  
                } catch (IllegalAccessException e) {  
                    e.printStackTrace ();  
                } catch (InvocationTargetException e) {  
                    e.printStackTrace ();  
                }  
  
			  // 将临时 TableBean 对象添加到集合 orderBeans  
                orderBeans.add (tmpOrderBean);  
            } else {                                    // 商品表  
                try {  
                    BeanUtils.copyProperties (pdBean,value);  
                } catch (IllegalAccessException e) {  
                    e.printStackTrace ();  
                } catch (InvocationTargetException e) {  
                    e.printStackTrace ();  
                }  
            }  
        }  
  
        // 遍历集合 orderBeans, 替换掉每个 orderBean 的 pid 为 pname, 然后写出  
        for (TableBean orderBean : orderBeans) {  
  
            orderBean.setPname (pdBean.getPname ());  
  
		   // 写出修改后的 orderBean 对象  
            context.write (orderBean,NullWritable.get ());  
        }  
    }  
}
</code></pre>
<p>（4）编写 TableDriver 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.reducejoin;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
import java.io.IOException;  
  
public class TableDriver {  
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {  
        Job job = Job.getInstance (new Configuration ());  
  
        job.setJarByClass (TableDriver.class);  
        job.setMapperClass (TableMapper.class);  
        job.setReducerClass (TableReducer.class);  
  
        job.setMapOutputKeyClass (Text.class);  
        job.setMapOutputValueClass (TableBean.class);  
  
        job.setOutputKeyClass (TableBean.class);  
        job.setOutputValueClass (NullWritable.class);  
  
        FileInputFormat.setInputPaths (job, new Path ("D:\\input"));  
        FileOutputFormat.setOutputPath (job, new Path ("D:\\output"));  
  
        boolean b = job.waitForCompletion (true);  
        System.exit (b ? 0 : 1);  
    }  
}
</code></pre>
<p><strong>4</strong>）测试</p>
<p>运行程序查看结果</p>
<pre><code>1004	小米	4
1001	小米	1
1005	华为	5
1002	华为	2
1006	格力	6
1003	格力	3
</code></pre>
<p><strong>5</strong>）总结</p>
<p>缺点：这种方式中，合并的操作是在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜。</p>
<p><strong>解决方案：Map 端实现数据合并。</strong></p>
<h3 id="363-map-join"><a class="header" href="#363-map-join">3.6.3 Map Join</a></h3>
<p><strong>1</strong>）使用场景</p>
<p>Map Join 适用于一张表十分小、一张表很大的场景。</p>
<p><strong>2</strong>）优点</p>
<p>思考：在 Reduce 端处理过多的表，非常容易产生数据倾斜。怎么办？</p>
<p>在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数据的压力，尽可能的减少数据倾斜。</p>
<p><strong>3</strong>）具体办法：采用 DistributedCache</p>
<p>​ （1）在 Mapper 的 setup 阶段，将文件读取到缓存集合中。</p>
<p>​ （2）在 Driver 驱动类中加载缓存。</p>
<pre><code class="language-java">// 缓存普通文件到 Task 运行节点。  
job.addCacheFile (new URI ("file:///e:/cache/pd.txt"));  
// 如果是集群运行，需要设置 HDFS 路径  
job.addCacheFile (new URI ("hdfs://hadoop102:8020/cache/pd.txt"));
</code></pre>
<h3 id="364-map-join-案例实操"><a class="header" href="#364-map-join-案例实操">3.6.4 Map Join 案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>表 订单数据表 t_order</p>
<div class="table-wrapper"><table><thead><tr><th>id</th><th>pid</th><th>amount</th></tr></thead><tbody>
<tr><td>1001</td><td>01</td><td>1</td></tr>
<tr><td>1002</td><td>02</td><td>2</td></tr>
<tr><td>1003</td><td>03</td><td>3</td></tr>
<tr><td>1004</td><td>01</td><td>4</td></tr>
<tr><td>1005</td><td>02</td><td>5</td></tr>
<tr><td>1006</td><td>03</td><td>6</td></tr>
</tbody></table>
</div>
<p>表 商品信息表 t_product</p>
<div class="table-wrapper"><table><thead><tr><th>pid</th><th>pname</th></tr></thead><tbody>
<tr><td>01</td><td>小米</td></tr>
<tr><td>02</td><td>华为</td></tr>
<tr><td>03</td><td>格力</td></tr>
</tbody></table>
</div>
<p>​ 将商品信息表中数据根据商品 pid 合并到订单数据表中。</p>
<p>表最终数据形式</p>
<div class="table-wrapper"><table><thead><tr><th>id</th><th>pname</th><th>amount</th></tr></thead><tbody>
<tr><td>1001</td><td>小米</td><td>1</td></tr>
<tr><td>1004</td><td>小米</td><td>4</td></tr>
<tr><td>1002</td><td>华为</td><td>2</td></tr>
<tr><td>1005</td><td>华为</td><td>5</td></tr>
<tr><td>1003</td><td>格力</td><td>3</td></tr>
<tr><td>1006</td><td>格力</td><td>6</td></tr>
</tbody></table>
</div>
<p><strong>2</strong>）需求分析</p>
<p>MapJoin 适用于关联表中有小表的情形。</p>
<p><img src="https://image.3001.net/images/20221110/16680586993772.png" alt="image-20221110133816471" /></p>
<p><strong>3</strong>）实现代码</p>
<p>（1）先在 MapJoinDriver 驱动类中添加缓存文件</p>
<pre><code class="language-java">package com.TianHan.mapreduce.mapJoin;  
    
import org.apache.hadoop.conf.Configuration;    
import org.apache.hadoop.fs.Path;    
import org.apache.hadoop.io.NullWritable;    
import org.apache.hadoop.io.Text;    
import org.apache.hadoop.mapreduce.Job;    
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;    
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;    
    
import java.io.IOException;    
import java.net.URI;    
import java.net.URISyntaxException;    
    
public class MapJoinDriver {    
    
    public static void main (String [] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException {    
    
        // 1 获取 job 信息    
		Configuration conf = new Configuration ();    
        Job job = Job.getInstance (conf);    
        // 2 设置加载 jar 包路径    
		job.setJarByClass (MapJoinDriver.class);    
        // 3 关联 mapper    
		job.setMapperClass (MapJoinMapper.class);    
        // 4 设置 Map 输出 KV 类型    
		job.setMapOutputKeyClass (Text.class);    
        job.setMapOutputValueClass (NullWritable.class);    
        // 5 设置最终输出 KV 类型    
		job.setOutputKeyClass (Text.class);    
        job.setOutputValueClass (NullWritable.class);    
    
        // 加载缓存数据    
		job.addCacheFile (new URI ("file:///E:/BigData/hadoop/inputJoin/tablecache/pd.txt"));  
        // Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0    
		job.setNumReduceTasks (0);    
    
        // 6 设置输入输出路径    
		FileInputFormat.setInputPaths (job, new Path ("E:\\BigData\\hadoop\\inputJoin\\input"));  
        FileOutputFormat.setOutputPath (job, new Path ("E:\\BigData\\hadoop\\inputJoin\\output"));  
        // 7 提交    
		boolean b = job.waitForCompletion (true);    
        System.exit (b ? 0 : 1);    
    }    
}
</code></pre>
<p>（2）在 MapJoinMapper 类中的 setup 方法中读取缓存文件</p>
<pre><code class="language-java">package com.TianHan.mapreduce.mapJoin;  
  
import org.apache.commons.lang3.StringUtils;  
import org.apache.hadoop.fs.FSDataInputStream;    
import org.apache.hadoop.fs.FileSystem;    
import org.apache.hadoop.fs.Path;    
import org.apache.hadoop.io.IOUtils;    
import org.apache.hadoop.io.LongWritable;    
import org.apache.hadoop.io.NullWritable;    
import org.apache.hadoop.io.Text;    
import org.apache.hadoop.mapreduce.Mapper;    
    
import java.io.BufferedReader;    
import java.io.IOException;    
import java.io.InputStreamReader;    
import java.net.URI;    
import java.util.HashMap;    
import java.util.Map;    
    
public class MapJoinMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {    
    
    private Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;();    
    private Text text = new Text ();    
    
    // 任务开始前将 pd 数据缓存进 pdMap    
@Override    
protected void setup (Context context) throws IOException, InterruptedException {    
    
        // 通过缓存文件得到小表数据 pd.txt    
		URI [] cacheFiles = context.getCacheFiles ();    
        Path path = new Path (cacheFiles [0]);    
    
        // 获取文件系统对象，并开流    
		FileSystem fs = FileSystem.get (context.getConfiguration ());    
        FSDataInputStream fis = fs.open (path);    
    
        // 通过包装流转换为 reader, 方便按行读取    
		BufferedReader reader = new BufferedReader (new InputStreamReader (fis, "UTF-8"));    
    
        // 逐行读取，按行处理    
		String line;    
        while (StringUtils.isNotEmpty (line = reader.readLine ())) {    
            // 切割一行        
			//01    小米  
            String [] split = line.split ("\t");    
            pdMap.put (split [0], split [1]);    
        }    
    
        // 关流    
		IOUtils.closeStream (reader);    
    }    
    
    @Override    
protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {    
    
        // 读取大表 order.txt 数据  
        //1001  01 1  
        String [] fields = value.toString ().split ("\t");    
    
        // 通过大表每行数据的 pid, 去 pdMap 里面取出 pname    
		String pname = pdMap.get (fields [1]);    
    
        // 将大表每行数据的 pid 替换为 pname    
		text.set (fields [0] + "\t" + pname + "\t" + fields [2]);    
    
        // 写出    
		context.write (text,NullWritable.get ());    
    }    
}
</code></pre>
<h2 id="程序运行结果"><a class="header" href="#程序运行结果">程序运行结果</a></h2>
<p>part-m-00000</p>
<pre><code>1001	小米	1
1002	华为	2
1003	格力	3
1004	小米	4
1005	华为	5
1006	格力	6
</code></pre>
<h2 id="37-数据清洗etl"><a class="header" href="#37-数据清洗etl">3.7 数据清洗（ETL）</a></h2>
<p>ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据仓库</p>
<p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序。</p>
<p><strong>1</strong>）需求</p>
<p>去除日志中字段个数小于等于 11 的日志。</p>
<p>（1）输入数据</p>
<p>web.log</p>
<p>在项目中找</p>
<p>（2）期望输出数据</p>
<p>每行字段长度都大于 11。</p>
<p><strong>2</strong>）需求分析</p>
<p>需要在 Map 阶段对输入的数据根据规则进行过滤清洗。</p>
<p><strong>3</strong>）实现代码</p>
<p>（1）编写 WebLogMapper 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.weblog;  
import java.io.IOException;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
  
public class WebLogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;{  
	  
	@Override  
	protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
		  
		// 1 获取 1 行数据  
		String line = value.toString ();  
		  
		// 2 解析日志  
		boolean result = parseLog (line,context);  
		  
		// 3 日志不合法退出  
		if (!result) {  
			return;  
		}  
		  
		// 4 日志合法就直接写出  
		context.write (value, NullWritable.get ());  
	}  
  
	// 2 封装解析日志的方法  
	private boolean parseLog (String line, Context context) {  
  
		// 1 截取  
		String [] fields = line.split (" ");  
		  
		// 2 日志长度大于 11 的为合法  
		if (fields.length &gt; 11) {  
			return true;  
		} else {  
			return false;  
		}  
	}  
}
</code></pre>
<p>（2）编写 WebLogDriver 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.weblog;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
public class WebLogDriver {  
	public static void main (String [] args) throws Exception {  
  
// 输入输出路径需要根据自己电脑上实际的输入输出路径设置  
        args = new String [] { "D:/input/inputlog", "D:/output1" };  
  
		// 1 获取 job 信息  
		Configuration conf = new Configuration ();  
		Job job = Job.getInstance (conf);  
  
		// 2 加载 jar 包  
		job.setJarByClass (WebLogDriver.class);  
  
		// 3 关联 map  
		job.setMapperClass (WebLogMapper.class);  
  
		// 4 设置最终输出类型  
		job.setOutputKeyClass (Text.class);  
		job.setOutputValueClass (NullWritable.class);  
  
		// 设置 reducetask 个数为 0  
		job.setNumReduceTasks (0);  
  
		// 5 设置输入和输出路径  
		FileInputFormat.setInputPaths (job, new Path (args [0]));  
		FileOutputFormat.setOutputPath (job, new Path (args [1]));  
  
		// 6 提交  
         boolean b = job.waitForCompletion (true);  
         System.exit (b ? 0 : 1);  
	}  
}
</code></pre>
<h2 id="附录etl-清洗规则"><a class="header" href="#附录etl-清洗规则">附录：ETL 清洗规则</a></h2>
<p>https://developer.aliyun.com/article/1319420</p>
<p>好，干货开始。数据清洗的目的可以从两个角度上看一是为了解决数据质量问题，二是让数据更适合做挖掘。不同的目的下分不同的情况，也都有相应的解决方式和方法。</p>
<h3 id="解决数据质量问题"><a class="header" href="#解决数据质量问题">解决数据质量问题</a></h3>
<p>这部分主要是规范数据，满足业务的使用，解决数据质量的各种问题，其目的包括但不限于：</p>
<ol>
<li>数据的完整性 ---- 例如人的属性中缺少性别、籍贯、年龄等</li>
<li>数据的唯一性 ---- 例如不同来源的数据出现重复的情况</li>
<li>数据的权威性 ---- 例如同一个指标出现多个来源的数据，且数值不一样</li>
<li>数据的合法性 ---- 例如获取的数据与常识不符，年龄大于 150 岁</li>
<li>数据的一致性 ---- 例如不同来源的不同指标，实际内涵是一样的，或是同一指标内涵不一致</li>
</ol>
<p>数据清洗的结果是对各种脏数据进行对应方式的处理，得到标准的、干净的、连续的数据，提供给数据统计、数据挖掘等使用。</p>
<p>那么为了解决以上的各种问题，我们需要不同的手段和方法来一一处理。</p>
<p>每种问题都有各种情况，每种情况适用不同的处理方法，具体如下：</p>
<p>1：解决数据的完整性问题：</p>
<blockquote>
<p>解题思路：数据缺失，那么补上就好了。补数据有什么方法？</p>
</blockquote>
<ul>
<li>通过其他信息补全，例如使用身份证件号码推算性别、籍贯、出生日期、年龄等</li>
<li>通过前后数据补全，例如时间序列缺数据了，可以使用前后的均值，缺的多了，可以使用平滑等处理，记得 Matlab 还是什么工具可以自动补全</li>
<li>实在补不全的，虽然很可惜，但也必须要剔除。但是不要删掉，没准以后可以用得上</li>
</ul>
<p>2：解决数据的唯一性问题</p>
<blockquote>
<p>解题思路：去除重复记录，只保留一条。去重的方法有：</p>
</blockquote>
<ul>
<li>按主键去重，用 sql 或者 excel “去除重复记录” 即可，</li>
<li>按规则去重，编写一系列的规则，对重复情况复杂的数据进行去重。例如不同渠道来的客户数据，可以通过相同的关键信息进行匹配，合并去重。</li>
</ul>
<p>3：解决数据的权威性问题</p>
<blockquote>
<p>解题思路：用最权威的那个渠道的数据方法：对不同渠道设定权威级别，例如：在家里，首先得相信媳妇说的。</p>
</blockquote>
<p>4：解决数据的合法性问题</p>
<blockquote>
<p>解题思路：设定判定规则</p>
</blockquote>
<ol>
<li>设定强制合法规则，凡是不在此规则范围内的，强制设为最大值，或者判为无效，剔除</li>
</ol>
<ul>
<li>字段类型合法规则：日期字段格式为 “2010-10-10”</li>
<li>字段内容合法规则：性别 in （男、女、未知）；出生日期 &lt;= 今天</li>
</ul>
<ol start="2">
<li>设定警告规则，凡是不在此规则范围内的，进行警告，然后人工处理</li>
</ol>
<ul>
<li>警告规则：年龄 &gt; 110</li>
</ul>
<ol start="3">
<li>离群值人工特殊处理，使用分箱、聚类、回归、等方式发现离群值</li>
</ol>
<p>5：解决数据的一致性问题</p>
<blockquote>
<p>解题思路：建立元数据体系，包含但不限于：</p>
</blockquote>
<ol>
<li>指标体系（度量）</li>
<li>维度（分组、统计口径）</li>
<li>单位</li>
<li>频度</li>
<li>数据</li>
</ol>
<p>tips：</p>
<p>如果数据质量问题比较严重，建议跟技术团队好好聊聊。</p>
<p>如果需要控制的范围越来越大，这就不是 ETL 工程师的工作了，得升级为数据治理了，下次有空再分享。</p>
<h3 id="供应算法原料"><a class="header" href="#供应算法原料">供应算法原料</a></h3>
<ol>
<li>
<p>这部分主要是让数据更适合数据挖掘，作为算法训练的原料。其目标包括但不限于：</p>
</li>
<li>
<p>高维度 ---- 不适合挖掘</p>
</li>
<li>
<p>维度太低 ---- 不适合挖掘</p>
</li>
<li>
<p>无关信息 ---- 减少存储</p>
</li>
<li>
<p>字段冗余 ---- 一个字段是其他字段计算出来的，会造成相关系数为 1 或者主成因分析异常）</p>
</li>
<li>
<p>多指标数值、单位不同 ---- 如 GDP 与城镇居民人均收入数值相差过大</p>
</li>
</ol>
<p>1：解决高维度问题</p>
<blockquote>
<p>解题思路：降维，方法包括但不限于：</p>
</blockquote>
<ol>
<li>主成分分析</li>
<li>随机森林</li>
</ol>
<p>2：解决维度低或缺少维度问题</p>
<blockquote>
<p>解题思路：抽象，方法包括但不限于：</p>
</blockquote>
<ol>
<li>各种汇总，平均、加总、最大、最小等</li>
<li>各种离散化，聚类、自定义分组等</li>
</ol>
<p>3：解决无关信息和字段冗余</p>
<blockquote>
<p>解决方法：剔除字段</p>
</blockquote>
<p>4：解决多指标数值、单位不同问题</p>
<blockquote>
<p>解决方法：归一化，方法包括但不限于：</p>
</blockquote>
<ol>
<li>最小 - 最大</li>
<li>零 - 均值</li>
<li>小数定标</li>
</ol>
<p>其实 ETL 工程师有非常好的数据功底，无论是转那个岗都方便，你缺少的是系统的学习和迈出去的勇气。</p>
<h2 id="附录常用正则表达式"><a class="header" href="#附录常用正则表达式">附录常用正则表达式</a></h2>
<p><a href="https://blog.csdn.net/ws54ws54/article/details/110220049">Java 常用正则表达式大全 (史上最全的正则表达式 - 匹配中英文、字母和数字)</a></p>
<p>在做项目的过程中，使用正则表达式来匹配一段文本中的特定种类字符，是比较常用的一种方式，下面是对常用的正则匹配做了一个归纳整理。</p>
<h3 id="一校验数字的表达式"><a class="header" href="#一校验数字的表达式">一、校验数字的表达式</a></h3>
<pre><code>1 数字：^[0-9]*$
2 n 位的数字：^\d {n}$
3 至少 n 位的数字：^\d {n,}$
4 m-n 位的数字：^\d {m,n}$
5 零和非零开头的数字：^(0|[1-9][0-9]*)$
6 非零开头的最多带两位小数的数字：^([1-9][0-9]*)(.[0-9]{1,2})?$
7 带 1-2 位小数的正数或负数：^(\-)?\d+(\.\d {1,2})?$
8 正数、负数、和小数：^(\-|\+)?\d+(\.\d+)?$
9 有两位小数的正实数：^[0-9]+(.[0-9]{2})?$
10 有 1~3 位小数的正实数：^[0-9]+(.[0-9]{1,3})?$
11 非零的正整数：^[1-9]\d*$ 或 ^([1-9][0-9]*){1,3}$ 或 ^\+?[1-9][0-9]*$
12 非零的负整数：^\-[1-9][] 0-9"*$ 或 ^-[1-9]\d*$
13 非负整数：^\d+$ 或 ^[1-9]\d*|0$
14 非正整数：^-[1-9]\d*|0$ 或 ^((-\d+)|(0+))$
15 非负浮点数：^\d+(\.\d+)?$ 或 ^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$
16 非正浮点数：^((-\d+(\.\d+)?)|(0+(\.0+)?))$ 或 ^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$
17 正浮点数：^[1-9]\d*\.\d*|0\.\d*[1-9]\d*$ 或 ^(([0-9]+\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\.[0-9]+)|([0-9]*[1-9][0-9]*))$
18 负浮点数：^-([1-9]\d*\.\d*|0\.\d*[1-9]\d*)$ 或 ^(-(([0-9]+\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\.[0-9]+)|([0-9]*[1-9][0-9]*)))$
19 浮点数：^(-?\d+)(\.\d+)?$ 或 ^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$
</code></pre>
<h3 id="二校验字符的表达式"><a class="header" href="#二校验字符的表达式">二、校验字符的表达式</a></h3>
<pre><code>1 汉字：^[\u4e00-\u9fa5]{0,}$
2 英文和数字：^[A-Za-z0-9]+$ 或 ^[A-Za-z0-9]{4,40}$
3 长度为 3-20 的所有字符：^.{3,20}$
4 由 26 个英文字母组成的字符串：^[A-Za-z]+$
5 由 26 个大写英文字母组成的字符串：^[A-Z]+$
6 由 26 个小写英文字母组成的字符串：^[a-z]+$
7 由数字和 26 个英文字母组成的字符串：^[A-Za-z0-9]+$
8 由数字、26 个英文字母或者下划线组成的字符串：^\w+$ 或 ^\w {3,20}$
9 中文、英文、数字包括下划线：^[\u4E00-\u9FA5A-Za-z0-9_]+$
10 中文、英文、数字但不包括下划线等符号：^[\u4E00-\u9FA5A-Za-z0-9]+$ 或 ^[\u4E00-\u9FA5A-Za-z0-9]{2,20}$
11 可以输入含有 ^%&amp;’,;=?KaTeX parse error: Can't use function '\"' in math mode at position 1: \̲"̲等字符：`[^%&amp;',;=?\x22]+12 禁止输入含有～的字符：[^~\x22]+ 三、特殊需求表达式 1 Email 地址：^\w+([-+.]\w+)@\w+([-.]\w+).\w+([-.]\w+) KaTeX parse error: Undefined control sequence: \s at position 108: …`[a-zA-z]+://[^\̲s̲]*`或`^http://…4 手机号码：^(13 [0-9]|14 [5|7]|15 [0|1|2|3|5|6|7|8|9]|18 [0|1|2|3|5|6|7|8|9])\d {8}$
5 电话号码 (“XXX-XXXXXXX”、“XXXX-XXXXXXXX”、“XXX-XXXXXXX”、“XXX-XXXXXXXX”、"XXXXXXX" 和 "XXXXXXXX)：^(\(\d {3,4}-)|\d {3.4}-)?\d {7,8}$
6 国内电话号码 (0511-4405222、021-87888822)：\d {3}-\d {8}|\d {4}-\d {7}
7 身份证号 (15 位、18 位数字)：^\d {15}|\d {18}$
8 短身份证号码 (数字、字母 x 结尾)：^([0-9]){7,18}(x|X)?$ 或 ^\d {8,18}|[0-9x]{8,18}|[0-9X]{8,18}?$
9 帐号是否合法 (字母开头，允许 5-16 字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$
10 密码 (以字母开头，长度在 6~18 之间，只能包含字母、数字和下划线)：^[a-zA-Z]\w {5,17}$
11 强密码 (必须包含大小写字母和数字的组合，不能使用特殊字符，长度在 8-10 之间)：^(?=.*\d)(?=.*[a-z])(?=.*[A-Z]).{8,10}$
12 日期格式：^\d {4}-\d {1,2}-\d {1,2}
13 一年的 12 个月 (01～09 和 1～12)：^(0?[1-9]|1 [0-2])$
14 一个月的 31 天 (01～09 和 1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$
15 钱的输入格式：
16 1. 有四种钱的表示形式我们可以接受:“10000.00” 和 “10,000.00”, 和没有 “分” 的 “10000” 和 “10,000”：^[1-9][0-9]*$
17 2. 这表示任意一个不以 0 开头的数字，但是，这也意味着一个字符 "0" 不通过，所以我们采用下面的形式：^(0|[1-9][0-9]*)$
18 3. 一个 0 或者一个不以 0 开头的数字。我们还可以允许开头有一个负号：^(0|-?[1-9][0-9]*)$
19 4. 这表示一个 0 或者一个可能为负的开头不为 0 的数字。让用户以 0 开头好了。把负号的也去掉，因为钱总不能是负的吧。下面我们要加的是说明可能的小数部分：^[0-9]+(.[0-9]+)?$
20 5. 必须说明的是，小数点后面至少应该有 1 位数，所以 "10.“是不通过的，但是 “10” 和 “10.2” 是通过的：^[0-9]+(.[0-9]{2})?$
21 6. 这样我们规定小数点后面必须有两位，如果你认为太苛刻了，可以这样：^[0-9]+(.[0-9]{1,2})?$
22 7. 这样就允许用户只写一位小数。下面我们该考虑数字中的逗号了，我们可以这样：^[0-9]{1,3}(,[0-9]{3})*(.[0-9]{1,2})?$
23 8.1 到 3 个数字，后面跟着任意个 逗号 + 3 个数字，逗号成为可选，而不是必须：^([0-9]+|[0-9]{1,3}(,[0-9]{3})*)(.[0-9]{1,2})?$
24 备注：这就是最终结果了，别忘了”+“可以用”" 替代如果你觉得空字符串也可以接受的话 (奇怪，为什么？) 最后，别忘了在用函数时去掉去掉那个反斜杠，一般的错误都在这里
25 xml 文件：^([a-zA-Z]+-?)+[a-zA-Z0-9]+\\.[x|X][m|M][l|L]$
26 中文字符的正则表达式：[\u4e00-\u9fa5]
27 双字节字符：[^\x00-\xff] (包括汉字在内，可以用来计算字符串的长度 (一个双字节字符长度计 2，ASCII 字符计 1))
28 空白行的正则表达式：\n\s*\r (可以用来删除空白行)
29 HTML 标记的正则表达式：&lt;(\S*?)[^&gt;]*&gt;.*?&lt;/\1&gt;|&lt;.*? /&gt; (网上流传的版本太糟糕，上面这个也仅仅能部分，对于复杂的嵌套标记依旧无能为力)
30 首尾空白字符的正则表达式：^\s*|\s*$ 或 (^\s*)|(\s*$) (可以用来删除行首行尾的空白字符 (包括空格、制表符、换页符等等)，非常有用的表达式)
31 腾讯 QQ 号：[1-9][0-9]{4,} (腾讯 QQ 号从 10000 开始)
32 中国邮政编码：[1-9]\d {5}(?!\d) (中国邮政编码为 6 位数字)
33 IP 地址：\d+\.\d+\.\d+\.\d+ (提取 IP 地址时有用)
</code></pre>
<h2 id="38-mapreduce-开发总结"><a class="header" href="#38-mapreduce-开发总结">3.8 MapReduce 开发总结</a></h2>
<p><strong>1</strong>）输入数据接口：InputFormat</p>
<p>（1）默认使用的实现类是：TextInputFormat</p>
<p>（2）TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为 key，行内容作为 value 返回。</p>
<p>（3）CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</p>
<p><strong>2</strong>）逻辑处理接口：Mapper</p>
<p>用户根据业务需求实现其中三个方法：map ()、setup () 和 cleanup ()</p>
<p><strong>3</strong>）Partitioner 分区</p>
<p>（1）有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；<code>key.hashCode ()&amp;Integer.MAXVALUE % numReduces</code></p>
<p>（2）如果业务上有特别的需求，可以自定义分区。</p>
<p><strong>4</strong>）Comparable 排序</p>
<p>（1）当我们用自定义的对象作为 key 来输出时，就必须要实现 <code>WritableComparable</code> 接口，重写其中的 <code>compareTo ()</code> 方法。</p>
<p>（2）部分排序：对最终输出的每一个文件进行内部排序。</p>
<p>（3）全排序：对所有数据进行排序，通常只有一个 Reduce。</p>
<p>（4）二次排序：排序的条件有两个。</p>
<p><strong>5</strong>）Combiner 合并</p>
<p>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。</p>
<p><strong>6</strong>）逻辑处理接口：Reducer</p>
<p>用户根据业务需求实现其中三个方法：reduce ()、setup () 和 cleanup ()</p>
<p><strong>7</strong>）输出数据接口：OutputFormat</p>
<p>（1）默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</p>
<p>（2）用户还可以自定义 OutputFormat。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hadoop数据压缩"><a class="header" href="#hadoop数据压缩">Hadoop数据压缩</a></h1>
<ul>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#41-%E6%A6%82%E8%BF%B0">4.1 概述</a></li>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#42-mapreduce%E6%94%AF%E6%8C%81%E7%9A%84%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81">4.2 MapReduce支持的压缩编码</a></li>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#43-%E5%8E%8B%E7%BC%A9%E6%96%B9%E5%BC%8F%E9%80%89%E6%8B%A9">4.3 压缩方式选择</a>
<ul>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#431-gzip%E5%8E%8B%E7%BC%A9">4.3.1 Gzip压缩</a></li>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#432-bzip2%E5%8E%8B%E7%BC%A9">4.3.2 Bzip2压缩</a></li>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#433-lzo%E5%8E%8B%E7%BC%A9">4.3.3 Lzo压缩</a></li>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#434-snappy%E5%8E%8B%E7%BC%A9">4.3.4 Snappy压缩</a></li>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#435-%E5%8E%8B%E7%BC%A9%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9">4.3.5 压缩位置选择</a></li>
</ul>
</li>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#44-%E5%8E%8B%E7%BC%A9%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE">4.4 压缩参数配置</a></li>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#45-%E5%8E%8B%E7%BC%A9%E5%AE%9E%E6%93%8D%E6%A1%88%E4%BE%8B">4.5 压缩实操案例</a>
<ul>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#451-map%E8%BE%93%E5%87%BA%E7%AB%AF%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9">4.5.1 Map输出端采用压缩</a></li>
<li><a href="Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#452-reduce%E8%BE%93%E5%87%BA%E7%AB%AF%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9">4.5.2 Reduce输出端采用压缩</a></li>
</ul>
</li>
</ul>
<h2 id="41-概述"><a class="header" href="#41-概述">4.1 概述</a></h2>
<p><strong>1</strong>）压缩的好处和坏处</p>
<p>好处：减少磁盘IO、减少磁盘存储空间。</p>
<p>坏处：增加CPU开销。</p>
<p><strong>2</strong>）压缩原则</p>
<p>（1）运算密集型的Job，少用压缩</p>
<p>（2）IO密集型的Job，多用压缩</p>
<h2 id="42-mapreduce支持的压缩编码"><a class="header" href="#42-mapreduce支持的压缩编码">4.2 MapReduce支持的压缩编码</a></h2>
<p>1）压缩算法对比介绍</p>
<div class="table-wrapper"><table><thead><tr><th>压缩格式</th><th>Hadoop自带？</th><th>算法</th><th>文件扩展名</th><th>是否可切片</th><th>换成压缩格式后，原来的程序是否需要修改</th></tr></thead><tbody>
<tr><td>DEFLATE</td><td>是，直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样，不需要修改</td></tr>
<tr><td>Gzip</td><td>是，直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样，不需要修改</td></tr>
<tr><td>bzip2</td><td>是，直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样，不需要修改</td></tr>
<tr><td>LZO</td><td>否，需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引，还需要指定输入格式</td></tr>
<tr><td>Snappy</td><td>是，直接使用</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样，不需要修改</td></tr>
</tbody></table>
</div>
<p>2）压缩性能的比较</p>
<div class="table-wrapper"><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody>
<tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr>
<tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr>
<tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr>
</tbody></table>
</div>
<p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p>
<p>Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger.On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p>
<h2 id="43-压缩方式选择"><a class="header" href="#43-压缩方式选择">4.3 压缩方式选择</a></h2>
<p>​ 压缩方式选择时重点考虑：压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片。</p>
<h3 id="431-gzip压缩"><a class="header" href="#431-gzip压缩">4.3.1 Gzip压缩</a></h3>
<p>优点：压缩率比较高</p>
<p>缺点：不支持Split；压缩/解压速度一般</p>
<h3 id="432-bzip2压缩"><a class="header" href="#432-bzip2压缩">4.3.2 Bzip2压缩</a></h3>
<p>优点：压缩率高；支持Split</p>
<p>缺点：压缩/解压速度慢</p>
<h3 id="433-lzo压缩"><a class="header" href="#433-lzo压缩">4.3.3 Lzo压缩</a></h3>
<p>优点：压缩/解压速度比较快；支持Split</p>
<p>缺点：压缩率一般；想支持切片需要额外创建索引</p>
<h3 id="434-snappy压缩"><a class="header" href="#434-snappy压缩">4.3.4 Snappy压缩</a></h3>
<p>优点：压缩和解压缩速度快</p>
<p>缺点：不支持Split；压缩率一般</p>
<p>注意：使用Snappy需要Hadoop3.x及Linux Kernel 7.5+</p>
<h3 id="435-压缩位置选择"><a class="header" href="#435-压缩位置选择">4.3.5 压缩位置选择</a></h3>
<p>压缩可以在MapReduce作用的任意阶段启用。</p>
<p><img src="https://image.3001.net/images/20221110/16680589476259.png" alt="image-20221110134223786" /></p>
<h2 id="44-压缩参数配置"><a class="header" href="#44-压缩参数配置">4.4 压缩参数配置</a></h2>
<p>1）为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器</p>
<div class="table-wrapper"><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody>
<tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr>
<tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr>
<tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr>
<tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr>
<tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr>
</tbody></table>
</div>
<p>查看相关信息使用<code>hadoop checknative</code>命令</p>
<p>2）要在Hadoop中启用压缩，可以配置如下参数</p>
<div class="table-wrapper"><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody>
<tr><td>io.compression.codecs （在core-site.xml中配置）</td><td>无，这个需要在命令行输入hadoop checknative查看</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr>
<tr><td>mapreduce.map.output.compress（在mapred-site.xml中配置）</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr>
<tr><td>mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td></tr>
<tr><td>mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr>
<tr><td>mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr>
</tbody></table>
</div>
<h2 id="45-压缩实操案例"><a class="header" href="#45-压缩实操案例">4.5 压缩实操案例</a></h2>
<h3 id="451-map输出端采用压缩"><a class="header" href="#451-map输出端采用压缩">4.5.1 Map输出端采用压缩</a></h3>
<p>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。</p>
<p>1）开启并设置map端输出压缩</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.io.compress.BZip2Codec;	  
import org.apache.hadoop.io.compress.CompressionCodec;  
import org.apache.hadoop.io.compress.GzipCodec;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
public class WordCountDriver {  
  
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {  
  
		Configuration conf = new Configuration();  
  
		// 开启map端输出压缩  
		conf.setBoolean("mapreduce.map.output.compress", true);  
  
		// 设置map端输出压缩方式  
		conf.setClass("mapreduce.map.output.compress.codec", BZip2Codec.class,CompressionCodec.class);  
  
		Job job = Job.getInstance(conf);  
  
		job.setJarByClass(WordCountDriver.class);  
  
		job.setMapperClass(WordCountMapper.class);  
		job.setReducerClass(WordCountReducer.class);  
  
		job.setMapOutputKeyClass(Text.class);  
		job.setMapOutputValueClass(IntWritable.class);  
  
		job.setOutputKeyClass(Text.class);  
		job.setOutputValueClass(IntWritable.class);  
  
		FileInputFormat.setInputPaths(job, new Path(args[0]));  
		FileOutputFormat.setOutputPath(job, new Path(args[1]));  
  
		boolean result = job.waitForCompletion(true);  
  
		System.exit(result ? 0 : 1);  
	}  
}
</code></pre>
<p>2）Mapper保持不变</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
  
public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{  
  
	Text k = new Text();  
	IntWritable v = new IntWritable(1);  
  
	@Override  
	protected void map(LongWritable key, Text value, Context context)throws IOException, InterruptedException {  
  
		// 1 获取一行  
		String line = value.toString();  
  
		// 2 切割  
		String[] words = line.split(" ");  
  
		// 3 循环写出  
		for(String word:words){  
			k.set(word);  
			context.write(k, v);  
		}  
	}  
}
</code></pre>
<p>3）Reducer保持不变</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
  
public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{  
  
	IntWritable v = new IntWritable();  
  
	@Override  
	protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,  
			Context context) throws IOException, InterruptedException {  
		  
		int sum = 0;  
  
		// 1 汇总  
		for(IntWritable value:values){  
			sum += value.get();  
		}  
		  
         v.set(sum);  
  
         // 2 输出  
		context.write(key, v);  
	}  
}
</code></pre>
<h3 id="452-reduce输出端采用压缩"><a class="header" href="#452-reduce输出端采用压缩">4.5.2 Reduce输出端采用压缩</a></h3>
<p>基于WordCount案例处理。</p>
<p>1）修改驱动，开启并设置reduce端输出压缩</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.io.compress.BZip2Codec;  
import org.apache.hadoop.io.compress.DefaultCodec;  
import org.apache.hadoop.io.compress.GzipCodec;  
import org.apache.hadoop.io.compress.Lz4Codec;  
import org.apache.hadoop.io.compress.SnappyCodec;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
public class WordCountDriver {  
  
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {  
		  
		Configuration conf = new Configuration();  
		  
		Job job = Job.getInstance(conf);  
		  
		job.setJarByClass(WordCountDriver.class);  
		  
		job.setMapperClass(WordCountMapper.class);  
		job.setReducerClass(WordCountReducer.class);  
		  
		job.setMapOutputKeyClass(Text.class);  
		job.setMapOutputValueClass(IntWritable.class);  
		  
		job.setOutputKeyClass(Text.class);  
		job.setOutputValueClass(IntWritable.class);  
		  
		FileInputFormat.setInputPaths(job, new Path(args[0]));  
		FileOutputFormat.setOutputPath(job, new Path(args[1]));  
		  
		// 开启reduce端输出压缩  
		FileOutputFormat.setCompressOutput(job, true);  
  
		// 设置压缩的方式  
	    FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);   
//	    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);   
//	    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);   
	      
		boolean result = job.waitForCompletion(true);  
		  
		System.exit(result?0:1);  
	}  
}
</code></pre>
<p>2）Mapper保持不变</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
  
public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{  
  
	Text k = new Text();  
	IntWritable v = new IntWritable(1);  
  
	@Override  
	protected void map(LongWritable key, Text value, Context context)throws IOException, InterruptedException {  
  
		// 1 获取一行  
		String line = value.toString();  
  
		// 2 切割  
		String[] words = line.split(" ");  
  
		// 3 循环写出  
		for(String word:words){  
			k.set(word);  
			context.write(k, v);  
		}  
	}  
}
</code></pre>
<p>3）Reducer保持不变</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
  
public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{  
  
	IntWritable v = new IntWritable();  
  
	@Override  
	protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,  
			Context context) throws IOException, InterruptedException {  
		  
		int sum = 0;  
  
		// 1 汇总  
		for(IntWritable value:values){  
			sum += value.get();  
		}  
		  
         v.set(sum);  
  
         // 2 输出  
		context.write(key, v);  
	}  
}
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
