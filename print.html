<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Big Data</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Big Data</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="chapter-1"><a class="header" href="#chapter-1">Chapter 1</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hadoop"><a class="header" href="#hadoop">Hadoop</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs"><a class="header" href="#hdfs">HDFS</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs-概述"><a class="header" href="#hdfs-概述">HDFS 概述</a></h1>
<ul>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#hdfs-%E4%BA%A7%E5%87%BA%E8%83%8C%E6%99%AF%E5%8F%8A%E5%AE%9A%E4%B9%89">HDFS 产出背景及定义</a>
<ul>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#1-hdfs-%E4%BA%A7%E7%94%9F%E8%83%8C%E6%99%AF">1 ）HDFS 产生背景</a></li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#2-hdfs-%E5%AE%9A%E4%B9%89">2 ）HDFS 定义</a></li>
</ul>
</li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#hdfs-%E4%BC%98%E7%BC%BA%E7%82%B9">HDFS 优缺点</a></li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#hdfs-%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84">HDFS 组成架构</a></li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#hdfs-%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F-%E9%9D%A2%E8%AF%95%E9%87%8D%E7%82%B9">HDFS 文件块大小 （面试重点）</a></li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#namenode-%E5%92%8C-secondarynamenode">NameNode 和 SecondaryNameNode</a>
<ul>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#nn-%E5%92%8C-2nn-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">NN 和 2NN 工作机制</a></li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#namenode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">NameNode工作机制</a></li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#fsimage-%E5%92%8C-edits-%E8%A7%A3%E6%9E%90">Fsimage 和 Edits 解析</a></li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#checkpoint-%E6%97%B6%E9%97%B4%E8%AE%BE%E7%BD%AE">CheckPoint 时间设置</a></li>
</ul>
</li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#datanode">DataNode</a>
<ul>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#datanode-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">DataNode 工作机制</a></li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7">数据完整性</a></li>
<li><a href="Hadoop/HDFS%E6%A6%82%E8%BF%B0.html#datanode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE">DataNode掉线时限参数设置</a></li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027142704.png" alt="" /></p>
<h2 id="hdfs-产出背景及定义"><a class="header" href="#hdfs-产出背景及定义">HDFS 产出背景及定义</a></h2>
<h3 id="1-hdfs-产生背景"><a class="header" href="#1-hdfs-产生背景">1 ）HDFS 产生背景</a></h3>
<p>随着数据量越来越大， 在一个操作系统存不下所有的数据， 那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS 只是分布式文件管理系统中的一种。</p>
<h3 id="2-hdfs-定义"><a class="header" href="#2-hdfs-定义">2 ）HDFS 定义</a></h3>
<p>HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>
<p>HDFS 的使用场景：<strong>适合一次写入，多次读出的场景。 一个文件经过创建、写入和关闭之后就不需要改变</strong>。</p>
<h2 id="hdfs-优缺点"><a class="header" href="#hdfs-优缺点">HDFS 优缺点</a></h2>
<p>HDFS优点</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027144347.png" alt="" /></p>
<p>HDFS缺点</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027144402.png" alt="" /></p>
<h2 id="hdfs-组成架构"><a class="header" href="#hdfs-组成架构">HDFS 组成架构</a></h2>
<p>HDFS组成架构示意图</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027145753.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027145821.png" alt="" /></p>
<p>简单形容：NameNode是老板，DataNode是打工仔，Client是客户，Secondary NameNode是秘书</p>
<h2 id="hdfs-文件块大小-面试重点"><a class="header" href="#hdfs-文件块大小-面试重点">HDFS 文件块大小 （面试重点）</a></h2>
<p>HDFS 文件块大小</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241027145939.png" alt="" /></p>
<p>思考：为什么块的大小不能设置太小，也不能设置太大？</p>
<p>（1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；
（2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</p>
<p>总结：<strong>HDFS块的大小设置主要取决于磁盘传输速率</strong>。</p>
<p>e.g.</p>
<p>普通的机械硬盘，传输速率100M/s →HDFS块设置为128M</p>
<p>固态硬盘，传输速率200-300M/s →HDFS块设置为256M</p>
<h2 id="namenode-和-secondarynamenode"><a class="header" href="#namenode-和-secondarynamenode">NameNode 和 SecondaryNameNode</a></h2>
<h3 id="nn-和-2nn-工作机制"><a class="header" href="#nn-和-2nn-工作机制">NN 和 2NN 工作机制</a></h3>
<p>思考：NameNode 中的元数据是存储在哪里的？</p>
<p>首先，我们做个假设，如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。</p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数据丢失。因此，引入 Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits 中。这样，一旦 NameNode 节点断电，可以通过 FsImage 和 Edits 的合并，合成元数据。 但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这个操作由NameNode节点完成， 又会效率过低。 因此， <strong>引入一个新的节点SecondaryNamenode，专门用于 FsImage 和 Edits 的合并。</strong></p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/d99676a51208185f6f4c380d6f1e9354.png" alt="a" /></p>
<h3 id="namenode工作机制"><a class="header" href="#namenode工作机制">NameNode工作机制</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102132550.png" alt="" /></p>
<p>1 ） 第一阶段：NameNode 启动</p>
<p>（1）第一次启动 NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</p>
<p>（2）客户端对元数据进行增删改的请求。</p>
<p>（3）NameNode 记录操作日志，更新滚动日志。</p>
<p>（4）NameNode 在内存中对元数据进行增删改。</p>
<p>2 ） 第二阶段：Secondary NameNode 工作</p>
<p>（1）Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode是否检查结果。</p>
<p>（2）Secondary NameNode 请求执行 CheckPoint。</p>
<p>（3）NameNode 滚动正在写的 Edits 日志。</p>
<p>（4）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。</p>
<p>（5）Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。</p>
<p>（6）生成新的镜像文件 fsimage.chkpoint。</p>
<p>（7）拷贝 fsimage.chkpoint 到 NameNode。</p>
<p>（8）NameNode 将 fsimage.chkpoint 重新命名成 fsimage。</p>
<h3 id="fsimage-和-edits-解析"><a class="header" href="#fsimage-和-edits-解析">Fsimage 和 Edits 解析</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102132707.png" alt="" /></p>
<p>查看 oiv 和 oev 命令说明</p>
<pre><code>oiv            apply the offline fsimage viewer to an fsimage 
oev            apply the offline edits viewer to an edits file 
</code></pre>
<p>1 ）oiv 查看 Fsimage 文件</p>
<p>（1）基本语法</p>
<pre><code>hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径
</code></pre>
<p>（2）案例实操</p>
<pre><code>hdfs oiv -p XML -i fsimage_0000000000000000261 -o /opt/module/hadoop-3.1.3/fsimage.xml
</code></pre>
<p>思考：Fsimage 中没有记录块所对应 DataNode，为什么？</p>
<p>在集群启动后，要求 DataNode 上报数据块信息，并间隔一段时间后再次上报。</p>
<p>2 ）oev 查看 Edits 文件</p>
<p>（1）基本语法</p>
<pre><code>hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径
</code></pre>
<p>（2）案例实操</p>
<pre><code>hdfs oev -p XML -i edits_inprogress_0000000000000000262 -o/opt/module/hadoop-3.1.3/edits.xml
</code></pre>
<p>思考：NameNode 如何确定下次开机启动的时候合并哪些 Edits？</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102133225.png" alt="" /></p>
<p>注意时间，看到每间隔1h进行一次合并</p>
<p>集群一停止、开关机也要合并一次</p>
<h3 id="checkpoint-时间设置"><a class="header" href="#checkpoint-时间设置">CheckPoint 时间设置</a></h3>
<p>1 ） 通常情况下，SecondaryNameNode 每隔一小时执行一次</p>
<p>参照：hdfs-default.xml</p>
<pre><code class="language-xml">&lt;property&gt; 
  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; 
  &lt;value&gt;3600s&lt;/value&gt; 
&lt;/property&gt; 
</code></pre>
<p>2 ） 一分钟检查一次操作次数，当操作次数达到 1 百万时，SecondaryNameNode执行一次</p>
<pre><code class="language-xml">&lt;property&gt; 
  &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; 
  &lt;value&gt;1000000&lt;/value&gt; 
&lt;description&gt;操作动作次数&lt;/description&gt; 
&lt;/property&gt; 
 
&lt;property&gt; 
  &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; 
  &lt;value&gt;60s&lt;/value&gt; 
&lt;description&gt; 1 分钟检查一次操作次数&lt;/description&gt; 
&lt;/property&gt; 
</code></pre>
<h2 id="datanode"><a class="header" href="#datanode">DataNode</a></h2>
<h3 id="datanode-工作机制"><a class="header" href="#datanode-工作机制">DataNode 工作机制</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102130953.png" alt="" /></p>
<p>（1）一个数据块在 DataNode 上以文件形式存储在磁盘上，包括两个文件：一个是数据本身，一个是元数据包括数据块的长度、块数据的校验和以及时间戳。</p>
<p>（2）DataNode 启动后向 NameNode 注册，通过后，周期性（6 小时）的向 NameNode 上报所有的块信息。</p>
<p>DN 向 NN 汇报当前解读信息的时间间隔，默认 6 小时。</p>
<p>相关配置参数如下：</p>
<pre><code class="language-xml">&lt;property&gt; 
  &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; 
  &lt;value&gt;21600000&lt;/value&gt; 
  &lt;description&gt;Determines block reporting interval in 
milliseconds.&lt;/description&gt; 
&lt;/property&gt; 
</code></pre>
<p>注意：这里value值单位是毫秒</p>
<p>DN 扫描自己节点块信息列表的时间，默认 6 小时</p>
<p>相关配置参数如下：</p>
<pre><code class="language-xml">&lt;property&gt; 
  &lt;name&gt;dfs.datanode.directoryscan.interval&lt;/name&gt; 
  &lt;value&gt;21600s&lt;/value&gt; 
  &lt;description&gt;Interval in seconds for Datanode to scan data 
  directories and reconcile the difference between blocks in memory and on 
the disk. 
  Support multiple time unit suffix(case insensitive), as described 
  in dfs.heartbeat.interval. 
  &lt;/description&gt; 
&lt;/property&gt; 
</code></pre>
<p>注意：这里value值单位是秒</p>
<p>（3）心跳是每 3 秒一次，心跳返回结果带有 NameNode 给该 DataNode 的命令如复制块数据到另一台机器 或删除某个数据块。 如果超过 10 分钟+30 秒没有收到某个 DataNode 的心跳，则认为该节点不可用（挂了），不会再向其传输信息。</p>
<p>（4）集群运行中可以安全加入和删除一些机器。</p>
<h3 id="数据完整性"><a class="header" href="#数据完整性">数据完整性</a></h3>
<p>思考：DataNode 节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？</p>
<p>如下是 DataNode 节点保证数据完整性的方法。</p>
<p>（1）当 DataNode 读取 Block 的时候，它会计算 CheckSum。</p>
<p>（2）如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。</p>
<p>（3）Client 读取其他 DataNode 上的 Block。</p>
<p>（4）常见的校验算法 crc（32），md5（128），sha1（160）</p>
<p>（5）DataNode 在其文件创建后周期验证 CheckSum。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102131756.png" alt="" /></p>
<h3 id="datanode掉线时限参数设置"><a class="header" href="#datanode掉线时限参数设置">DataNode掉线时限参数设置</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102131954.png" alt="" /></p>
<p>相关配置参数如下：</p>
<pre><code class="language-xml">&lt;property&gt; 
    &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; 
    &lt;value&gt;300000&lt;/value&gt; 
&lt;/property&gt; 
 
&lt;property&gt; 
    &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; 
    &lt;value&gt;3&lt;/value&gt; 
&lt;/property&gt; 
</code></pre>
<p>需要注意的是 hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval 的单位为秒。</p>
<p>DataNode 被中止之后，可以执行<code>hdfs --daemon start datanode</code>重启</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs读写流程"><a class="header" href="#hdfs读写流程">HDFS读写流程</a></h1>
<ul>
<li><a href="Hadoop/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E5%86%99%E6%95%B0%E6%8D%AE">写数据</a>
<ul>
<li><a href="Hadoop/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E6%96%87%E4%BB%B6%E5%86%99%E5%85%A5">文件写入</a></li>
<li><a href="Hadoop/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97">网络拓扑-节点距离计算</a></li>
<li><a href="Hadoop/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5%E5%89%AF%E6%9C%AC%E5%AD%98%E5%82%A8%E8%8A%82%E7%82%B9%E9%80%89%E6%8B%A9">机架感知（副本存储节点选择）</a></li>
</ul>
</li>
<li><a href="Hadoop/HDFS%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B.html#%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B">读数据流程</a></li>
</ul>
<h2 id="写数据"><a class="header" href="#写数据">写数据</a></h2>
<h3 id="文件写入"><a class="header" href="#文件写入">文件写入</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102124812.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102124951.png" alt="" /></p>
<p>（1）客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。</p>
<p>（2）NameNode 返回是否可以上传。</p>
<p>（3）客户端请求第一个 Block 上传到哪几个 DataNode 服务器上。</p>
<p>（4）NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。</p>
<p>（5） 客户端通过 FSDataOutputStream 模块请求 dn1 上传数据， dn1 收到请求会继续调用
dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。</p>
<p>（6）dn1、dn2、dn3 逐级应答客户端。</p>
<p>（7） 客户端开始往 dn1 上传第一个 Block （先从磁盘读取数据放到一个本地内存缓存） ，
以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个 packet
会放入一个应答队列等待应答。</p>
<p>（8）当一个 Block 传输完成之后， 客户端再次请求 NameNode 上传第二个 Block 的服务
器。（重复执行 3-7 步）。</p>
<h3 id="网络拓扑-节点距离计算"><a class="header" href="#网络拓扑-节点距离计算">网络拓扑-节点距离计算</a></h3>
<p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接
收数据。那么这个最近距离怎么计算呢？</p>
<p><strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p>
<p>例如，假设有数据中心 d1 机架 r1 中的节点 n1。该节点可以表示为/d1/r1/n1。利用这种
标记，这里给出四种距离描述。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102125235.png" alt="" /></p>
<h3 id="机架感知副本存储节点选择"><a class="header" href="#机架感知副本存储节点选择">机架感知（副本存储节点选择）</a></h3>
<p>1 ）机架感知说明</p>
<p>（1）官方说明</p>
<p>http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication</p>
<blockquote>
<p>For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on the local machine if the writer is on a datanode, otherwise on a random datanode, another replica on a node in a different (remote) rack, and the last on a different node in the same remote rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance.</p>
</blockquote>
<p>（2）源码说明</p>
<p>Crtl + n 查找 BlockPlacementPolicyDefault，在该类中查找 chooseTargetInOrder 方法。</p>
<p>2 ）Hadoop3.1.3 副本节点选择</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102125546.png" alt="" /></p>
<p>第一个选择最近的节点</p>
<p>第二个节点跨机架保证副本的可靠性</p>
<p>第三个节点还是兼顾效率</p>
<h2 id="读数据流程"><a class="header" href="#读数据流程">读数据流程</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102125848.png" alt="" /></p>
<p>（1）客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。</p>
<p>（2）挑选一台 DataNode服务器，请求读取数据。挑选过程遵循就近原则，然后随机（会考虑当前节点的负载能力）。</p>
<p>（3）DataNode 开始传输数据给客户端，从磁盘里面读取数据输入流，以 Packet 为单位来做校验。</p>
<p>（4）客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs的shell操作"><a class="header" href="#hdfs的shell操作">HDFS的Shell操作</a></h1>
<ul>
<li><a href="Hadoop/HDFS-Shell%E6%93%8D%E4%BD%9C.html#21-%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95">2.1 基本语法</a></li>
<li><a href="Hadoop/HDFS-Shell%E6%93%8D%E4%BD%9C.html#22-%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8">2.2 命令大全</a></li>
<li><a href="Hadoop/HDFS-Shell%E6%93%8D%E4%BD%9C.html#23-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AE%9E%E6%93%8D">2.3 常用命令实操</a>
<ul>
<li><a href="Hadoop/HDFS-Shell%E6%93%8D%E4%BD%9C.html#231-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C">2.3.1 准备工作</a></li>
<li><a href="Hadoop/HDFS-Shell%E6%93%8D%E4%BD%9C.html#232-%E4%B8%8A%E4%BC%A0">2.3.2 上传</a></li>
<li><a href="Hadoop/HDFS-Shell%E6%93%8D%E4%BD%9C.html#233-%E4%B8%8B%E8%BD%BD">2.3.3 下载</a></li>
<li><a href="Hadoop/HDFS-Shell%E6%93%8D%E4%BD%9C.html#234-hdfs-%E7%9B%B4%E6%8E%A5%E6%93%8D%E4%BD%9C">2.3.4 HDFS 直接操作</a></li>
</ul>
</li>
</ul>
<h2 id="21-基本语法"><a class="header" href="#21-基本语法">2.1 基本语法</a></h2>
<p><code>hadoop fs 具体命令</code> 和 <code>hdfs dfs 具体命令</code>两者等价。</p>
<h2 id="22-命令大全"><a class="header" href="#22-命令大全">2.2 命令大全</a></h2>
<p>执行<code>bin/hadoop fs</code>或<code>hdfs dfs</code>查看所有相关命令。</p>
<h2 id="23-常用命令实操"><a class="header" href="#23-常用命令实操">2.3 常用命令实操</a></h2>
<h3 id="231-准备工作"><a class="header" href="#231-准备工作">2.3.1 准备工作</a></h3>
<p>1）启动 Hadoop 集群（方便后续的测试）</p>
<p>其实如果看了之前的Hadoop入门(十三)——集群常用知识(面试题)与技巧总结里面写了一个快速启动集群的脚本，只需要一个命令即可启动集群</p>
<pre><code>[leokadia@hadoop102 bin]$ myhadoop.sh start
</code></pre>
<p>或者也可以像之前一样分别在102，103上使用以下两个命令：</p>
<pre><code>[leokadia@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
[leokadia@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
</code></pre>
<p>2）-help：输出这个命令参数</p>
<p>如果对哪一个命令的用法不是特别清楚，用以下指令查看该命令如何使用的：</p>
<p><code>hadoop fs -help rm</code></p>
<p>3）创建/Marvel 文件夹</p>
<p><code>hadoop fs -mkdir /Marvel</code></p>
<p>打开之后就有了一个漫威的文件夹</p>
<p>后续的命令就在漫威的文件夹里面开启我们的漫威之旅</p>
<p>一共分三大类命令：上传、下载、HDFS直接操作</p>
<h3 id="232-上传"><a class="header" href="#232-上传">2.3.2 上传</a></h3>
<p>注意以下命令执行目录都是<code>hadoop-3.1.3</code>，（这里其实可以是任何路径）</p>
<p>1）<code>-moveFromLocal</code>：从本地移动到 HDFS</p>
<p>在虚拟机中执行<code>vim Avengers.txt</code>创建一个txt文件</p>
<p>输入：The Avengers</p>
<p>执行<code>hadoop fs -moveFromLocal ./Avengers.txt /Marvel</code>将其上传到HDFS</p>
<p>源文件：Avengers.txt</p>
<p>目的目录：Marvel</p>
<p>2）<code>-copyFromLocal</code>：从本地文件系统中拷贝文件到 HDFS 路径去</p>
<p><code>hadoop fs -copyFromLocal X-Men.txt /Marvel</code></p>
<p>3）<code>-put</code>：等同于 -<code>copyFromLocal</code>，生产环境更习惯用 <code>put</code></p>
<p><code>hadoop fs -put ./Fantastic_Four.txt /Marvel</code></p>
<p>4）<code>-appendToFile</code>：追加一个文件到已经存在的文件末尾</p>
<p>HDFS只能追加，不允许随机修改，而且只能在文件的末尾进行追加</p>
<p><code>hadoop fs -appendToFile Iron_Man.txt /Marvel/Avengers.txt</code></p>
<h3 id="233-下载"><a class="header" href="#233-下载">2.3.3 下载</a></h3>
<p>注意以下命令执行目录都是<code>hadoop-3.1.3</code></p>
<p>1）<code>-copyToLocal</code>：从 HDFS 拷贝到本地</p>
<p>将Marvel中的Averagers.txt拷贝到当前文件夹</p>
<p><code>hadoop fs -copyToLocal /Marvel/Avengers.txt ./</code></p>
<p>2）<code>-get</code>：等同于 <code>-copyToLocal</code>，生产环境更习惯用 <code>get</code></p>
<p>将Marvel中的Averagers.txt拷贝到当前文件夹，并将拷贝的文件名更改为The_Avengers.txt</p>
<p><code>hadoop fs -get /Marvel/Avengers.txt ./The_Avengers.txt</code></p>
<h3 id="234-hdfs-直接操作"><a class="header" href="#234-hdfs-直接操作">2.3.4 HDFS 直接操作</a></h3>
<p>注意以下命令执行目录都是<code>hadoop-3.1.3</code>（任何路径都可以）</p>
<p>1）-ls: 显示目录信息</p>
<p>查询根目录</p>
<p><code>hadoop fs -ls /</code></p>
<p>查询Marvel目录</p>
<p><code>hadoop fs -ls /Marvel</code></p>
<p>2）-cat：显示文件内容</p>
<p><code>hadoop fs -cat /Marvel/Avengers.txt</code></p>
<p>3）-chgrp、-chmod、-chown：Linux 文件系统中的用法一样，修改文件所属权限</p>
<p><code>hadoop fs -chmod 666 /Marvel/Avengers.txt</code></p>
<p><code>hadoop fs -chown leokadia:leokadia /Marvel/Avengers.txt</code></p>
<p>4）-mkdir：创建路径</p>
<p><code>hadoop fs -mkdir /DC</code></p>
<p>再建一个Disney文件夹（因为后来漫威被迪士尼收购了）</p>
<p><code>hadoop fs -mkdir /Disney</code></p>
<p>5）-cp：从 HDFS 的一个路径拷贝到 HDFS 的另一个路径</p>
<p><code>hadoop fs -cp /Marvel/Avengers.txt /Disney</code></p>
<p>这个命令是拷贝，也就是说复仇者联盟在漫威中还有一份</p>
<p>6）-mv：在 HDFS 目录中移动文件</p>
<p><code>hadoop fs -mv /Marvel/Fantastic_Four.txt /Disney</code></p>
<p><code>hadoop fs -mv /Marvel/X-Men.txt /Disney</code></p>
<p>成功移动，注意，这个是移动，所以原来的文件夹里这两个文件没有了。</p>
<p>7）-tail：显示一个文件的末尾 1kb 的数据</p>
<p><code>hadoop fs -tail /Marvel/Avengers.txt</code></p>
<p>8）-rm：删除文件或文件夹</p>
<p><code>hadoop fs -rm /Marvel/Avengers.txt</code></p>
<p>9）-rm -r：递归删除目录及目录里面内容</p>
<p>删除文件夹及里面的内容</p>
<p><code>hadoop fs -rm -r /Marvel</code></p>
<p>没有Marvel文件夹了，呜呜呜。。。</p>
<p>注意：用 rm-r 之类的删除命令一定要慎重，慎重，再慎重！！！</p>
<p>10）-du 统计文件夹的大小信息</p>
<p><code>hadoop fs -du -s -h /Disney</code></p>
<p>说明：43 表示文件大小；129 表示 43*3 个副本；/Disney 表示查看的目录</p>
<p><code>hadoop fs -du -h /Disney</code></p>
<p>22+15+6=43 所以Disney文件总大小为43</p>
<p>11）-setrep：设置 HDFS 中文件的副本数量</p>
<p>给Avengers.txt设置10个副本：</p>
<p><code>hadoop fs -setrep 10 /Disney/Avengers.txt</code></p>
<p>这里设置的副本数只是记录在 NameNode 的元数据中， 是否真的会有这么多副本， 还得看DataNode 的数量。因为目前只有 3 台设备，最多也就 3 个副本，只有节点数的增加到 10台时，副本数才能达到 10。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hdfs的api操作"><a class="header" href="#hdfs的api操作">HDFS的API操作</a></h1>
<ul>
<li><a href="Hadoop/HDFS-API%E6%93%8D%E4%BD%9C.html#31-%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87">3.1 客户端环境准备</a></li>
<li><a href="Hadoop/HDFS-API%E6%93%8D%E4%BD%9C.html#32-hdfs-%E7%9A%84-api-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.2 HDFS 的 API 案例实操</a>
<ul>
<li><a href="Hadoop/HDFS-API%E6%93%8D%E4%BD%9C.html#321-%E7%94%A8%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9C%E7%A8%8B%E5%88%9B%E5%BB%BA%E7%9B%AE%E5%BD%95">3.2.1 用客户端远程创建目录</a></li>
<li><a href="Hadoop/HDFS-API%E6%93%8D%E4%BD%9C.html#322-hdfs-%E7%94%A8%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E6%B5%8B%E8%AF%95-%E5%8F%82%E6%95%B0%E4%BC%98%E5%85%88%E7%BA%A7-">3.2.2 HDFS 用客户端上传文件（测试 参数优先级 ）</a></li>
<li><a href="Hadoop/HDFS-API%E6%93%8D%E4%BD%9C.html#323-hdfs-%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD">3.2.3 HDFS 文件下载</a></li>
<li><a href="Hadoop/HDFS-API%E6%93%8D%E4%BD%9C.html#324-hdfs-%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E5%92%8C%E7%9B%AE%E5%BD%95">3.2.4 HDFS 删除文件和目录</a></li>
<li><a href="Hadoop/HDFS-API%E6%93%8D%E4%BD%9C.html#325-%E6%96%87%E4%BB%B6%E7%9A%84%E6%9B%B4%E5%90%8D%E5%92%8C%E7%A7%BB%E5%8A%A8">3.2.5 文件的更名和移动</a></li>
<li><a href="Hadoop/HDFS-API%E6%93%8D%E4%BD%9C.html#326-%E9%80%9A%E8%BF%87%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E6%96%B9%E5%BC%8F%E8%8E%B7%E5%8F%96-hdfs-%E6%96%87%E4%BB%B6-%E8%AF%A6%E6%83%85%E4%BF%A1%E6%81%AF">3.2.6 通过客户端的方式获取 HDFS 文件 详情信息</a></li>
<li><a href="Hadoop/HDFS-API%E6%93%8D%E4%BD%9C.html#327-hdfs-%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%B9%E5%88%A4%E6%96%AD">3.2.7 HDFS 文件和文件夹判断</a></li>
</ul>
</li>
</ul>
<p>刚刚（二）讲的是用Shell/Hadoop fs/HDFS/dfs的一些相关操作，相当于是在集群内部，跟集群的一些客户端打交道，这章讲的是：我们希望在Windows环境（办公环境）对远程的集群进行一个客户端访问，于是现在就在Windows环境上写代码，写HDFS客户端代码，远程连接上集群，对它们进行增删改查相关操作。</p>
<h2 id="31-客户端环境准备"><a class="header" href="#31-客户端环境准备">3.1 客户端环境准备</a></h2>
<p>想让我们的windows能够连接上远程的Hadoop集群，windows里面也得有相关的环境变量</p>
<p>1） 下载 hadoop-3.1.0 （windows版）到非中文路径 （比如E:\Sofware）</p>
<p>2 ） 配置 HADOOP_HOME 环境 变量</p>
<p>3 ） 配置 Path 环境 变量。</p>
<p>将HADOOP_HOME目录添加到对应的PATH目录</p>
<p>注意： 如果环境变量不起作用，可以 重启电脑 试试。</p>
<p>验证 Hadoop 环境变量是否正常。双击 winutils.exe，如果报如下错误。</p>
<p>说明缺少微软运行库 （正版系统往往有这个问题） 。 下载微软运行库安装包双击安装即可。</p>
<p>4 ） 在 IDEA 中 创建一个 Maven 工程 HdfsClientDemo ，并导入相应的依赖坐标+ 日志</p>
<p>创建Maven工程以及进行相关配置</p>
<p>5 ） 创建包名 ：com.leokadia.hdfs</p>
<p>6 ） 创建 HdfsClient 类</p>
<p>创建好了客户端类，接下来写代码操作远程的服务器集群集群</p>
<p>7 ） 执行 程序</p>
<p>客户端去操作 HDFS 时，是有一个用户身份的。默认情况下，HDFS 客户端 API 会从采用 Windows 默认用户访问 HDFS，会报权限异常错误。所以在访问 HDFS 时，一定要配置用户。</p>
<h2 id="32-hdfs-的-api-案例实操"><a class="header" href="#32-hdfs-的-api-案例实操">3.2 HDFS 的 API 案例实操</a></h2>
<h3 id="321-用客户端远程创建目录"><a class="header" href="#321-用客户端远程创建目录">3.2.1 用客户端远程创建目录</a></h3>
<pre><code class="language-java">package com.leokadia.hdfs;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.Configuration;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * @author sa
 * @create 2021-05-04 16:37
 *
 *
 * 客户端代码常用套路
 * 1、获取一个客户端对象
 * 2、执行相关的操作命令
 * 3、关闭资源
 * HDFS zookeeper
 */
public class HdfsClient {

    private FileSystem fs;

    @Before
    public void init() throws IOException, URISyntaxException, InterruptedException {

        //连接集群的nn地址
        URI uri = new URI("hdfs://hadoop102:8020");

        //创建一个配置文件
        Configuration configuration = new Configuration();

        //用户
        String user = "leokadia";

        // 1 获取客户端对象
        fs = FileSystem.get(uri, configuration, user);
    }

    @After
    public void close() throws IOException {
        // 3 关闭资源
        fs.close();
    }

    @Test
    public void testmkdir() throws URISyntaxException,IOException,InterruptedException {
        // 2 创建一个文件夹
        fs.mkdirs(new Path("/Marvel/Avengers"));

    }
}
</code></pre>
<p>运行@Test</p>
<p>成功创建文件夹</p>
<h3 id="322-hdfs-用客户端上传文件测试-参数优先级-"><a class="header" href="#322-hdfs-用客户端上传文件测试-参数优先级-">3.2.2 HDFS 用客户端上传文件（测试 参数优先级 ）</a></h3>
<p>1）上传文件</p>
<p>先在D盘根目录下创建一个待上传的文件</p>
<p>在刚刚的代码中加入如下代码</p>
<pre><code class="language-java"> // 上传
    @Test
    public void testPut() throws IOException {
        //参数解读：参数一：表示删除原数据；参数二：是否允许覆盖；参数三：原数据路径；参数四：目的地路径
        fs.copyFromLocalFile(false,false,new Path("D:\\Iron_Man.txt"),new Path("hdfs://hadoop102/Marvel/Avengers"));
    }

</code></pre>
<p>点击运行</p>
<p>2 ） 将 hdfs-site.xml 拷贝到项目的 resources 资源目录下</p>
<p>已知服务器的默认配置 （xxx-default.xml） 中的副本数是3，现在resources下新建一个file——hdfs-site.xml修改副本数，测试二者的优先级</p>
<p>在resources下新建一个file——hdfs-site.xml</p>
<p>将下面代码粘贴到里面：（修改副本数为1）</p>
<pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt; 
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt; 
 
&lt;configuration&gt; 
  &lt;property&gt; 
   &lt;name&gt;dfs.replication&lt;/name&gt;       
    &lt;value&gt;1&lt;/value&gt; 
  &lt;/property&gt; 
&lt;/configuration&gt; 
</code></pre>
<p>如果再上传一个文件，它的副本数为1，说明 resources 资源目录下的hdfs-site.xml 优先级高
再创建一个测试文件</p>
<p>执行上传，发现副本数为1</p>
<p>说明在项目资源目录下用户自定义的配置文件高</p>
<p>再测试客户端代码中配置副本的值的优先级：</p>
<p>在源代码中加上：</p>
<pre><code class="language-java">configuration.set("dfs.replication","2");
</code></pre>
<p>再运行一遍刚刚的代码</p>
<p>发现Spider_Man.txt副本数变为2了</p>
<p>说明客户端代码中设置的值 &gt;ClassPath 下的用户自定义配置文件</p>
<p>3 ）总结： 参数 优先级</p>
<p>参数优先级排序：</p>
<p>（1）客户端代码中设置的值 &gt;
（2）ClassPath 下的用户自定义配置文件 &gt;
（3） 然后是服务器的自定义配置 （xxx-site.xml） &gt;
（4） 服务器的默认配置 （xxx-default.xml）</p>
<h3 id="323-hdfs-文件下载"><a class="header" href="#323-hdfs-文件下载">3.2.3 HDFS 文件下载</a></h3>
<pre><code class="language-java">// 文件下载
    @Test
    public void testGet() throws IOException {
        //参数解读：参数一： boolean delSrc 指是否将原文件删除；参数二：Path src 指要下载的原文件路径
        // 参数三：Path dst 指将文件下载到的目标地址路径；参数四：boolean useRawLocalFileSystem 是否开启文件校验
        fs.copyToLocalFile(false, new Path("hdfs://hadoop102/Marvel/Avengers/Iron_Man.txt"), new Path("D:\\Robert.txt"), false);
    }

</code></pre>
<h3 id="324-hdfs-删除文件和目录"><a class="header" href="#324-hdfs-删除文件和目录">3.2.4 HDFS 删除文件和目录</a></h3>
<pre><code class="language-java"> // 删除
    @Test
    public void testRm() throws IOException {

        // 参数解读：参数1：要删除的路径； 参数2 ： 是否递归删除
        // 删除文件(不再演示了)
        fs.delete(new Path("/jdk-8u212-linux-x64.tar.gz"),false);

        // 删除空目录
        fs.delete(new Path("/delete_test_empty"), false);

        // 删除非空目录
        fs.delete(new Path("/Marvel"), true);
    }

</code></pre>
<h3 id="325-文件的更名和移动"><a class="header" href="#325-文件的更名和移动">3.2.5 文件的更名和移动</a></h3>
<p>新建一个测试文件夹和相应的测试文件</p>
<pre><code class="language-java">// 文件的更名和移动
    @Test
    public void testmv() throws IOException {
        // 参数解读：参数1 ：原文件路径； 参数2 ：目标文件路径
        // 对文件名称的修改
        fs.rename(new Path("/move/from.txt"), new Path("/move/new.txt"));

        // 文件的移动和更名
        fs.rename(new Path("/move/new.txt"),new Path("/to.txt"));

        // 目录更名
        fs.rename(new Path("/move"), new Path("/shift"));

    }

</code></pre>
<h3 id="326-通过客户端的方式获取-hdfs-文件-详情信息"><a class="header" href="#326-通过客户端的方式获取-hdfs-文件-详情信息">3.2.6 通过客户端的方式获取 HDFS 文件 详情信息</a></h3>
<p>查看文件名称、权限、长度、块信息</p>
<pre><code class="language-java">// 获取文件详细信息
    @Test
    public void fileDetail() throws IOException {

        // 获取所有文件信息
        RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path("/"), true);

        // 遍历文件
        while (listFiles.hasNext()) {
            LocatedFileStatus fileStatus = listFiles.next();

            System.out.println("==========" + fileStatus.getPath() + "=========");
            System.out.println(fileStatus.getPermission());
            System.out.println(fileStatus.getOwner());
            System.out.println(fileStatus.getGroup());
            System.out.println(fileStatus.getLen());
            System.out.println(fileStatus.getModificationTime());
            System.out.println(fileStatus.getReplication());
            System.out.println(fileStatus.getBlockSize());
            System.out.println(fileStatus.getPath().getName());

            // 获取块信息
            BlockLocation[] blockLocations = fileStatus.getBlockLocations();

            System.out.println(Arrays.toString(blockLocations));

        }
    }

</code></pre>
<h3 id="327-hdfs-文件和文件夹判断"><a class="header" href="#327-hdfs-文件和文件夹判断">3.2.7 HDFS 文件和文件夹判断</a></h3>
<pre><code class="language-java">// 判断是文件夹还是文件
    @Test
    public void testFile() throws IOException {

        FileStatus[] listStatus = fs.listStatus(new Path("/"));

        for (FileStatus status : listStatus) {

            if (status.isFile()) {
                System.out.println("文件：" + status.getPath().getName());
            } else {
                System.out.println("目录：" + status.getPath().getName());
            }
        }
    }

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="mapreduce"><a class="header" href="#mapreduce">MapReduce</a></h1>
<ul>
<li><a href="Hadoop/MapReduce.html#%E6%A6%82%E8%A7%88">概览</a></li>
<li><a href="Hadoop/MapReduce.html#mapreduce-%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93">MapReduce 开发总结</a></li>
<li><a href="Hadoop/MapReduce.html#%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">常见错误及解决方案</a></li>
</ul>
<h2 id="概览"><a class="header" href="#概览">概览</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102144251.png" alt="" /></p>
<p><a href="https://yangmour.github.io/2022/11/02/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83/hadoop-3.1.3/04_%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8BHadoop%EF%BC%88MapReduce%EF%BC%89V3.3/">完整博文来源</a></p>
<h2 id="mapreduce-开发总结"><a class="header" href="#mapreduce-开发总结">MapReduce 开发总结</a></h2>
<p><strong>1</strong>）输入数据接口：InputFormat</p>
<p>（1）默认使用的实现类是：TextInputFormat</p>
<p>（2）TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为 key，行内容作为 value 返回。</p>
<p>（3）CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</p>
<p><strong>2</strong>）逻辑处理接口：Mapper</p>
<p>用户根据业务需求实现其中三个方法：map ()、setup () 和 cleanup ()</p>
<p><strong>3</strong>）Partitioner 分区</p>
<p>（1）有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；<code>key.hashCode ()&amp;Integer.MAXVALUE % numReduces</code></p>
<p>（2）如果业务上有特别的需求，可以自定义分区。</p>
<p><strong>4</strong>）Comparable 排序</p>
<p>（1）当我们用自定义的对象作为 key 来输出时，就必须要实现 <code>WritableComparable</code> 接口，重写其中的 <code>compareTo ()</code> 方法。</p>
<p>（2）部分排序：对最终输出的每一个文件进行内部排序。</p>
<p>（3）全排序：对所有数据进行排序，通常只有一个 Reduce。</p>
<p>（4）二次排序：排序的条件有两个。</p>
<p><strong>5</strong>）Combiner 合并</p>
<p>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。</p>
<p><strong>6</strong>）逻辑处理接口：Reducer</p>
<p>用户根据业务需求实现其中三个方法：reduce ()、setup () 和 cleanup ()</p>
<p><strong>7</strong>）输出数据接口：OutputFormat</p>
<p>（1）默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</p>
<p>（2）用户还可以自定义 OutputFormat。</p>
<h2 id="常见错误及解决方案"><a class="header" href="#常见错误及解决方案">常见错误及解决方案</a></h2>
<p>1）导包容易出错。尤其 Text 和 CombineTextInputFormat。</p>
<p>2）Mapper 中第一个输入的参数必须是 LongWritable 或者 NullWritable，不可以是 IntWritable. 报的错误是类型转换异常。</p>
<p>3）java.lang.Exception: java.io.IOException: Illegal partition for 13926435656 (4)，说明 Partition 和 ReduceTask 个数没对上，调整 ReduceTask 个数。</p>
<p>4）如果分区数不是 1，但是 reducetask 为 1，是否执行分区过程。答案是：不执行分区过程。因为在 MapTask 的源码中，执行分区的前提是先判断 ReduceNum 个数是否大于 1。不大于 1 肯定不执行。</p>
<p>5）在 Windows 环境编译的 jar 包导入到 Linux 环境中运行，</p>
<pre><code>hadoop jar wc.jar com.atguigu.mapreduce.wordcount.WordCountDriver/user/atguigu//user/atguigu/output
</code></pre>
<p>报如下错误：</p>
<pre><code>Exception in thread “main” java.lang.UnsupportedClassVersionError: com/atguigu/mapreduce/wordcount/WordCountDriver : Unsupported major.minor version 52.0
</code></pre>
<p>原因是 Windows 环境用的 jdk1.7，Linux 环境用的 jdk1.8。</p>
<p>解决方案：统一 jdk 版本。</p>
<p>6）缓存 <code>pd.txt</code> 小文件案例中，报找不到 <code>pd.txt</code> 文件</p>
<p>原因：大部分为路径书写错误。还有就是要检查 <code>pd.txt.txt</code> 的问题。还有个别电脑写相对路径找不到 <code>pd.txt</code>，可以修改为绝对路径。</p>
<p>7）报类型转换异常。</p>
<p>通常都是在驱动函数中设置 <code>Map</code> 输出和最终输出时编写错误。</p>
<p>Map 输出的 key 如果没有排序，也会报类型转换异常。</p>
<p>8）集群中运行 <code>wc.jar</code> 时出现了无法获得输入文件。</p>
<p>原因：<code>WordCount</code> 案例的输入文件不能放用 <code>HDFS</code> 集群的根目录。</p>
<p>9）出现了如下相关异常</p>
<pre><code>Exception in thread "main" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0 (Ljava/lang/String;I) Z  
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0 (Native Method)  
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access (NativeIO.java:609)  
	at org.apache.hadoop.fs.FileUtil.canRead (FileUtil.java:977)  
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.  
	at org.apache.hadoop.util.Shell.getQualifiedBinPath (Shell.java:356)  
	at org.apache.hadoop.util.Shell.getWinUtilsPath (Shell.java:371)  
	at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:364)
</code></pre>
<p>解决方案：拷贝 <code>hadoop.dll</code> 文件到 <code>Windows</code> 目录 <code>C:\Windows\System32</code>。个别同学电脑还需要修改 <code>Hadoop</code> 源码。</p>
<p>方案二：创建包 <code>org.apache.hadoop.io.nativeio</code>，并将 NativeIO.java 拷贝到该包名下</p>
<p><a href="https://blog.csdn.net/syl_ccc/article/details/105946007">详情参考 org.apache.hadoop.io.nativeio.NativeIO$Windows.access0 (Ljava/lang/String;I) Z 的解决办法</a></p>
<p>10）自定义 Outputformat 时，注意在 RecordWirter 中的 close 方法必须关闭流资源。否则输出的文件内容中数据为空。</p>
<pre><code class="language-java">@Override  
public void close (TaskAttemptContext context) throws IOException, InterruptedException {  
		if (atguigufos != null) {  
			atguigufos.close ();  
		}  
		if (otherfos != null) {  
			otherfos.close ();  
		}  
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="mapreduce-概述"><a class="header" href="#mapreduce-概述">MapReduce 概述</a></h1>
<ul>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E5%AE%9A%E4%B9%89">MapReduce 定义</a></li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E4%BC%98%E7%BC%BA%E7%82%B9">MapReduce 优缺点</a>
<ul>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#%E4%BC%98%E7%82%B9">优点</a></li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#%E7%BC%BA%E7%82%B9">缺点</a></li>
</ul>
</li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3">MapReduce 核心编程思想</a></li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E8%BF%9B%E7%A8%8B">MapReduce 进程</a></li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#%E5%AE%98%E6%96%B9-wordcount-%E6%BA%90%E7%A0%81">官方 WordCount 源码</a></li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96%E7%B1%BB%E5%9E%8B">常用数据序列化类型</a></li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#mapreduce-%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83">MapReduce 编程规范</a></li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#wordcount-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">WordCount 案例实操</a>
<ul>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#%E6%9C%AC%E5%9C%B0%E6%B5%8B%E8%AF%95">本地测试</a></li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#wordcount-%E6%A1%88%E4%BE%8B-debug-%E8%B0%83%E8%AF%95">WordCount 案例 Debug 调试</a></li>
<li><a href="Hadoop/MapReduce%E6%A6%82%E8%BF%B0.html#%E6%8F%90%E4%BA%A4%E5%88%B0%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95">提交到集群测试</a></li>
</ul>
</li>
</ul>
<h2 id="mapreduce-定义"><a class="header" href="#mapreduce-定义">MapReduce 定义</a></h2>
<p>MapReduce 是一个分布式运算程序的编程框架，是用户开发 “基于 Hadoop 的数据分析应用” 的核心框架。</p>
<p>MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。</p>
<h2 id="mapreduce-优缺点"><a class="header" href="#mapreduce-优缺点">MapReduce 优缺点</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102144435.png" alt="" /></p>
<h3 id="优点"><a class="header" href="#优点">优点</a></h3>
<p>1 ）MapReduce 易于编程</p>
<p>它简单的实现一些接口， 就可以完成一个分布式程序， 这个分布式程序可以分布到大量廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得 MapReduce 编程变得非常流行。</p>
<p>2 ） 良好的扩展性</p>
<p>当你的计算资源不能得到满足的时候， 你可以通过简单的增加机器来扩展它的计算能力。</p>
<p>3 ） 高容错性</p>
<p>MapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上， 这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败， 而且这个过程不需要人工参与， 而完全是由 Hadoop 内部完成的。</p>
<p>4 ） 适合 PB 级以上海量数据的离线处理</p>
<p>可以实现上千台服务器集群并发工作，提供数据处理能力。</p>
<h3 id="缺点"><a class="header" href="#缺点">缺点</a></h3>
<p>1 ） 不擅长实时计算</p>
<p>MapReduce 无法像 MySQL 一样，在毫秒或者秒级内返回结果。</p>
<p>2 ） 不擅长流式计算</p>
<p>流式计算的输入数据是动态的， 而 MapReduce 的输入数据集是静态的， 不能动态变化。
这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。</p>
<p>3 ） 不擅长 DAG （有向无环图）计算</p>
<p>多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce 并不是不能做， 而是使用后， 每个 MapReduce 作业的输出结果都会写入到磁盘，会造成大量的磁盘 IO，导致性能非常的低下。</p>
<h2 id="mapreduce-核心编程思想"><a class="header" href="#mapreduce-核心编程思想">MapReduce 核心编程思想</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102144500.png" alt="" /></p>
<p>（1）分布式的运算程序往往需要分成至少 2 个阶段。</p>
<p>（2）第一个阶段的 MapTask 并发实例，完全并行运行，互不相干。</p>
<p>（3）第二个阶段的 ReduceTask 并发实例互不相干，但是他们的数据依赖于上一个阶段的所有 MapTask 并发实例的输出。</p>
<p>（4）MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行。</p>
<p>总结：分析 WordCount 数据流走向深入理解 MapReduce 核心思想。</p>
<h2 id="mapreduce-进程"><a class="header" href="#mapreduce-进程">MapReduce 进程</a></h2>
<p>任务、job、MR 都表示任务。</p>
<p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程：</p>
<p>（1）MrAppMaster：负责整个程序的过程调度及状态协调。</p>
<p>（2）MapTask：负责 Map 阶段的整个数据处理流程。</p>
<p>（3）ReduceTask：负责 Reduce 阶段的整个数据处理流程。</p>
<h2 id="官方-wordcount-源码"><a class="header" href="#官方-wordcount-源码">官方 WordCount 源码</a></h2>
<p>采用反编译工具反编译源码，发现 WordCount 案例有 Map 类、Reduce 类和驱动类，且数据的类型是 Hadoop 自身封装的序列化类型。</p>
<p>如何查看里面的代码程序呢？使用反编译工具 jd-gui</p>
<p><a href="https://blog.csdn.net/zlbdmm/article/details/104653823">java 反编译工具 jd-gui 下载与使用</a></p>
<p>查看 <code>hadoop-mapreduce-examples-3.3.6.jar</code> 得到如下的源代码：</p>
<pre><code class="language-java">/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.examples;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class WordCount {

  public static class TokenizerMapper 
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{
    
    private final static IntWritable one = new IntWritable (1);
    private Text word = new Text ();
      
    public void map (Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer (value.toString ());
      while (itr.hasMoreTokens ()) {
        word.set (itr.nextToken ());
        context.write (word, one);
      }
    }
  }
  
  public static class IntSumReducer 
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
    private IntWritable result = new IntWritable ();

    public void reduce (Text key, Iterable&lt;IntWritable&gt; values, 
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get ();
      }
      result.set (sum);
      context.write (key, result);
    }
  }

  public static void main (String [] args) throws Exception {
    Configuration conf = new Configuration ();
    String [] otherArgs = new GenericOptionsParser (conf, args).getRemainingArgs ();
    if (otherArgs.length &lt; 2) {
      System.err.println ("Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;");
      System.exit (2);
    }
    Job job = Job.getInstance (conf, "word count");
    job.setJarByClass (WordCount.class);
    job.setMapperClass (TokenizerMapper.class);
    job.setCombinerClass (IntSumReducer.class);
    job.setReducerClass (IntSumReducer.class);
    job.setOutputKeyClass (Text.class);
    job.setOutputValueClass (IntWritable.class);
    for (int i = 0; i &lt; otherArgs.length - 1; ++i) {
      FileInputFormat.addInputPath (job, new Path (otherArgs [i]));
    }
    FileOutputFormat.setOutputPath (job,
      new Path (otherArgs [otherArgs.length - 1]));
    System.exit (job.waitForCompletion (true) ? 0 : 1);
  }
}

</code></pre>
<h2 id="常用数据序列化类型"><a class="header" href="#常用数据序列化类型">常用数据序列化类型</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102145930.png" alt="" /></p>
<div class="table-wrapper"><table><thead><tr><th>Java 类型</th><th>Hadoop Writable 类型</th></tr></thead><tbody>
<tr><td>Boolean</td><td>BooleanWritable</td></tr>
<tr><td>Byte</td><td>ByteWritable</td></tr>
<tr><td>Int</td><td>IntWritable</td></tr>
<tr><td>Float</td><td>FloatWritable</td></tr>
<tr><td>Long</td><td>LongWritable</td></tr>
<tr><td>Double</td><td>DoubleWritable</td></tr>
<tr><td>String</td><td>Text</td></tr>
<tr><td>Map</td><td>MapWritable</td></tr>
<tr><td>Array</td><td>ArrayWritable</td></tr>
<tr><td>Null</td><td>NullWritable</td></tr>
</tbody></table>
</div>
<h2 id="mapreduce-编程规范"><a class="header" href="#mapreduce-编程规范">MapReduce 编程规范</a></h2>
<p>用户编写的程序分成三个部分：Mapper、Reducer 和 Driver。</p>
<p>1．Mapper 阶段</p>
<p>（1）用户自定义的 Mapper 要继承自己的父类</p>
<pre><code class="language-java">public static class TokenizerMapper 
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{}
</code></pre>
<p>（2）Mapper 的输入数据是 KV 对的形式（KV 的类型可自定义）</p>
<p>p.s. K 是这一行的首字符偏移量，V 是这一行的内容。</p>
<p>（3）Mapper 中的业务逻辑写在 map () 方法中</p>
<pre><code class="language-java">public void map (Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer (value.toString ());
      while (itr.hasMoreTokens ()) {
        word.set (itr.nextToken ());
        context.write (word, one);
      }
    }
</code></pre>
<p>（4）Mapper 的输出数据是 KV 对的形式（KV 的类型可自定义）</p>
<p>（5）map () 方法（MapTask 进程）对每一个 &lt; K,V &gt; 调用一次</p>
<p>2．Reducer 阶段</p>
<p>（1）用户自定义的 Reducer 要继承自己的父类</p>
<pre><code class="language-java">public static class IntSumReducer 
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt;{}
</code></pre>
<p>（2）Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV</p>
<p>（3）Reducer 的业务逻辑写在 reduce () 方法中</p>
<pre><code class="language-java">public void reduce (Text key, Iterable&lt;IntWritable&gt; values, 
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get ();
      }
      result.set (sum);
      context.write (key, result);
    }
</code></pre>
<p>（4）ReduceTask 进程对每一组相同 k 的 &lt; k,v &gt; 组调用一次 reduce () 方法</p>
<p>3．Driver 阶段</p>
<p>相当于 YARN 集群的客户端，用于提交我们整个程序到 YARN 集群，提交的是封装了 MapReduce 程序相关运行参数的 job 对象</p>
<h2 id="wordcount-案例实操"><a class="header" href="#wordcount-案例实操">WordCount 案例实操</a></h2>
<h3 id="本地测试"><a class="header" href="#本地测试">本地测试</a></h3>
<p>1 ） 需求</p>
<p>在给定的文本文件中统计输出每一个单词出现的总次数，输出结果默认按照 utf-8 编码顺序排列</p>
<p>（1）输入数据</p>
<p>创建一个文件并写入想要测试的数据，例如：</p>
<pre><code>Avengers Avengers
DC DC
Mavel Mavel
Iron_Man
Captain_America
Thor
Hulk
Black_Widow
Hawkeye
Black_Panther
Spider_Man
Doctor_Strange
Ant_Man
Vision
Scarlet_Witch
Winter_Soldier
Loki
Star_Lord
Gamora
Rocket_Raccoon
Groot
</code></pre>
<p>（2）期望输出数据（涉及输入的排序问题）</p>
<pre><code>Ant_Man	1
Avengers	2
Black_Panther	1
Black_Widow	1
Captain_America	1
DC	2
Doctor_Strange	1
Gamora	1
Groot	1
Hawkeye	1
Hulk	1
Iron_Man	1
Loki	1
Mavel	2
Rocket_Raccoon	1
Scarlet_Witch	1
Spider_Man	1
Star_Lord	1
Thor	1
Vision	1
Winter_Soldier	1
</code></pre>
<p>2 ） 需求分析</p>
<p>按照 MapReduce 编程规范，分别编写 Mapper、Reducer、Driver 类。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241102150832.png" alt="" /></p>
<p>3 ） 环境准备</p>
<p>（1）创建 maven 工程 MapReduceDemo</p>
<p>（2）在 pom.xml 文件中添加版本信息以及相关依赖</p>
<pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.TianHan&lt;/groupId&gt;
    &lt;artifactId&gt;wordcount&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;properties&gt;
        &lt;maven.compiler.source&gt;21&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;21&lt;/maven.compiler.target&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;3.3.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;
            &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt;
            &lt;version&gt;5.11.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;version&gt;2.0.16&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

&lt;/project&gt;
</code></pre>
<p>（2）在项目的 src/main/resources 目录下，新建一个文件，命名为 “log4j.properties” 用于打印相关日志。在该文件中填入：</p>
<pre><code>log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=% d % p [% c] - % m% n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=% d % p [% c] - % m% n
</code></pre>
<p>（3）创建包 mapreduce.wordcount 然后创建三个类</p>
<p>4 ） 编写程序</p>
<p>（1）编写 Mapper 类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.wordcount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

/**
 * KEYIN, map 阶段输入的 key 的类型：LongWritable
 * VALUEIN,map 阶段输入 value 类型：Text
 * KEYOUT,map 阶段输出的 Key 类型：Text
 * VALUEOUT,map 阶段输出的 value 类型：IntWritable
 */
public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    private Text outK = new Text ();
    private IntWritable outV = new IntWritable (1);  //map 阶段不进行聚合

    @Override
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {

        // 1 获取一行
        //xxxxxx xxxxxx
        String line = value.toString ();

        // 2 切割 (取决于原始数据的中间分隔符)
        //xxxxxxx
        //xxxxxxx
        String [] words = line.split (" ");

        // 3 循环写出
        for (String word : words) {
            // 封装 outk
            outK.set (word);

            // 写出
            context.write (outK, outV);
        }
    }
}
</code></pre>
<p>（2）编写 Reducer 类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.wordcount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * KEYIN, reduce 阶段输入的 key 的类型：Text
 * VALUEIN,reduce 阶段输入 value 类型：IntWritable
 * KEYOUT,reduce 阶段输出的 Key 类型：Text
 * VALUEOUT,reduce 阶段输出的 value 类型：IntWritable
 */
public class WordCountReducer extends Reducer&lt;Text, IntWritable,Text,IntWritable&gt; {
    private IntWritable outV = new IntWritable ();

    @Override
    protected void reduce (Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {

        int sum = 0;
        //xxxxxxx xxxxxxx -&gt;(xxxxxxx,1),(xxxxxxx,1)
        //xxxxxxx, (1,1)
        // 将 values 进行累加
        for (IntWritable value : values) {
            sum += value.get ();
        }

        outV.set (sum);

        // 写出
        context.write (key,outV);
    }
}
</code></pre>
<p>（3）编写 Driver 驱动类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.wordcount;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class WordCountDriver {

    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {

        // 1 获取 job
        Configuration conf = new Configuration ();
        Job job = Job.getInstance (conf);

        // 2 设置 jar 包路径
        job.setJarByClass (WordCountDriver.class);

        // 3 关联 mapper 和 reducer
        job.setMapperClass (WordCountMapper.class);
        job.setReducerClass (WordCountReducer.class);

        // 4 设置 map 输出的 kv 类型
        job.setMapOutputKeyClass (Text.class);
        job.setMapOutputValueClass (IntWritable.class);

        // 5 设置最终输出的 kV 类型
        job.setOutputKeyClass (Text.class);
        job.setOutputValueClass (IntWritable.class);

        // 6 设置输入路径和输出路径
        FileInputFormat.setInputPaths (job, new Path ("E:\\BigData\\hadoop\\input"));
        FileOutputFormat.setOutputPath (job, new Path ("E:\\BigData\\hadoop\\output"));

        // 7 提交 job
        boolean result = job.waitForCompletion (true);

        System.exit (result ? 0 : 1);
    }
}
</code></pre>
<p>5 ） 本地测试</p>
<p>（1）由于这里通过 Maven 安装了 hadoop-client，所以不需要配置 HADOOP_HOME 变量以及 Windows 运行依赖即可成功运行程序。</p>
<p>（2）在 IDEA 上运行程序</p>
<p>注意：此时如果再运行一遍，会报错。在 mapreduce 中，如果输出路径存在会报错。</p>
<h3 id="wordcount-案例-debug-调试"><a class="header" href="#wordcount-案例-debug-调试">WordCount 案例 Debug 调试</a></h3>
<p>在以下几个地方打好断点：Mapper 类中 map 函数第一行、开始 setup、结束 cleanup</p>
<p>通过调试可以更清楚的理解机制。至少三遍。</p>
<h3 id="提交到集群测试"><a class="header" href="#提交到集群测试">提交到集群测试</a></h3>
<p>刚刚上面的代码是在本地运行的，是通过下载了 hadoop 相关的依赖，运用本地模式运行的。</p>
<p>我们还需要把程序推送到生产环境（Linux 环境）中。</p>
<p>（1）用 Maven 打包，需要添加的打包插件依赖</p>
<p>将下面的代码放在之前配置的依赖后面（对应 pom.xml 文件）</p>
<pre><code class="language-xml">&lt;build&gt; 
    &lt;plugins&gt; 
        &lt;plugin&gt; 
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; 
            &lt;version&gt;3.6.1&lt;/version&gt; 
            &lt;configuration&gt; 
                &lt;source&gt;1.8&lt;/source&gt; 
                &lt;target&gt;1.8&lt;/target&gt; 
            &lt;/configuration&gt; 
        &lt;/plugin&gt; 
        &lt;plugin&gt; 
            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; 
            &lt;configuration&gt; 
                &lt;descriptorRefs&gt; 
                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; 
                &lt;/descriptorRefs&gt; 
            &lt;/configuration&gt; 
            &lt;executions&gt; 
                &lt;execution&gt; 
                    &lt;id&gt;make-assembly&lt;/id&gt; 
                    &lt;phase&gt;package&lt;/phase&gt; 
                    &lt;goals&gt; 
                        &lt;goal&gt;single&lt;/goal&gt; 
                    &lt;/goals&gt; 
                &lt;/execution&gt; 
            &lt;/executions&gt; 
        &lt;/plugin&gt; 
    &lt;/plugins&gt; 
&lt;/build&gt; 
</code></pre>
<p><code>maven-compiler-plugin</code>：：打包但不带有所需 jar 包，jar 包小</p>
<p><code>maven-assembly-plugin</code>：打包并带有所需 jar 包，jar 包大</p>
<p>这需要根据需求使用。</p>
<p>（2）将程序打成 jar 包</p>
<p>打包完毕，生成 jar 包，去文件夹里查看一下</p>
<p>（3）修改不带依赖的 jar 包名称为 wc.jar，并拷贝该 jar 包到 Hadoop 集群的 /opt/module/hadoop-3.3.6 路径</p>
<p>思考：刚刚的程序中，我们写的路径是本地 Windows 系统中的路径，上传到 Linux 环境后它其实没有这个路径，输入输出路径不存在，于是我们需要对它进行修改，改成对应的集群路径。</p>
<p>如果想更灵活一点 —— 根据传入的路径来确定输入的路径，需要使用 <code>args [0]</code> 和 <code>args [1]</code></p>
<p>我们再创建一个 wordcount2 包，跟 wordcount 内容一致，就将输入输出路径修改了一下。</p>
<p>对于新改的程序，先点 clean 把前面的删掉，再点 package 进行打包。</p>
<p>将新的包按上面的操作更名 wc.jar</p>
<p>根据命令行设定输入输出路径，使用 <code>args [0]，args [1]</code>。</p>
<pre><code class="language-java">// 6 设置输入路径和输出路径
FileInputFormat.setInputPaths (job, new Path (args [0]));
FileOutputFormat.setOutputPath (job, new Path (args [1]));
</code></pre>
<p>（4）启动 Hadoop 集群</p>
<pre><code>start-dfs.sh
</code></pre>
<p>先在 HDFS 集群中设置刚刚要处理的源文件：</p>
<p>在集群中建一个 Marvel 文件夹，然后在文件夹中上传我们之前要处理的 Marvel.txt 源文件</p>
<p>（5）执行 WordCount 程序</p>
<p>生成 jar 包，导入 jar 包到集群，再重新运行程序</p>
<pre><code>hadoop jar wc.jar com.TianHan.mapreduce.wordcount.WordCountDriver mapreduce/input mapreduce/output
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="序列化"><a class="header" href="#序列化">序列化</a></h1>
<ul>
<li><a href="Hadoop/%E5%BA%8F%E5%88%97%E5%8C%96.html#21-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A6%82%E8%BF%B0">2.1 序列化概述</a></li>
<li><a href="Hadoop/%E5%BA%8F%E5%88%97%E5%8C%96.html#22-%E8%87%AA%E5%AE%9A%E4%B9%89-bean-%E5%AF%B9%E8%B1%A1%E5%AE%9E%E7%8E%B0%E5%BA%8F%E5%88%97%E5%8C%96%E6%8E%A5%E5%8F%A3writable">2.2 自定义 bean 对象实现序列化接口（Writable）</a></li>
<li><a href="Hadoop/%E5%BA%8F%E5%88%97%E5%8C%96.html#23-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">2.3 序列化案例实操</a></li>
</ul>
<h2 id="21-序列化概述"><a class="header" href="#21-序列化概述">2.1 序列化概述</a></h2>
<p>1）<strong>什么是序列化</strong></p>
<p>​ <strong>序列化</strong>就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。</p>
<p>​ <strong>反序列化</strong>就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</p>
<p>2）<strong>为什么要序列化</strong></p>
<p>​ 一般来说，“活的” 对象只生存在内存里，关机断电就没有了。而且 “活的” 对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储 “活的” 对象，可以将 “活的” 对象发送到远程计算机。</p>
<p>3）为什么不用 Java 的序列化</p>
<p>​ Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop 自己开发了一套序列化机制（Writable）。</p>
<p>4）Hadoop 序列化特点：</p>
<p><strong>（1</strong>）<strong>紧凑</strong>：高效使用存储空间。</p>
<p><strong>（2</strong>）快速：读写数据的额外开销小。</p>
<p><strong>（3</strong>）互操作：支持多语言的交互</p>
<h2 id="22-自定义-bean-对象实现序列化接口writable"><a class="header" href="#22-自定义-bean-对象实现序列化接口writable">2.2 自定义 bean 对象实现序列化接口（Writable）</a></h2>
<p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在 Hadoop 框架内部传递一个 bean 对象，那么该对象就需要实现序列化接口。</p>
<p>具体实现 bean 对象序列化步骤如下 7 步。</p>
<p>（1）必须实现 Writable 接口</p>
<p>（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造</p>
<pre><code class="language-java">public FlowBean () {
	super ();
}
</code></pre>
<p>（3）重写序列化方法</p>
<pre><code class="language-java">@Override
public void write (DataOutput out) throws IOException {
	out.writeLong (upFlow);
	out.writeLong (downFlow);
	out.writeLong (sumFlow);
}
</code></pre>
<p>（4）重写反序列化方法</p>
<pre><code class="language-java">@Override
public void readFields (DataInput in) throws IOException {
	upFlow = in.readLong ();
	downFlow = in.readLong ();
	sumFlow = in.readLong ();
}

</code></pre>
<p>（5）<strong>注意</strong>反序列化的顺序和序列化的顺序完全一致</p>
<p>（6）要想把结果显示在文件中，需要重写 toString ()，可用”<code>\t</code>” 分开，方便后续用。</p>
<p>（7）如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口，因为 MapReduce 框中的 Shuffle 过程要求对 key 必须能排序。详见后面排序案例。</p>
<pre><code class="language-java">@Override
public int compareTo (FlowBean o) {
	// 倒序排列，从大到小
	return this.sumFlow &gt; o.getSumFlow () ? -1 : 1;
}
</code></pre>
<h2 id="23-序列化案例实操"><a class="header" href="#23-序列化案例实操">2.3 序列化案例实操</a></h2>
<p><strong>1</strong>）需求</p>
<p>统计每一个手机号耗费的总上行流量、总下行流量、总流量</p>
<p>（1）输入数据</p>
<pre><code>vim phone_data.txt
</code></pre>
<p>相应数据为：</p>
<pre><code>1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
</code></pre>
<p>（2）输入数据格式：</p>
<pre><code>7    13560436666   120.196.100.99      1116       954           200 
id   手机号码      网络 ip             上行流量  下行流量   网络状态码  
</code></pre>
<p>（3）期望输出数据格式</p>
<pre><code>13560436666         1116         954               2070  
手机号码        上行流量     下行流量          总流量  
</code></pre>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img/20241105160111.png" alt="" /></p>
<p><strong>3</strong>）编写 MapReduce 程序</p>
<p>（1）编写流量统计的 Bean 对象</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writable;

import org.apache.hadoop.io.Writable;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

//1 继承 Writable 接口
public class FlowBean implements Writable {

    private long upFlow; // 上行流量
    private long downFlow; // 下行流量
    private long sumFlow; // 总流量

    //2 提供无参构造
    public FlowBean () {
    }

    //3 提供三个参数的 getter 和 setter 方法
    public long getUpFlow () {
        return upFlow;
    }

    public void setUpFlow (long upFlow) {
        this.upFlow = upFlow;
    }

    public long getDownFlow () {
        return downFlow;
    }

    public void setDownFlow (long downFlow) {
        this.downFlow = downFlow;
    }

    public long getSumFlow () {
        return sumFlow;
    }

    public void setSumFlow (long sumFlow) {
        this.sumFlow = sumFlow;
    }

    public void setSumFlow () {
        this.sumFlow = this.upFlow + this.downFlow;
    }

    //4 实现序列化和反序列化方法，注意顺序一定要保持一致
    @Override
    public void write (DataOutput dataOutput) throws IOException {
        dataOutput.writeLong (upFlow);
        dataOutput.writeLong (downFlow);
        dataOutput.writeLong (sumFlow);
    }

    @Override
    public void readFields (DataInput dataInput) throws IOException {
        this.upFlow = dataInput.readLong ();
        this.downFlow = dataInput.readLong ();
        this.sumFlow = dataInput.readLong ();
    }

    //5 重写 ToString
    @Override
    public String toString () {
        return upFlow + "\t" + downFlow + "\t" + sumFlow;
    }
}

</code></pre>
<p>（2）编写 Mapper 类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writable;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

public class FlowMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; {
    private Text outK = new Text ();
    private FlowBean outV = new FlowBean ();

    @Override
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {

        //1 获取一行数据，转成字符串
        String line = value.toString ();

        //2 切割数据
        String [] split = line.split ("\t");

        //3 抓取我们需要的数据：手机号，上行流量，下行流量
        String phone = split [1];
        String up = split [split.length - 3];
        String down = split [split.length - 2];

        //4 封装 outK outV
        outK.set (phone);
        outV.setUpFlow (Long.parseLong (up));
        outV.setDownFlow (Long.parseLong (down));
        outV.setSumFlow ();

        //5 写出 outK outV
        context.write (outK, outV);
    }
}

</code></pre>
<p>（3）编写 Reducer 类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writable;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

public class FlowReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; {
    private FlowBean outV = new FlowBean ();
    @Override
    protected void reduce (Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException {

        long totalUp = 0;
        long totalDown = 0;

        //1 遍历 values, 将其中的上行流量，下行流量分别累加
        for (FlowBean flowBean : values) {
            totalUp += flowBean.getUpFlow ();
            totalDown += flowBean.getDownFlow ();
        }

        //2 封装 outKV
        outV.setUpFlow (totalUp);
        outV.setDownFlow (totalDown);
        outV.setSumFlow ();

        //3 写出 outK outV
        context.write (key,outV);
    }
}
</code></pre>
<p>（4）编写 Driver 驱动类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writable;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

public class FlowDriver {
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {

        //1 获取 job 对象
        Configuration conf = new Configuration ();
        Job job = Job.getInstance (conf);

        //2 关联本 Driver 类
        job.setJarByClass (FlowDriver.class);

        //3 关联 Mapper 和 Reducer
        job.setMapperClass (FlowMapper.class);
        job.setReducerClass (FlowReducer.class);
        
		//4 设置 Map 端输出 KV 类型
        job.setMapOutputKeyClass (Text.class);
        job.setMapOutputValueClass (FlowBean.class);
        
		//5 设置程序最终输出的 KV 类型
        job.setOutputKeyClass (Text.class);
        job.setOutputValueClass (FlowBean.class);
        
		//6 设置程序的输入输出路径
        FileInputFormat.setInputPaths (job, new Path ("D:\\inputflow"));
        FileOutputFormat.setOutputPath (job, new Path ("D:\\flowoutput"));
        
		//7 提交 Job
        boolean b = job.waitForCompletion (true);
        System.exit (b ? 0 : 1);
    }
}
</code></pre>
<p>4）输出结果</p>
<pre><code>13470253144	180	180	360
13509468723	7335	110349	117684
13560439638	918	4938	5856
13568436656	3597	25635	29232
13590439668	1116	954	2070
13630577991	6960	690	7650
13682846555	1938	2910	4848
13729199489	240	0	240
13736230513	2481	24681	27162
13768778790	120	120	240
13846544121	264	0	264
13956435636	132	1512	1644
13966251146	240	0	240
13975057813	11058	48243	59301
13992314666	3008	3720	6728
15043685818	3659	3538	7197
15910133277	3156	2936	6092
15959002129	1938	180	2118
18271575951	1527	2106	3633
18390173782	9531	2412	11943
84188413	4116	1432	5548
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="mapreduce框架原理"><a class="header" href="#mapreduce框架原理">MapReduce框架原理</a></h1>
<ul>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#31-inputformat-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5">3.1 InputFormat 数据输入</a>
<ul>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#311-%E5%88%87%E7%89%87%E4%B8%8E-maptask-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6">3.1.1 切片与 MapTask 并行度决定机制</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#312-job-%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%92%8C%E5%88%87%E7%89%87%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3">3.1.2 Job 提交流程源码和切片源码详解</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#313-fileinputformat-%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6">3.1.3 FileInputFormat 切片机制</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#314-textinputformat">3.1.4 TextInputFormat</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#315-combinetextinputformat-%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6">3.1.5 CombineTextInputFormat 切片机制</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#316-combinetextinputformat-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.1.6 CombineTextInputFormat 案例实操</a></li>
</ul>
</li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#32-mapreduce-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B">3.2 MapReduce 工作流程</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#33-shuffle-%E6%9C%BA%E5%88%B6">3.3 Shuffle 机制</a>
<ul>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#331-shuffle-%E6%9C%BA%E5%88%B6">3.3.1 Shuffle 机制</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#332-partition-%E5%88%86%E5%8C%BA">3.3.2 Partition 分区</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#333-partition-%E5%88%86%E5%8C%BA%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.3.3 Partition 分区案例实操</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#334-writablecomparable-%E6%8E%92%E5%BA%8F">3.3.4 WritableComparable 排序</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#335-writablecomparable-%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E5%85%A8%E6%8E%92%E5%BA%8F">3.3.5 WritableComparable 排序案例实操（全排序）</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#336-writablecomparable-%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E5%8C%BA%E5%86%85%E6%8E%92%E5%BA%8F">3.3.6 WritableComparable 排序案例实操（区内排序）</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#337-combiner-%E5%90%88%E5%B9%B6">3.3.7 Combiner 合并</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#338-combiner-%E5%90%88%E5%B9%B6%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.3.8 Combiner 合并案例实操</a></li>
</ul>
</li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#34-outputformat-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA">3.4 OutputFormat 数据输出</a>
<ul>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#341-outputformat-%E6%8E%A5%E5%8F%A3%E5%AE%9E%E7%8E%B0%E7%B1%BB">3.4.1 OutputFormat 接口实现类</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#342-%E8%87%AA%E5%AE%9A%E4%B9%89-outputformat-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.4.2 自定义 OutputFormat 案例实操</a></li>
</ul>
</li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#35-mapreduce-%E5%86%85%E6%A0%B8%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90">3.5 MapReduce 内核源码解析</a>
<ul>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#351-maptask-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">3.5.1 MapTask 工作机制</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#352-reducetask-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6">3.5.2 ReduceTask 工作机制</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#353-reducetask-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6">3.5.3 ReduceTask 并行度决定机制</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#354-maptask--reducetask-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90">3.5.4 MapTask &amp; ReduceTask 源码解析</a></li>
</ul>
</li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#36-join-%E5%BA%94%E7%94%A8">3.6 Join 应用</a>
<ul>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#361-reduce-join">3.6.1 Reduce Join</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#362-reduce-join-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.6.2 Reduce Join 案例实操</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#363-map-join">3.6.3 Map Join</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#364-map-join-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D">3.6.4 Map Join 案例实操</a></li>
</ul>
</li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C">程序运行结果</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#37-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97etl">3.7 数据清洗（ETL）</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E9%99%84%E5%BD%95etl-%E6%B8%85%E6%B4%97%E8%A7%84%E5%88%99">附录：ETL 清洗规则</a>
<ul>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E9%97%AE%E9%A2%98">解决数据质量问题</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E4%BE%9B%E5%BA%94%E7%AE%97%E6%B3%95%E5%8E%9F%E6%96%99">供应算法原料</a></li>
</ul>
</li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E9%99%84%E5%BD%95%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F">附录常用正则表达式</a>
<ul>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E4%B8%80%E6%A0%A1%E9%AA%8C%E6%95%B0%E5%AD%97%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F">一、校验数字的表达式</a></li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#%E4%BA%8C%E6%A0%A1%E9%AA%8C%E5%AD%97%E7%AC%A6%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F">二、校验字符的表达式</a></li>
</ul>
</li>
<li><a href="Hadoop/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86.html#38-mapreduce-%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93">3.8 MapReduce 开发总结</a></li>
</ul>
<p>​<img src="https://image.3001.net/images/20221106/1667741555906.png" alt="image-20221106213218443" /></p>
<h2 id="31-inputformat-数据输入"><a class="header" href="#31-inputformat-数据输入">3.1 InputFormat 数据输入</a></h2>
<h3 id="311-切片与-maptask-并行度决定机制"><a class="header" href="#311-切片与-maptask-并行度决定机制">3.1.1 切片与 MapTask 并行度决定机制</a></h3>
<p><strong>1</strong>）问题引出</p>
<p>​MapTask 的并行度决定 Map 阶段的任务处理并发度，进而影响到整个 Job 的处理速度。</p>
<p>​思考：1G 的数据，启动 8 个 MapTask，可以提高集群的并发处理能力。那么 1K 的数据，也启动 8 个 MapTask，会提高集群性能吗？MapTask 并行任务是否越多越好呢？哪些因素影响了 MapTask 并行度？</p>
<p><strong>2</strong>）MapTask 并行度决定机制</p>
<p>​ <strong>数据块：</strong> Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。</p>
<p>​ <strong>数据切片：</strong> 数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</p>
<p><img src="https://image.3001.net/images/20221106/16677415962785.png" alt="image-20221106213259161" /></p>
<h3 id="312-job-提交流程源码和切片源码详解"><a class="header" href="#312-job-提交流程源码和切片源码详解">3.1.2 Job 提交流程源码和切片源码详解</a></h3>
<p>源码阅读三大要点：job.xml、xxx.jar、job.split</p>
<p>1）Job 提交流程源码详解</p>
<pre><code class="language-java">waitForCompletion ()

submit ();

// 1 建立连接
connect ();	
	// 1）创建提交 Job 的代理
	new Cluster (getConfiguration ());
		// （1）判断是本地运行环境还是 yarn 集群运行环境
		initialize (jobTrackAddr, conf); 

	// 2 提交 job
	submitter.submitJobInternal (Job.this, cluster)

// 1）创建给集群提交数据的 Stag 路径
Path jobStagingArea = JobSubmissionFiles.getStagingDir (cluster, conf);

// 2）获取 jobid ，并创建 Job 路径
JobID jobId = submitClient.getNewJobID ();

// 3）拷贝 jar 包到集群
copyAndConfigureFiles (job, submitJobDir);	
rUploader.uploadFiles (job, jobSubmitDir);

// 4）计算切片，生成切片规划文件
writeSplits (job, submitJobDir);
	maps = writeNewSplits (job, jobSubmitDir);
	input.getSplits (job);

// 5）向 Stag 路径写 XML 配置文件
writeConf (conf, submitJobFile);
conf.writeXml (out);

// 6）提交 Job, 返回提交状态
status = submitClient.submitJob (jobId, submitJobDir.toString (), job.getCredentials ());

</code></pre>
<p><img src="https://image.3001.net/images/20221106/16677416364405.png" alt="image-20221106213339571" /></p>
<p><strong>2</strong>）FileInputFormat 切片源码解析（input.getSplits (job)）</p>
<p><img src="https://image.3001.net/images/20221106/16677416493309.png" alt="image-20221106213352169" /></p>
<h3 id="313-fileinputformat-切片机制"><a class="header" href="#313-fileinputformat-切片机制">3.1.3 FileInputFormat 切片机制</a></h3>
<p><img src="https://image.3001.net/images/20221106/16677417044403.png" alt="image-20221106213447314" /></p>
<p><img src="https://image.3001.net/images/20221106/16677417312383.png" alt="image-20221106213514220" /></p>
<h3 id="314-textinputformat"><a class="header" href="#314-textinputformat">3.1.4 TextInputFormat</a></h3>
<p><strong>1</strong>）FileInputFormat 实现类</p>
<p>​思考：在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。那么，针对不同的数据类型，MapReduce 是如何读取这些数据的呢？</p>
<p>​FileInputFormat 常见的接口实现类包括：<strong>TextInputFormat</strong>、KeyValueTextInputFormat、NLineInputFormat、<strong>CombineTextInputFormat</strong>和自定义 InputFormat 等。</p>
<p><strong>2</strong>）TextInputFormat</p>
<p>​TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable 类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text 类型。</p>
<p>​以下是一个示例，比如，一个分片包含了如下 4 条文本记录。</p>
<pre><code>Rich learning form
Intelligent learning engine
Learning more convenient
From the real demand for more close to the enterprise
</code></pre>
<p>每条记录表示为以下键 / 值对：</p>
<pre><code>(0,Rich learning form)
(20,Intelligent learning engine)
(49,Learning more convenient)
(74,From the real demand for more close to the enterprise)
</code></pre>
<h3 id="315-combinetextinputformat-切片机制"><a class="header" href="#315-combinetextinputformat-切片机制">3.1.5 CombineTextInputFormat 切片机制</a></h3>
<p>框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask，这样如果有大量小文件，就会产生大量的 MapTask，处理效率极其低下。</p>
<p><strong>1</strong>）应用场景：</p>
<p>​<code>CombineTextInputFormat</code> 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。</p>
<p><strong>2</strong>）虚拟存储切片最大值设置</p>
<p><code>​CombineTextInputFormat.setMaxInputSplitSize (job, 4194304);// 4m</code></p>
<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
<p><strong>3</strong>）切片机制</p>
<p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>
<p><img src="https://image.3001.net/images/20221110/1668056403686.png" alt="image-20221110125959434" /></p>
<p>（1）虚拟存储过程：</p>
<p>​ 将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；<strong>当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片）。</strong></p>
<p>​ 例如 setMaxInputSplitSize 值为 4M，输入文件大小为 8.02M，则先逻辑上分成一个 4M。剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储文件，所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件。</p>
<p>（2）切片过程：</p>
<p>（a）判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独形成一个切片。</p>
<p>（b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</p>
<p>（c）测试举例：有 4 个小文件大小分别为 1.7M、5.1M、3.4M 以及 6.8M 这四个小文件，则虚拟存储之后形成 6 个文件块，大小分别为：</p>
<p>1.7M，（2.55M、2.55M），3.4M 以及（3.4M、3.4M）</p>
<p>最终会形成 3 个切片，大小分别为：</p>
<p>（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</p>
<h3 id="316-combinetextinputformat-案例实操"><a class="header" href="#316-combinetextinputformat-案例实操">3.1.6 CombineTextInputFormat 案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>将输入的大量小文件合并成一个切片统一处理。</p>
<p>（1）输入数据</p>
<p>准备 4 个小文件</p>
<p>（2）期望</p>
<p>期望一个切片处理 4 个文件</p>
<p><strong>2</strong>）实现过程</p>
<p>（1）不做任何处理，运行 1.8 节的 WordCount 案例程序，观察切片个数为 4。</p>
<pre><code>number of splits:4
</code></pre>
<p>（2）在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 3。</p>
<p>（a）驱动类中添加代码如下：</p>
<pre><code class="language-java">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.class
job.setInputFormatClass (CombineTextInputFormat.class);

// 虚拟存储切片最大值设置 4m
CombineTextInputFormat.setMaxInputSplitSize (job, 4194304);
</code></pre>
<p>（b）运行如果为 3 个切片。</p>
<pre><code>number of splits:3
</code></pre>
<p>（3）在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 1。</p>
<p>（a）驱动中添加代码如下：</p>
<pre><code class="language-java">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.class
job.setInputFormatClass (CombineTextInputFormat.class);

// 虚拟存储切片最大值设置 20m
CombineTextInputFormat.setMaxInputSplitSize (job, 20971520);
</code></pre>
<p>（b）运行如果为 1 个切片</p>
<pre><code>number of splits:1
</code></pre>
<h2 id="32-mapreduce-工作流程"><a class="header" href="#32-mapreduce-工作流程">3.2 MapReduce 工作流程</a></h2>
<p><img src="https://image.3001.net/images/20221110/16680566693739.png" alt="image-20221110130426443" /></p>
<p><img src="https://image.3001.net/images/20221110/16680566799674.png" alt="image-20221110130436072" /></p>
<p>​上面的流程是整个 MapReduce 最全工作流程，但是 Shuffle 过程只是从第 7 步开始到第 16 步结束，具体 Shuffle 过程详解，如下：</p>
<p>（1）MapTask 收集我们的 map () 方法输出的 kv 对，放到内存缓冲区中</p>
<p>（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p>
<p>（3）多个溢出文件会被合并成大的溢出文件</p>
<p>（4）在溢出过程及合并的过程中，都要调用 Partitioner 进行分区和针对 key 进行排序</p>
<p>（5）ReduceTask 根据自己的分区号，去各个 MapTask 机器上取相应的结果分区数据</p>
<p>（6）ReduceTask 会抓取到同一个分区的来自不同 MapTask 的结果文件，ReduceTask 会将这些文件再进行合并（归并排序）</p>
<p>（7）合并成大文件后，Shuffle 的过程也就结束了，后面进入 ReduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 Group，调用用户自定义的 reduce () 方法）</p>
<p><strong>注意：</strong></p>
<p>（1）Shuffle 中的缓冲区大小会影响到 MapReduce 程序的执行效率，原则上说，缓冲区越大，磁盘 IO 的次数越少，执行速度就越快。</p>
<p>（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认 100M。</p>
<h2 id="33-shuffle-机制"><a class="header" href="#33-shuffle-机制">3.3 Shuffle 机制</a></h2>
<h3 id="331-shuffle-机制"><a class="header" href="#331-shuffle-机制">3.3.1 Shuffle 机制</a></h3>
<p>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。</p>
<p><img src="https://image.3001.net/images/20221110/1668056711326.png" alt="image-20221110130507683" /></p>
<p>对 key 的索引按照字典顺序快速排序</p>
<p>分组格式：{key，（value1， value2， ……）}</p>
<h3 id="332-partition-分区"><a class="header" href="#332-partition-分区">3.3.2 Partition 分区</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680567231755.png" alt="image-20221110130520540" /></p>
<p>在 Driver 中添加 <code>job.setNumReduceTasks (2);</code>，然后 DeBug 逐步查找下面的代码：</p>
<pre><code class="language-java">public void write (K key, V value) throws IOException, InterruptedException {  
    this.collector.collect (key, value, this.partitioner.getPartition (key, value, this.partitions));  
}
</code></pre>
<pre><code class="language-java">//  
// Source code recreated from a .class file by IntelliJ IDEA  
// (powered by FernFlower decompiler)  
//  
  
package org.apache.hadoop.mapreduce.lib.partition;  
  
import org.apache.hadoop.classification.InterfaceAudience.Public;  
import org.apache.hadoop.classification.InterfaceStability.Stable;  
import org.apache.hadoop.mapreduce.Partitioner;  
  
@Public  
@Stable  
public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; {  
    public HashPartitioner () {  
    }  
  
    public int getPartition (K key, V value, int numReduceTasks) {  
        return (key.hashCode () &amp; Integer.MAX_VALUE) % numReduceTasks;  
    }  
}
</code></pre>
<p>在 Driver 中不设置 <code>job.setNumReduceTasks (2);</code>，然后 DeBug 逐步查找下面的代码：</p>
<pre><code class="language-java">NewOutputCollector (JobContext jobContext, JobConf job, TaskUmbilicalProtocol umbilical, Task.TaskReporter reporter) throws IOException, ClassNotFoundException {  
    this.collector = MapTask.this.createSortingCollector (job, reporter);  
    this.partitions = jobContext.getNumReduceTasks ();  
    if (this.partitions &gt; 1) {  
        this.partitioner = (Partitioner) ReflectionUtils.newInstance (jobContext.getPartitionerClass (), job);  
    } else {  
        this.partitioner = new Partitioner&lt;K, V&gt;() {  
            public int getPartition (K key, V value, int numPartitions) {  
                return NewOutputCollector.this.partitions - 1;  
            }  
        };  
    }  
  
}  
  
public void write (K key, V value) throws IOException, InterruptedException {  
    this.collector.collect (key, value, this.partitioner.getPartition (key, value, this.partitions));  
}
</code></pre>
<p><code>new Partitioner&lt;K, V&gt;(){}</code> 匿名内部类，<code>job.setNumReduceTasks</code> 默认是 1，只生成一个 0 号分区</p>
<p><img src="https://image.3001.net/images/20221110/16680567322654.png" alt="image-20221110130528974" /></p>
<p><img src="https://image.3001.net/images/20221110/16680567452392.png" alt="image-20221110130542610" /></p>
<h3 id="333-partition-分区案例实操"><a class="header" href="#333-partition-分区案例实操">3.3.3 Partition 分区案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p>
<p>（1）输入数据</p>
<p>​<code>phone_data.txt</code></p>
<pre><code>1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
</code></pre>
<p>（2）期望输出数据</p>
<p>​ 手机号 136、137、138、139 开头都分别放到一个独立的 4 个文件中，其他开头的放到一个文件中。</p>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://image.3001.net/images/20221110/1668056811486.png" alt="image-20221110130647783" /></p>
<p><strong>3</strong>）在案例<strong>2.3</strong>的基础上，增加一个分区类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.partitioner;  
  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Partitioner;  
  
public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; {  
  
    @Override  
    public int getPartition (Text text, FlowBean flowBean, int numPartitions) {  
        // 获取手机号前三位 prePhone  
        String phone = text.toString ();  
        String prePhone = phone.substring (0, 3);  
  
        // 定义一个分区号变量 partition, 根据 prePhone 设置分区号  
        int partition;  
  
        if ("136".equals (prePhone)){  
            partition = 0;  
        } else if ("137".equals (prePhone)){  
            partition = 1;  
        } else if ("138".equals (prePhone)){  
            partition = 2;  
        } else if ("139".equals (prePhone)){  
            partition = 3;  
        } else {  
            partition = 4;  
        }  
  
        // 最后返回分区号 partition  
        return partition;  
    }  
}
</code></pre>
<p>改进版写法：</p>
<pre><code class="language-java">package com.TianHan.mapreduce.partitioner2;  
  
import com.TianHan.mapreduce.partitioner2.FlowBean;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Partitioner;  
  
public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; {  
  
    @Override  
    public int getPartition (Text text, FlowBean flowBean, int numPartitions) {  
        // 获取手机号前三位 prePhone  
        String phone = text.toString ();  
        String prePhone = phone.substring (0, 3);  
  
        // 定义一个分区号变量 partition, 根据 prePhone 设置分区号  
  
        // 最后返回分区号 partition  
        return switch (prePhone) {  
            case "136" -&gt; 0;  
            case "137" -&gt; 1;  
            case "138" -&gt; 2;  
            case "139" -&gt; 3;  
            default -&gt; 4;  
        };  
    }  
}
</code></pre>
<p><strong>4</strong>）在驱动函数中增加自定义数据分区设置和 ReduceTask 设置</p>
<pre><code class="language-java">package com.TianHan.mapreduce.partitioner2;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import java.io.IOException;  
  
public class FlowDriver {  
  
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {  
  
        //1 获取 job 对象    
		Configuration conf = new Configuration ();  
        Job job = Job.getInstance (conf);  
  
        //2 关联本 Driver 类    
		job.setJarByClass (FlowDriver.class);  
  
        //3 关联 Mapper 和 Reducer    
		job.setMapperClass (FlowMapper.class);  
        job.setReducerClass (FlowReducer.class);  
  
        //4 设置 Map 端输出数据的 KV 类型    
		job.setMapOutputKeyClass (Text.class);  
        job.setMapOutputValueClass (FlowBean.class);  
  
        //5 设置程序最终输出的 KV 类型    
		job.setOutputKeyClass (Text.class);  
        job.setOutputValueClass (FlowBean.class);  
  
        //8 指定自定义分区器    
		job.setPartitionerClass (ProvincePartitioner.class);  
  
        //9 同时指定相应数量的 ReduceTask    
		job.setNumReduceTasks (5);  
  
        //6 设置输入输出路径    
		FileInputFormat.setInputPaths (job, new Path ("E:\\BigData\\hadoop\\input"));  
        FileOutputFormat.setOutputPath (job, new Path ("E:\\BigData\\hadoop\\output2"));  
  
        //7 提交 Job    
		boolean b = job.waitForCompletion (true);  
        System.exit (b ? 0 : 1);  
    }  
}
</code></pre>
<p>指定自定义分区器 <code>job.setPartitionerClass (ProvincePartitioner.class);</code></p>
<p>同时指定相应数量的 ReduceTask<code>job.setNumReduceTasks (5);</code></p>
<p>自定义分区器和 ReduceTask 数量要相等，否则可能报错 <code>Illegal partition ..</code>。</p>
<p>设置为 1 即 <code>job.setNumReduceTasks (1);</code> 走默认 partioner 方法，不会报错，但是无法达到想要的效果。</p>
<pre><code class="language-java">this.partitioner = new Partitioner&lt;K, V&gt;() {  
    public int getPartition (K key, V value, int numPartitions) {  
        return NewOutputCollector.this.partitions - 1;  
    }  
};
</code></pre>
<p>设置为 2 或 3 或 4 可能报错 <code>Illegal partition ..</code>。</p>
<p>设置为 5 刚刚好。</p>
<p>设置为 6 等大于 5 的数时不报错，但是多余文件中内容为空。</p>
<h3 id="334-writablecomparable-排序"><a class="header" href="#334-writablecomparable-排序">3.3.4 WritableComparable 排序</a></h3>
<p><img src="https://image.3001.net/images/20221110/1668056910753.png" alt="image-20221110130827331" /></p>
<p><img src="https://image.3001.net/images/20221110/1668056922989.png" alt="image-20221110130839547" /></p>
<p><img src="https://image.3001.net/images/20221110/16680569339822.png" alt="image-20221110130850481" /></p>
<p>自定义排序<strong>WritableComparable</strong>原理分析</p>
<p>​bean 对象做为 key 传输，需要实现 WritableComparable 接口重写 compareTo 方法，就可以实现排序。</p>
<pre><code class="language-java">@Override  
public int compareTo (FlowBean bean) {  
  
	int result;  
		  
	// 按照总流量大小，倒序排列  
	if (this.sumFlow &gt; bean.getSumFlow ()) {  
		result = -1;  
	} else if (this.sumFlow &lt; bean.getSumFlow ()) {  
		result = 1;  
	} else {  
		result = 0;  
	}  
  
	return result;  
}
</code></pre>
<h3 id="335-writablecomparable-排序案例实操全排序"><a class="header" href="#335-writablecomparable-排序案例实操全排序">3.3.5 WritableComparable 排序案例实操（全排序）</a></h3>
<p><strong>1</strong>）需求</p>
<p>根据案例 2.3 序列化案例产生的结果再次对总流量进行倒序排序。</p>
<p>（1）输入数据</p>
<p>​<code>phone_data.txt</code></p>
<pre><code>1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
</code></pre>
<p>第一次处理后的数据中只保留下面这个文件：</p>
<pre><code>part-r-00000
</code></pre>
<p>（2）期望输出数据</p>
<pre><code>13509468723 7335 110349 117684
13736230513 2481 24681 27162
13956435636 132 1512 1644
13846544121 264 0 264
。。。 。。。
</code></pre>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://image.3001.net/images/20221110/1668057054897.png" alt="image-20221110131050892" /></p>
<p><strong>3</strong>）代码实现</p>
<p>（1）FlowBean 对象在在需求 1 基础上增加了比较功能</p>
<pre><code class="language-java">import org.apache.hadoop.io.WritableComparable;  
import java.io.DataInput;  
import java.io.DataOutput;  
import java.io.IOException;  
  
public class FlowBean implements WritableComparable&lt;FlowBean&gt; {  
  
    private long upFlow; // 上行流量  
    private long downFlow; // 下行流量  
    private long sumFlow; // 总流量  
  
    // 提供无参构造  
    public FlowBean () {  
    }  
  
    // 生成三个属性的 getter 和 setter 方法  
    public long getUpFlow () {  
        return upFlow;  
    }  
  
    public void setUpFlow (long upFlow) {  
        this.upFlow = upFlow;  
    }  
  
    public long getDownFlow () {  
        return downFlow;  
    }  
  
    public void setDownFlow (long downFlow) {  
        this.downFlow = downFlow;  
    }  
  
    public long getSumFlow () {  
        return sumFlow;  
    }  
  
    public void setSumFlow (long sumFlow) {  
        this.sumFlow = sumFlow;  
    }  
  
    public void setSumFlow () {  
        this.sumFlow = this.upFlow + this.downFlow;  
    }  
  
    // 实现序列化和反序列化方法，注意顺序一定要一致  
    @Override  
    public void write (DataOutput out) throws IOException {  
        out.writeLong (this.upFlow);  
        out.writeLong (this.downFlow);  
        out.writeLong (this.sumFlow);  
  
    }  
  
    @Override  
    public void readFields (DataInput in) throws IOException {  
        this.upFlow = in.readLong ();  
        this.downFlow = in.readLong ();  
        this.sumFlow = in.readLong ();  
    }  
  
    // 重写 ToString, 最后要输出 FlowBean  
    @Override  
    public String toString () {  
        return upFlow + "\t" + downFlow + "\t" + sumFlow;  
    }  
  
    @Override  
    public int compareTo (FlowBean o) {  
  
        // 按照总流量比较，倒序排列  
        if (this.sumFlow &gt; o.sumFlow){  
            return -1;  
        } else if (this.sumFlow &lt; o.sumFlow){  
            return 1;  
        } else {  
            return 0;  
        }  
    }  
}
</code></pre>
<p>（2）编写 Mapper 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
import java.io.IOException;  
  
public class FlowMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; {  
    private FlowBean outK = new FlowBean ();  
    private Text outV = new Text ();  
  
    @Override  
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
  
        //1 获取一行数据  
        String line = value.toString ();  
  
        //2 按照 "\t", 切割数据  
        String [] split = line.split ("\t");  
  
        //3 封装 outK outV  
        outK.setUpFlow (Long.parseLong (split [1]));  
        outK.setDownFlow (Long.parseLong (split [2]));  
        outK.setSumFlow ();  
        outV.set (split [0]);  
  
        //4 写出 outK outV  
        context.write (outK,outV);  
    }  
}
</code></pre>
<p>（3）编写 Reducer 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
import java.io.IOException;  
  
public class FlowReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; {  
    @Override  
    protected void reduce (FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {  
  
        // 遍历 values 集合，循环写出，避免总流量相同的情况  
        for (Text value : values) {  
            // 调换 KV 位置，反向写出  
            context.write (value,key);  
        }  
    }  
}
</code></pre>
<p>总流量相同（key 相同）的键值对同时进入 reduce 方法。</p>
<p>（4）编写 Driver 类</p>
<pre><code class="language-java">import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import java.io.IOException;  
  
public class FlowDriver {  
  
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {  
  
        //1 获取 job 对象  
        Configuration conf = new Configuration ();  
        Job job = Job.getInstance (conf);  
  
        //2 关联本 Driver 类  
        job.setJarByClass (FlowDriver.class);  
  
        //3 关联 Mapper 和 Reducer  
        job.setMapperClass (FlowMapper.class);  
        job.setReducerClass (FlowReducer.class);  
  
        //4 设置 Map 端输出数据的 KV 类型  
        job.setMapOutputKeyClass (FlowBean.class);  
        job.setMapOutputValueClass (Text.class);  
  
        //5 设置程序最终输出的 KV 类型  
        job.setOutputKeyClass (Text.class);  
        job.setOutputValueClass (FlowBean.class);  
  
        //6 设置输入输出路径  
        FileInputFormat.setInputPaths (job, new Path ("D:\\inputflow2"));  
        FileOutputFormat.setOutputPath (job, new Path ("D:\\comparout"));  
  
        //7 提交 Job  
        boolean b = job.waitForCompletion (true);  
        System.exit (b ? 0 : 1);  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>13509468723	7335	110349	117684
13975057813	11058	48243	59301
13568436656	3597	25635	29232
13736230513	2481	24681	27162
18390173782	9531	2412	11943
13630577991	6960	690	7650
15043685818	3659	3538	7197
13992314666	3008	3720	6728
15910133277	3156	2936	6092
13560439638	918	4938	5856
84188413	4116	1432	5548
13682846555	1938	2910	4848
18271575951	1527	2106	3633
15959002129	1938	180	2118
13590439668	1116	954	2070
13956435636	132	1512	1644
13470253144	180	180	360
13846544121	264	0	264
13729199489	240	0	240
13768778790	120	120	240
13966251146	240	0	240
</code></pre>
<p>二次排序案例（总流量相同则按照上行流量升序排列）</p>
<p>修改 FlowBean.java 文件中的比较方式：</p>
<pre><code class="language-java">@Override  
public int compareTo (FlowBean o) {  
    if (this.sumFlow &gt; o.sumFlow) {  
        return -1;  
    } else if (this.sumFlow &lt; o.sumFlow) {  
        return 1;  
    } else {  
        if (this.upFlow &gt; o.upFlow){  
            return 1;  
        } else if (this.upFlow &lt; o.upFlow){  
            return -1;  
        } else {  
            return 0;  
        }  
    }  
}
</code></pre>
<p>简单写法：</p>
<pre><code class="language-java">@Override  
public int compareTo (FlowBean o) {  
    if (this.sumFlow &gt; o.sumFlow) {  
        return -1;  
    } else if (this.sumFlow &lt; o.sumFlow) {  
        return 1;  
    } else {  
        return this.upFlow.compareTo (o.upFlow);  
    }  
}
</code></pre>
<p>输出效果如下所示：</p>
<pre><code>13509468723	7335	110349	117684
13975057813	11058	48243	59301
13568436656	3597	25635	29232
13736230513	2481	24681	27162
18390173782	9531	2412	11943
13630577991	6960	690	7650
15043685818	3659	3538	7197
13992314666	3008	3720	6728
15910133277	3156	2936	6092
13560439638	918	4938	5856
84188413	4116	1432	5548
13682846555	1938	2910	4848
18271575951	1527	2106	3633
15959002129	1938	180	2118
13590439668	1116	954	2070
13956435636	132	1512	1644
13470253144	180	180	360
13846544121	264	0	264
13768778790	120	120	240
13729199489	240	0	240
13966251146	240	0	240
</code></pre>
<h3 id="336-writablecomparable-排序案例实操区内排序"><a class="header" href="#336-writablecomparable-排序案例实操区内排序">3.3.6 WritableComparable 排序案例实操（区内排序）</a></h3>
<p><strong>1</strong>）需求</p>
<p>要求每个省份手机号输出的文件中按照总流量内部排序。</p>
<p><strong>2</strong>）需求分析</p>
<p>​ 基于前一个需求，增加自定义分区类，分区按照省份手机号设置。</p>
<p><img src="https://image.3001.net/images/20221110/16680571604100.png" alt="image-20221110131236743" /></p>
<p><strong>3</strong>）案例实操</p>
<p>（1）增加自定义分区类</p>
<pre><code class="language-java">import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Partitioner;  
  
public class ProvincePartitioner extends Partitioner&lt;FlowBean, Text&gt; {  
  
    @Override  
    public int getPartition (FlowBean flowBean, Text text, int numPartitions) {  
        // 获取手机号前三位  
        String phone = text.toString ();  
        String prePhone = phone.substring (0, 3);  
  
        // 定义一个分区号变量 partition, 根据 prePhone 设置分区号  
        int partition;  
        if ("136".equals (prePhone)){  
            partition = 0;  
        } else if ("137".equals (prePhone)){  
            partition = 1;  
        } else if ("138".equals (prePhone)){  
            partition = 2;  
        } else if ("139".equals (prePhone)){  
            partition = 3;  
        } else {  
            partition = 4;  
        }  
  
        // 最后返回分区号 partition  
        return partition;  
    }  
}
</code></pre>
<p>（2）在驱动类中添加分区类</p>
<pre><code class="language-java">package com.TianHan.mapreduce.writableComparablePartitioner;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class FlowDriver {
    public static void main (String [] args) throws IOException, InterruptedException, ClassNotFoundException {
        Configuration conf = new Configuration ();
        Job job = Job.getInstance (conf);

        job.setJarByClass (FlowDriver.class);

        job.setMapperClass (FlowMapper.class);
        job.setReducerClass (FlowReducer.class);

        job.setMapOutputKeyClass (FlowBean.class);
        job.setMapOutputValueClass (Text.class);

        job.setOutputKeyClass (Text.class);
        job.setOutputValueClass (FlowBean.class);

        // 设置自定义分区器
        job.setPartitionerClass (ProvincePartitioner.class);
        // 设置对应的 ReduceTask 的个数
        job.setNumReduceTasks (5);

        FileInputFormat.setInputPaths (job, new Path ("E:\\BigData\\hadoop\\output"));
        FileOutputFormat.setOutputPath (job, new Path ("E:\\BigData\\hadoop\\output5"));

        boolean res = job.waitForCompletion (true);
        System.exit (res ? 0 : 1);
    }
}

</code></pre>
<h3 id="337-combiner-合并"><a class="header" href="#337-combiner-合并">3.3.7 Combiner 合并</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680572024854.png" alt="image-20221110131319588" /></p>
<p>（6）自定义 Combiner 实现步骤</p>
<p>（a）自定义一个 Combiner 继承 Reducer，重写 Reduce 方法</p>
<pre><code class="language-java">public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {  
  
    private IntWritable outV = new IntWritable ();  
  
    @Override  
    protected void reduce (Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {  
  
        int sum = 0;  
        for (IntWritable value : values) {  
            sum += value.get ();  
        }  
       
        outV.set (sum);  
       
        context.write (key,outV);  
    }  
}
</code></pre>
<p>（b）在 Job 驱动类中设置：</p>
<pre><code class="language-java">job.setCombinerClass (WordCountCombiner.class);
</code></pre>
<h3 id="338-combiner-合并案例实操"><a class="header" href="#338-combiner-合并案例实操">3.3.8 Combiner 合并案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>​ 统计过程中对每一个 MapTask 的输出进行局部汇总，以减小网络传输量即采用 Combiner 功能。</p>
<p>（1）数据输入</p>
<p><code>hello.txt</code></p>
<pre><code>banzhang ni hao  
xihuan hadoop banzhang  
banzhang ni hao  
xihuan hadoop banzhang
</code></pre>
<p>（2）期望输出数据</p>
<p>期望：Combine 输入数据多，输出时经过合并，输出数据减少。</p>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://image.3001.net/images/20221110/16680572608397.png" alt="image-20221110131417534" /></p>
<p><strong>3</strong>）案例实操 - 方案一</p>
<p>（1）增加一个 WordCountCombiner 类继承 Reducer</p>
<pre><code class="language-java">import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
import java.io.IOException;  
  
public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {  
  
private IntWritable outV = new IntWritable ();  
  
    @Override  
    protected void reduce (Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {  
  
        int sum = 0;  
        for (IntWritable value : values) {  
            sum += value.get ();  
        }  
  
        // 封装 outKV  
        outV.set (sum);  
  
        // 写出 outKV  
        context.write (key,outV);  
    }  
}
</code></pre>
<p>（2）在 WordcountDriver 驱动类中指定 Combiner</p>
<pre><code class="language-java">// 指定需要使用 combiner，以及用哪个类作为 combiner 的逻辑  
job.setCombinerClass (WordCountCombiner.class);
</code></pre>
<p><strong>4</strong>）案例实操 - 方案二（推荐）</p>
<p>（1）由于 Reducer 类中已经实现了相同的方法，下面将在 WordcountDriver 驱动类中指定 WordcountReducer 作为 Combiner</p>
<pre><code class="language-java">// 指定需要使用 Combiner，以及用哪个类作为 Combiner 的逻辑  
job.setCombinerClass (WordCountReducer.class);
</code></pre>
<p>运行程序，如下图所示</p>
<p><img src="https://image.3001.net/images/20221110/1668057397598.png" alt="image-20221110131634660" /></p>
<p>注意：没有 Reduce 阶段就没有 Shuffle 阶段。</p>
<h2 id="34-outputformat-数据输出"><a class="header" href="#34-outputformat-数据输出">3.4 OutputFormat 数据输出</a></h2>
<h3 id="341-outputformat-接口实现类"><a class="header" href="#341-outputformat-接口实现类">3.4.1 OutputFormat 接口实现类</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680574261360.png" alt="image-20221110131703273" /></p>
<h3 id="342-自定义-outputformat-案例实操"><a class="header" href="#342-自定义-outputformat-案例实操">3.4.2 自定义 OutputFormat 案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>过滤输入的 <code>log</code> 日志，包含 <code>atguigu</code> 的网站输出到 <code>e:/atguigu.log</code>，不包含 <code>atguigu</code> 的网站输出到 <code>e:/other.log</code>。</p>
<p>（1）输入数据</p>
<p><code>log.txt</code></p>
<pre><code>http://www.baidu.com  
http://www.google.com  
http://cn.bing.com  
http://www.atguigu.com  
http://www.sohu.com  
http://www.sina.com  
http://www.sin2a.com  
http://www.sin2desa.com  
http://www.sindsafa.com
</code></pre>
<p>（2）期望输出数据</p>
<p>​ atguigu.log</p>
<pre><code>http://www.atguigu.com
</code></pre>
<p>other.log</p>
<pre><code>http://cn.bing.com  
http://www.baidu.com  
http://www.google.com  
http://www.sin2a.com  
http://www.sin2desa.com  
http://www.sina.com  
http://www.sindsafa.com  
http://www.sohu.com
</code></pre>
<p><strong>2</strong>）需求分析</p>
<p><img src="https://image.3001.net/images/20221110/16680577096922.png" alt="image-20221110132146534" /></p>
<p><strong>3</strong>）案例实操</p>
<p>（1）编写 LogMapper 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
  
import java.io.IOException;  
  
public class LogMapper extends Mapper&lt;LongWritable, Text,Text, NullWritable&gt; {  
    @Override  
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
        // 不做任何处理，直接写出一行 log 数据  
        context.write (value,NullWritable.get ());  
    }  
}
</code></pre>
<p>（2）编写 LogReducer 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
  
import java.io.IOException;  
  
public class LogReducer extends Reducer&lt;Text, NullWritable,Text, NullWritable&gt; {  
    @Override  
    protected void reduce (Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {  
        // 防止有相同的数据，迭代写出  
        for (NullWritable value : values) {  
            context.write (key,NullWritable.get ());  
        }  
    }  
}
</code></pre>
<p>（3）自定义一个 LogOutputFormat 类</p>
<pre><code class="language-java">import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.RecordWriter;  
import org.apache.hadoop.mapreduce.TaskAttemptContext;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
import java.io.IOException;  
  
public class LogOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; {  
    @Override  
    public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter (TaskAttemptContext job) throws IOException, InterruptedException {  
        // 创建一个自定义的 RecordWriter 返回  
        LogRecordWriter logRecordWriter = new LogRecordWriter (job);  
        return logRecordWriter;  
    }  
}
</code></pre>
<p>（4）编写 LogRecordWriter 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.outputformat;  
  
import org.apache.hadoop.fs.FSDataOutputStream;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.IOUtils;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.RecordWriter;  
import org.apache.hadoop.mapreduce.TaskAttemptContext;  
  
import java.io.IOException;  
  
public class LogRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; {  
  
    private FSDataOutputStream atguiguOut;  
    private FSDataOutputStream otherOut;  
  
    public LogRecordWriter (TaskAttemptContext job) {  
        try {  
            // 获取文件系统对象  
            FileSystem fs = FileSystem.get (job.getConfiguration ());  
            // 用文件系统对象创建两个输出流对应不同的目录  
            atguiguOut = fs.create (new Path ("d:/hadoop/atguigu.log"));  
            otherOut = fs.create (new Path ("d:/hadoop/other.log"));  
        } catch (IOException e) {  
            e.printStackTrace ();  
        }  
    }  
  
    @Override  
    public void write (Text key, NullWritable value) throws IOException, InterruptedException {  
        String log = key.toString ();  
        // 根据一行的 log 数据是否包含 atguigu, 判断两条输出流输出的内容  
        if (log.contains ("atguigu")) {  
            atguiguOut.writeBytes (log + "\n");  
        } else {  
            otherOut.writeBytes (log + "\n");  
        }  
    }  
  
    @Override  
    public void close (TaskAttemptContext context) throws IOException, InterruptedException {  
        // 关流  
        IOUtils.closeStream (atguiguOut);  
        IOUtils.closeStream (otherOut);  
    }  
}
</code></pre>
<p>（5）编写 LogDriver 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.outputformat;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
import java.io.IOException;  
  
public class LogDriver {  
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {  
  
        Configuration conf = new Configuration ();  
        Job job = Job.getInstance (conf);  
  
        job.setJarByClass (LogDriver.class);  
        job.setMapperClass (LogMapper.class);  
        job.setReducerClass (LogReducer.class);  
  
        job.setMapOutputKeyClass (Text.class);  
        job.setMapOutputValueClass (NullWritable.class);  
  
        job.setOutputKeyClass (Text.class);  
        job.setOutputValueClass (NullWritable.class);  
  
        // 设置自定义的 outputformat  
        job.setOutputFormatClass (LogOutputFormat.class);  
  
        FileInputFormat.setInputPaths (job, new Path ("D:\\input"));  
        // 虽然我们自定义了 outputformat，但是因为我们的 outputformat 继承自 fileoutputformat  
        // 而 fileoutputformat 要输出一个_SUCCESS 文件，所以在这还得指定一个输出目录  
        FileOutputFormat.setOutputPath (job, new Path ("D:\\logoutput"));  
  
        boolean b = job.waitForCompletion (true);  
        System.exit (b ? 0 : 1);  
    }  
}
</code></pre>
<h2 id="35-mapreduce-内核源码解析"><a class="header" href="#35-mapreduce-内核源码解析">3.5 MapReduce 内核源码解析</a></h2>
<h3 id="351-maptask-工作机制"><a class="header" href="#351-maptask-工作机制">3.5.1 MapTask 工作机制</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680580349014.png" alt="image-20221110132710797" /></p>
<p>​ （1）Read 阶段：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中解析出一个个 key/value。</p>
<p>​ （2）Map 阶段：该节点主要是将解析出的 key/value 交给用户编写 map () 函数处理，并产生一系列新的 key/value。</p>
<p>​ （3）Collect 收集阶段：在用户编写 map () 函数中，当数据处理完成后，一般会调用 OutputCollector.collect () 输出结果。在该函数内部，它会将生成的 key/value 分区（调用 Partitioner），并写入一个环形内存缓冲区中。</p>
<p>​ （4）Spill 阶段：即 “溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>​ 溢写阶段详情：</p>
<p>​ 步骤 1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号 Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。</p>
<p>​ 步骤 2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件 output/spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p>
<p>​ 步骤 3：将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件 output/spillN.out.index 中。</p>
<p>​ （5）Merge 阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>​ 当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output/file.out 中，同时生成相应的索引文件 output/file.out.index。</p>
<p>​ 在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并 mapreduce.task.io.sort.factor（默认 10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>​ 让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<h3 id="352-reducetask-工作机制"><a class="header" href="#352-reducetask-工作机制">3.5.2 ReduceTask 工作机制</a></h3>
<p><img src="https://image.3001.net/images/20221110/16680580777443.png" alt="image-20221110132754693" /></p>
<p>​ （1）Copy 阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>​ （2）Sort 阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照 MapReduce 语义，用户编写 reduce () 函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。</p>
<p>​ （3）Reduce 阶段：reduce () 函数将计算结果写到 HDFS 上。</p>
<h3 id="353-reducetask-并行度决定机制"><a class="header" href="#353-reducetask-并行度决定机制">3.5.3 ReduceTask 并行度决定机制</a></h3>
<p><strong>回顾：</strong> MapTask 并行度由切片个数决定，切片个数由输入文件和切片规则决定。</p>
<p><strong>思考：</strong> ReduceTask 并行度由谁决定？</p>
<p><strong>1</strong>）设置 ReduceTask 并行度（个数）</p>
<p>​ ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置：</p>
<pre><code class="language-java">// 默认值是 1，手动设置为 4  
job.setNumReduceTasks (4);
</code></pre>
<p><strong>2</strong>）实验：测试 ReduceTask 多少合适</p>
<p>（1）实验环境：1 个 Master 节点，16 个 Slave 节点：CPU:8GHZ，内存: 2G</p>
<p>（2）实验结论：</p>
<p>表 改变 ReduceTask（数据量为 1GB）</p>
<p>MapTask =16</p>
<div class="table-wrapper"><table><thead><tr><th>ReduceTask</th><th>1</th><th>5</th><th>10</th><th>15</th><th>16</th><th>20</th><th>25</th><th>30</th><th>45</th><th>60</th></tr></thead><tbody>
<tr><td>总时间</td><td>892</td><td>146</td><td>110</td><td>92</td><td>88</td><td>100</td><td>128</td><td>101</td><td>145</td><td>104</td></tr>
</tbody></table>
</div>
<p><strong>3</strong>）注意事项</p>
<p><img src="https://image.3001.net/images/20221110/16680581412729.png" alt="image-20221110132857785" /></p>
<h3 id="354-maptask--reducetask-源码解析"><a class="header" href="#354-maptask--reducetask-源码解析">3.5.4 MapTask &amp; ReduceTask 源码解析</a></h3>
<p><strong>1</strong>）MapTask 源码解析流程</p>
<p><img src="https://image.3001.net/images/20221110/16680581638498.png" alt="image-20221110132920449" /></p>
<p><strong>2</strong>）ReduceTask 源码解析流程</p>
<p><img src="https://image.3001.net/images/20221110/16680582064495.png" alt="image-20221110133003542" /></p>
<h2 id="36-join-应用"><a class="header" href="#36-join-应用">3.6 Join 应用</a></h2>
<h3 id="361-reduce-join"><a class="header" href="#361-reduce-join">3.6.1 Reduce Join</a></h3>
<p>​Map 端的主要工作：为来自不同表或文件的 key/value 对，打标签以区别不同来源的记录。然后用连接字段作为 key，其余部分和新加的标志作为 value，最后进行输出。</p>
<p>Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进行合并就 ok 了。</p>
<h3 id="362-reduce-join-案例实操"><a class="header" href="#362-reduce-join-案例实操">3.6.2 Reduce Join 案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>order.txt</p>
<pre><code>1001	01	1
1002	02	2
1003	03	3
1004	01	4
1005	02	5
1006	03	6
</code></pre>
<p>pd.txt</p>
<pre><code>01	小米
02	华为
03	格力
</code></pre>
<p>表 4-4 订单数据表 t_order</p>
<div class="table-wrapper"><table><thead><tr><th>id</th><th>pid</th><th>amount</th></tr></thead><tbody>
<tr><td>1001</td><td>01</td><td>1</td></tr>
<tr><td>1002</td><td>02</td><td>2</td></tr>
<tr><td>1003</td><td>03</td><td>3</td></tr>
<tr><td>1004</td><td>01</td><td>4</td></tr>
<tr><td>1005</td><td>02</td><td>5</td></tr>
<tr><td>1006</td><td>03</td><td>6</td></tr>
</tbody></table>
</div>
<p>表 4-5 商品信息表 t_product</p>
<div class="table-wrapper"><table><thead><tr><th>pid</th><th>pname</th></tr></thead><tbody>
<tr><td>01</td><td>小米</td></tr>
<tr><td>02</td><td>华为</td></tr>
<tr><td>03</td><td>格力</td></tr>
</tbody></table>
</div>
<p>​将商品信息表中数据根据商品 pid 合并到订单数据表中。</p>
<p>表 4-6 最终数据形式</p>
<div class="table-wrapper"><table><thead><tr><th>id</th><th>pname</th><th>amount</th></tr></thead><tbody>
<tr><td>1001</td><td>小米</td><td>1</td></tr>
<tr><td>1004</td><td>小米</td><td>4</td></tr>
<tr><td>1002</td><td>华为</td><td>2</td></tr>
<tr><td>1005</td><td>华为</td><td>5</td></tr>
<tr><td>1003</td><td>格力</td><td>3</td></tr>
<tr><td>1006</td><td>格力</td><td>6</td></tr>
</tbody></table>
</div>
<p><strong>2</strong>）需求分析</p>
<p>​ 通过将关联条件作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。</p>
<p><img src="https://image.3001.net/images/20221110/16680584431906.png" alt="image-20221110133359824" /></p>
<p><strong>3</strong>）代码实现</p>
<p>（1）创建商品和订单合并后的 TableBean 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.reducejoin;  
  
import org.apache.hadoop.io.Writable;  
  
import java.io.DataInput;  
import java.io.DataOutput;  
import java.io.IOException;  
  
public class TableBean implements Writable {  
  
    private String id; // 订单 id  
    private String pid; // 产品 id  
    private int amount; // 产品数量  
    private String pname; // 产品名称  
    private String flag; // 判断是 order 表还是 pd 表的标志字段  
  
    public TableBean () {  
    }  
  
    public String getId () {  
        return id;  
    }  
  
    public void setId (String id) {  
        this.id = id;  
    }  
  
    public String getPid () {  
        return pid;  
    }  
  
    public void setPid (String pid) {  
        this.pid = pid;  
    }  
  
    public int getAmount () {  
        return amount;  
    }  
  
    public void setAmount (int amount) {  
        this.amount = amount;  
    }  
  
    public String getPname () {  
        return pname;  
    }  
  
    public void setPname (String pname) {  
        this.pname = pname;  
    }  
  
    public String getFlag () {  
        return flag;  
    }  
  
    public void setFlag (String flag) {  
        this.flag = flag;  
    }  
  
    @Override  
    public String toString () {  
        return id + "\t" + pname + "\t" + amount;  
    }  
  
    @Override  
    public void write (DataOutput out) throws IOException {  
        out.writeUTF (id);  
        out.writeUTF (pid);  
        out.writeInt (amount);  
        out.writeUTF (pname);  
        out.writeUTF (flag);  
    }  
  
    @Override  
    public void readFields (DataInput in) throws IOException {  
        this.id = in.readUTF ();  
        this.pid = in.readUTF ();  
        this.amount = in.readInt ();  
        this.pname = in.readUTF ();  
        this.flag = in.readUTF ();  
    }  
}
</code></pre>
<p>（2）编写 TableMapper 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.reducejoin;  
  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.InputSplit;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.lib.input.FileSplit;  
  
import java.io.IOException;  
  
public class TableMapper extends Mapper&lt;LongWritable,Text,Text,TableBean&gt; {  
  
    private String filename;  
    private Text outK = new Text ();  
    private TableBean outV = new TableBean ();  
  
    @Override  
    protected void setup (Context context) throws IOException, InterruptedException {  
        // 获取对应文件名称  
        InputSplit split = context.getInputSplit ();  
        FileSplit fileSplit = (FileSplit) split;  
        filename = fileSplit.getPath ().getName ();  
    }  
  
    @Override  
    protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
  
        // 获取一行  
        String line = value.toString ();  
  
        // 判断是哪个文件，然后针对文件进行不同的操作  
        if (filename.contains ("order")){  // 订单表的处理  
            String [] split = line.split ("\t");  
            // 封装 outK  
            outK.set (split [1]);  
            // 封装 outV  
            outV.setId (split [0]);  
            outV.setPid (split [1]);  
            outV.setAmount (Integer.parseInt (split [2]));  
            outV.setPname ("");  
            outV.setFlag ("order");  
        } else {                             // 商品表的处理  
            String [] split = line.split ("\t");  
            // 封装 outK  
            outK.set (split [0]);  
            // 封装 outV  
            outV.setId ("");  
            outV.setPid (split [0]);  
            outV.setAmount (0);  
            outV.setPname (split [1]);  
            outV.setFlag ("pd");  
        }  
  
        // 写出 KV  
        context.write (outK,outV);  
    }  
}
</code></pre>
<p>（3）编写 TableReducer 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.reducejoin;  
  
import org.apache.commons.beanutils.BeanUtils;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
  
import java.io.IOException;  
import java.lang.reflect.InvocationTargetException;  
import java.util.ArrayList;  
  
public class TableReducer extends Reducer&lt;Text,TableBean,TableBean, NullWritable&gt; {  
  
    @Override  
    protected void reduce (Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException {  
  
        ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;();  
        TableBean pdBean = new TableBean ();  
  
        for (TableBean value : values) {  
  
            // 判断数据来自哪个表  
            if ("order".equals (value.getFlag ())){   // 订单表  
  
			  // 创建一个临时 TableBean 对象接收 value，防止数据被覆盖
                TableBean tmpOrderBean = new TableBean ();  
  
                try {  
                    BeanUtils.copyProperties (tmpOrderBean,value);  
                } catch (IllegalAccessException e) {  
                    e.printStackTrace ();  
                } catch (InvocationTargetException e) {  
                    e.printStackTrace ();  
                }  
  
			  // 将临时 TableBean 对象添加到集合 orderBeans  
                orderBeans.add (tmpOrderBean);  
            } else {                                    // 商品表  
                try {  
                    BeanUtils.copyProperties (pdBean,value);  
                } catch (IllegalAccessException e) {  
                    e.printStackTrace ();  
                } catch (InvocationTargetException e) {  
                    e.printStackTrace ();  
                }  
            }  
        }  
  
        // 遍历集合 orderBeans, 替换掉每个 orderBean 的 pid 为 pname, 然后写出  
        for (TableBean orderBean : orderBeans) {  
  
            orderBean.setPname (pdBean.getPname ());  
  
		   // 写出修改后的 orderBean 对象  
            context.write (orderBean,NullWritable.get ());  
        }  
    }  
}
</code></pre>
<p>（4）编写 TableDriver 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.reducejoin;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
import java.io.IOException;  
  
public class TableDriver {  
    public static void main (String [] args) throws IOException, ClassNotFoundException, InterruptedException {  
        Job job = Job.getInstance (new Configuration ());  
  
        job.setJarByClass (TableDriver.class);  
        job.setMapperClass (TableMapper.class);  
        job.setReducerClass (TableReducer.class);  
  
        job.setMapOutputKeyClass (Text.class);  
        job.setMapOutputValueClass (TableBean.class);  
  
        job.setOutputKeyClass (TableBean.class);  
        job.setOutputValueClass (NullWritable.class);  
  
        FileInputFormat.setInputPaths (job, new Path ("D:\\input"));  
        FileOutputFormat.setOutputPath (job, new Path ("D:\\output"));  
  
        boolean b = job.waitForCompletion (true);  
        System.exit (b ? 0 : 1);  
    }  
}
</code></pre>
<p><strong>4</strong>）测试</p>
<p>运行程序查看结果</p>
<pre><code>1004	小米	4
1001	小米	1
1005	华为	5
1002	华为	2
1006	格力	6
1003	格力	3
</code></pre>
<p><strong>5</strong>）总结</p>
<p>缺点：这种方式中，合并的操作是在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜。</p>
<p><strong>解决方案：Map 端实现数据合并。</strong></p>
<h3 id="363-map-join"><a class="header" href="#363-map-join">3.6.3 Map Join</a></h3>
<p><strong>1</strong>）使用场景</p>
<p>Map Join 适用于一张表十分小、一张表很大的场景。</p>
<p><strong>2</strong>）优点</p>
<p>思考：在 Reduce 端处理过多的表，非常容易产生数据倾斜。怎么办？</p>
<p>在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数据的压力，尽可能的减少数据倾斜。</p>
<p><strong>3</strong>）具体办法：采用 DistributedCache</p>
<p>​ （1）在 Mapper 的 setup 阶段，将文件读取到缓存集合中。</p>
<p>​ （2）在 Driver 驱动类中加载缓存。</p>
<pre><code class="language-java">// 缓存普通文件到 Task 运行节点。  
job.addCacheFile (new URI ("file:///e:/cache/pd.txt"));  
// 如果是集群运行，需要设置 HDFS 路径  
job.addCacheFile (new URI ("hdfs://hadoop102:8020/cache/pd.txt"));
</code></pre>
<h3 id="364-map-join-案例实操"><a class="header" href="#364-map-join-案例实操">3.6.4 Map Join 案例实操</a></h3>
<p><strong>1</strong>）需求</p>
<p>表 订单数据表 t_order</p>
<div class="table-wrapper"><table><thead><tr><th>id</th><th>pid</th><th>amount</th></tr></thead><tbody>
<tr><td>1001</td><td>01</td><td>1</td></tr>
<tr><td>1002</td><td>02</td><td>2</td></tr>
<tr><td>1003</td><td>03</td><td>3</td></tr>
<tr><td>1004</td><td>01</td><td>4</td></tr>
<tr><td>1005</td><td>02</td><td>5</td></tr>
<tr><td>1006</td><td>03</td><td>6</td></tr>
</tbody></table>
</div>
<p>表 商品信息表 t_product</p>
<div class="table-wrapper"><table><thead><tr><th>pid</th><th>pname</th></tr></thead><tbody>
<tr><td>01</td><td>小米</td></tr>
<tr><td>02</td><td>华为</td></tr>
<tr><td>03</td><td>格力</td></tr>
</tbody></table>
</div>
<p>​ 将商品信息表中数据根据商品 pid 合并到订单数据表中。</p>
<p>表最终数据形式</p>
<div class="table-wrapper"><table><thead><tr><th>id</th><th>pname</th><th>amount</th></tr></thead><tbody>
<tr><td>1001</td><td>小米</td><td>1</td></tr>
<tr><td>1004</td><td>小米</td><td>4</td></tr>
<tr><td>1002</td><td>华为</td><td>2</td></tr>
<tr><td>1005</td><td>华为</td><td>5</td></tr>
<tr><td>1003</td><td>格力</td><td>3</td></tr>
<tr><td>1006</td><td>格力</td><td>6</td></tr>
</tbody></table>
</div>
<p><strong>2</strong>）需求分析</p>
<p>MapJoin 适用于关联表中有小表的情形。</p>
<p><img src="https://image.3001.net/images/20221110/16680586993772.png" alt="image-20221110133816471" /></p>
<p><strong>3</strong>）实现代码</p>
<p>（1）先在 MapJoinDriver 驱动类中添加缓存文件</p>
<pre><code class="language-java">package com.TianHan.mapreduce.mapJoin;  
    
import org.apache.hadoop.conf.Configuration;    
import org.apache.hadoop.fs.Path;    
import org.apache.hadoop.io.NullWritable;    
import org.apache.hadoop.io.Text;    
import org.apache.hadoop.mapreduce.Job;    
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;    
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;    
    
import java.io.IOException;    
import java.net.URI;    
import java.net.URISyntaxException;    
    
public class MapJoinDriver {    
    
    public static void main (String [] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException {    
    
        // 1 获取 job 信息    
		Configuration conf = new Configuration ();    
        Job job = Job.getInstance (conf);    
        // 2 设置加载 jar 包路径    
		job.setJarByClass (MapJoinDriver.class);    
        // 3 关联 mapper    
		job.setMapperClass (MapJoinMapper.class);    
        // 4 设置 Map 输出 KV 类型    
		job.setMapOutputKeyClass (Text.class);    
        job.setMapOutputValueClass (NullWritable.class);    
        // 5 设置最终输出 KV 类型    
		job.setOutputKeyClass (Text.class);    
        job.setOutputValueClass (NullWritable.class);    
    
        // 加载缓存数据    
		job.addCacheFile (new URI ("file:///E:/BigData/hadoop/inputJoin/tablecache/pd.txt"));  
        // Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0    
		job.setNumReduceTasks (0);    
    
        // 6 设置输入输出路径    
		FileInputFormat.setInputPaths (job, new Path ("E:\\BigData\\hadoop\\inputJoin\\input"));  
        FileOutputFormat.setOutputPath (job, new Path ("E:\\BigData\\hadoop\\inputJoin\\output"));  
        // 7 提交    
		boolean b = job.waitForCompletion (true);    
        System.exit (b ? 0 : 1);    
    }    
}
</code></pre>
<p>（2）在 MapJoinMapper 类中的 setup 方法中读取缓存文件</p>
<pre><code class="language-java">package com.TianHan.mapreduce.mapJoin;  
  
import org.apache.commons.lang3.StringUtils;  
import org.apache.hadoop.fs.FSDataInputStream;    
import org.apache.hadoop.fs.FileSystem;    
import org.apache.hadoop.fs.Path;    
import org.apache.hadoop.io.IOUtils;    
import org.apache.hadoop.io.LongWritable;    
import org.apache.hadoop.io.NullWritable;    
import org.apache.hadoop.io.Text;    
import org.apache.hadoop.mapreduce.Mapper;    
    
import java.io.BufferedReader;    
import java.io.IOException;    
import java.io.InputStreamReader;    
import java.net.URI;    
import java.util.HashMap;    
import java.util.Map;    
    
public class MapJoinMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {    
    
    private Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;();    
    private Text text = new Text ();    
    
    // 任务开始前将 pd 数据缓存进 pdMap    
@Override    
protected void setup (Context context) throws IOException, InterruptedException {    
    
        // 通过缓存文件得到小表数据 pd.txt    
		URI [] cacheFiles = context.getCacheFiles ();    
        Path path = new Path (cacheFiles [0]);    
    
        // 获取文件系统对象，并开流    
		FileSystem fs = FileSystem.get (context.getConfiguration ());    
        FSDataInputStream fis = fs.open (path);    
    
        // 通过包装流转换为 reader, 方便按行读取    
		BufferedReader reader = new BufferedReader (new InputStreamReader (fis, "UTF-8"));    
    
        // 逐行读取，按行处理    
		String line;    
        while (StringUtils.isNotEmpty (line = reader.readLine ())) {    
            // 切割一行        
			//01    小米  
            String [] split = line.split ("\t");    
            pdMap.put (split [0], split [1]);    
        }    
    
        // 关流    
		IOUtils.closeStream (reader);    
    }    
    
    @Override    
protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {    
    
        // 读取大表 order.txt 数据  
        //1001  01 1  
        String [] fields = value.toString ().split ("\t");    
    
        // 通过大表每行数据的 pid, 去 pdMap 里面取出 pname    
		String pname = pdMap.get (fields [1]);    
    
        // 将大表每行数据的 pid 替换为 pname    
		text.set (fields [0] + "\t" + pname + "\t" + fields [2]);    
    
        // 写出    
		context.write (text,NullWritable.get ());    
    }    
}
</code></pre>
<h2 id="程序运行结果"><a class="header" href="#程序运行结果">程序运行结果</a></h2>
<p>part-m-00000</p>
<pre><code>1001	小米	1
1002	华为	2
1003	格力	3
1004	小米	4
1005	华为	5
1006	格力	6
</code></pre>
<h2 id="37-数据清洗etl"><a class="header" href="#37-数据清洗etl">3.7 数据清洗（ETL）</a></h2>
<p>ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据仓库</p>
<p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序。</p>
<p><strong>1</strong>）需求</p>
<p>去除日志中字段个数小于等于 11 的日志。</p>
<p>（1）输入数据</p>
<p>web.log</p>
<p>在项目中找</p>
<p>（2）期望输出数据</p>
<p>每行字段长度都大于 11。</p>
<p><strong>2</strong>）需求分析</p>
<p>需要在 Map 阶段对输入的数据根据规则进行过滤清洗。</p>
<p><strong>3</strong>）实现代码</p>
<p>（1）编写 WebLogMapper 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.weblog;  
import java.io.IOException;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
  
public class WebLogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;{  
	  
	@Override  
	protected void map (LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
		  
		// 1 获取 1 行数据  
		String line = value.toString ();  
		  
		// 2 解析日志  
		boolean result = parseLog (line,context);  
		  
		// 3 日志不合法退出  
		if (!result) {  
			return;  
		}  
		  
		// 4 日志合法就直接写出  
		context.write (value, NullWritable.get ());  
	}  
  
	// 2 封装解析日志的方法  
	private boolean parseLog (String line, Context context) {  
  
		// 1 截取  
		String [] fields = line.split (" ");  
		  
		// 2 日志长度大于 11 的为合法  
		if (fields.length &gt; 11) {  
			return true;  
		} else {  
			return false;  
		}  
	}  
}
</code></pre>
<p>（2）编写 WebLogDriver 类</p>
<pre><code class="language-java">package com.atguigu.mapreduce.weblog;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.NullWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
public class WebLogDriver {  
	public static void main (String [] args) throws Exception {  
  
// 输入输出路径需要根据自己电脑上实际的输入输出路径设置  
        args = new String [] { "D:/input/inputlog", "D:/output1" };  
  
		// 1 获取 job 信息  
		Configuration conf = new Configuration ();  
		Job job = Job.getInstance (conf);  
  
		// 2 加载 jar 包  
		job.setJarByClass (WebLogDriver.class);  
  
		// 3 关联 map  
		job.setMapperClass (WebLogMapper.class);  
  
		// 4 设置最终输出类型  
		job.setOutputKeyClass (Text.class);  
		job.setOutputValueClass (NullWritable.class);  
  
		// 设置 reducetask 个数为 0  
		job.setNumReduceTasks (0);  
  
		// 5 设置输入和输出路径  
		FileInputFormat.setInputPaths (job, new Path (args [0]));  
		FileOutputFormat.setOutputPath (job, new Path (args [1]));  
  
		// 6 提交  
         boolean b = job.waitForCompletion (true);  
         System.exit (b ? 0 : 1);  
	}  
}
</code></pre>
<h2 id="附录etl-清洗规则"><a class="header" href="#附录etl-清洗规则">附录：ETL 清洗规则</a></h2>
<p>https://developer.aliyun.com/article/1319420</p>
<p>好，干货开始。数据清洗的目的可以从两个角度上看一是为了解决数据质量问题，二是让数据更适合做挖掘。不同的目的下分不同的情况，也都有相应的解决方式和方法。</p>
<h3 id="解决数据质量问题"><a class="header" href="#解决数据质量问题">解决数据质量问题</a></h3>
<p>这部分主要是规范数据，满足业务的使用，解决数据质量的各种问题，其目的包括但不限于：</p>
<ol>
<li>数据的完整性 ---- 例如人的属性中缺少性别、籍贯、年龄等</li>
<li>数据的唯一性 ---- 例如不同来源的数据出现重复的情况</li>
<li>数据的权威性 ---- 例如同一个指标出现多个来源的数据，且数值不一样</li>
<li>数据的合法性 ---- 例如获取的数据与常识不符，年龄大于 150 岁</li>
<li>数据的一致性 ---- 例如不同来源的不同指标，实际内涵是一样的，或是同一指标内涵不一致</li>
</ol>
<p>数据清洗的结果是对各种脏数据进行对应方式的处理，得到标准的、干净的、连续的数据，提供给数据统计、数据挖掘等使用。</p>
<p>那么为了解决以上的各种问题，我们需要不同的手段和方法来一一处理。</p>
<p>每种问题都有各种情况，每种情况适用不同的处理方法，具体如下：</p>
<p>1：解决数据的完整性问题：</p>
<blockquote>
<p>解题思路：数据缺失，那么补上就好了。补数据有什么方法？</p>
</blockquote>
<ul>
<li>通过其他信息补全，例如使用身份证件号码推算性别、籍贯、出生日期、年龄等</li>
<li>通过前后数据补全，例如时间序列缺数据了，可以使用前后的均值，缺的多了，可以使用平滑等处理，记得 Matlab 还是什么工具可以自动补全</li>
<li>实在补不全的，虽然很可惜，但也必须要剔除。但是不要删掉，没准以后可以用得上</li>
</ul>
<p>2：解决数据的唯一性问题</p>
<blockquote>
<p>解题思路：去除重复记录，只保留一条。去重的方法有：</p>
</blockquote>
<ul>
<li>按主键去重，用 sql 或者 excel “去除重复记录” 即可，</li>
<li>按规则去重，编写一系列的规则，对重复情况复杂的数据进行去重。例如不同渠道来的客户数据，可以通过相同的关键信息进行匹配，合并去重。</li>
</ul>
<p>3：解决数据的权威性问题</p>
<blockquote>
<p>解题思路：用最权威的那个渠道的数据方法：对不同渠道设定权威级别，例如：在家里，首先得相信媳妇说的。</p>
</blockquote>
<p>4：解决数据的合法性问题</p>
<blockquote>
<p>解题思路：设定判定规则</p>
</blockquote>
<ol>
<li>设定强制合法规则，凡是不在此规则范围内的，强制设为最大值，或者判为无效，剔除</li>
</ol>
<ul>
<li>字段类型合法规则：日期字段格式为 “2010-10-10”</li>
<li>字段内容合法规则：性别 in （男、女、未知）；出生日期 &lt;= 今天</li>
</ul>
<ol start="2">
<li>设定警告规则，凡是不在此规则范围内的，进行警告，然后人工处理</li>
</ol>
<ul>
<li>警告规则：年龄 &gt; 110</li>
</ul>
<ol start="3">
<li>离群值人工特殊处理，使用分箱、聚类、回归、等方式发现离群值</li>
</ol>
<p>5：解决数据的一致性问题</p>
<blockquote>
<p>解题思路：建立元数据体系，包含但不限于：</p>
</blockquote>
<ol>
<li>指标体系（度量）</li>
<li>维度（分组、统计口径）</li>
<li>单位</li>
<li>频度</li>
<li>数据</li>
</ol>
<p>tips：</p>
<p>如果数据质量问题比较严重，建议跟技术团队好好聊聊。</p>
<p>如果需要控制的范围越来越大，这就不是 ETL 工程师的工作了，得升级为数据治理了，下次有空再分享。</p>
<h3 id="供应算法原料"><a class="header" href="#供应算法原料">供应算法原料</a></h3>
<ol>
<li>
<p>这部分主要是让数据更适合数据挖掘，作为算法训练的原料。其目标包括但不限于：</p>
</li>
<li>
<p>高维度 ---- 不适合挖掘</p>
</li>
<li>
<p>维度太低 ---- 不适合挖掘</p>
</li>
<li>
<p>无关信息 ---- 减少存储</p>
</li>
<li>
<p>字段冗余 ---- 一个字段是其他字段计算出来的，会造成相关系数为 1 或者主成因分析异常）</p>
</li>
<li>
<p>多指标数值、单位不同 ---- 如 GDP 与城镇居民人均收入数值相差过大</p>
</li>
</ol>
<p>1：解决高维度问题</p>
<blockquote>
<p>解题思路：降维，方法包括但不限于：</p>
</blockquote>
<ol>
<li>主成分分析</li>
<li>随机森林</li>
</ol>
<p>2：解决维度低或缺少维度问题</p>
<blockquote>
<p>解题思路：抽象，方法包括但不限于：</p>
</blockquote>
<ol>
<li>各种汇总，平均、加总、最大、最小等</li>
<li>各种离散化，聚类、自定义分组等</li>
</ol>
<p>3：解决无关信息和字段冗余</p>
<blockquote>
<p>解决方法：剔除字段</p>
</blockquote>
<p>4：解决多指标数值、单位不同问题</p>
<blockquote>
<p>解决方法：归一化，方法包括但不限于：</p>
</blockquote>
<ol>
<li>最小 - 最大</li>
<li>零 - 均值</li>
<li>小数定标</li>
</ol>
<p>其实 ETL 工程师有非常好的数据功底，无论是转那个岗都方便，你缺少的是系统的学习和迈出去的勇气。</p>
<h2 id="附录常用正则表达式"><a class="header" href="#附录常用正则表达式">附录常用正则表达式</a></h2>
<p><a href="https://blog.csdn.net/ws54ws54/article/details/110220049">Java 常用正则表达式大全 (史上最全的正则表达式 - 匹配中英文、字母和数字)</a></p>
<p>在做项目的过程中，使用正则表达式来匹配一段文本中的特定种类字符，是比较常用的一种方式，下面是对常用的正则匹配做了一个归纳整理。</p>
<h3 id="一校验数字的表达式"><a class="header" href="#一校验数字的表达式">一、校验数字的表达式</a></h3>
<pre><code>1 数字：^[0-9]*$
2 n 位的数字：^\d {n}$
3 至少 n 位的数字：^\d {n,}$
4 m-n 位的数字：^\d {m,n}$
5 零和非零开头的数字：^(0|[1-9][0-9]*)$
6 非零开头的最多带两位小数的数字：^([1-9][0-9]*)(.[0-9]{1,2})?$
7 带 1-2 位小数的正数或负数：^(\-)?\d+(\.\d {1,2})?$
8 正数、负数、和小数：^(\-|\+)?\d+(\.\d+)?$
9 有两位小数的正实数：^[0-9]+(.[0-9]{2})?$
10 有 1~3 位小数的正实数：^[0-9]+(.[0-9]{1,3})?$
11 非零的正整数：^[1-9]\d*$ 或 ^([1-9][0-9]*){1,3}$ 或 ^\+?[1-9][0-9]*$
12 非零的负整数：^\-[1-9][] 0-9"*$ 或 ^-[1-9]\d*$
13 非负整数：^\d+$ 或 ^[1-9]\d*|0$
14 非正整数：^-[1-9]\d*|0$ 或 ^((-\d+)|(0+))$
15 非负浮点数：^\d+(\.\d+)?$ 或 ^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$
16 非正浮点数：^((-\d+(\.\d+)?)|(0+(\.0+)?))$ 或 ^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$
17 正浮点数：^[1-9]\d*\.\d*|0\.\d*[1-9]\d*$ 或 ^(([0-9]+\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\.[0-9]+)|([0-9]*[1-9][0-9]*))$
18 负浮点数：^-([1-9]\d*\.\d*|0\.\d*[1-9]\d*)$ 或 ^(-(([0-9]+\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\.[0-9]+)|([0-9]*[1-9][0-9]*)))$
19 浮点数：^(-?\d+)(\.\d+)?$ 或 ^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$
</code></pre>
<h3 id="二校验字符的表达式"><a class="header" href="#二校验字符的表达式">二、校验字符的表达式</a></h3>
<pre><code>1 汉字：^[\u4e00-\u9fa5]{0,}$
2 英文和数字：^[A-Za-z0-9]+$ 或 ^[A-Za-z0-9]{4,40}$
3 长度为 3-20 的所有字符：^.{3,20}$
4 由 26 个英文字母组成的字符串：^[A-Za-z]+$
5 由 26 个大写英文字母组成的字符串：^[A-Z]+$
6 由 26 个小写英文字母组成的字符串：^[a-z]+$
7 由数字和 26 个英文字母组成的字符串：^[A-Za-z0-9]+$
8 由数字、26 个英文字母或者下划线组成的字符串：^\w+$ 或 ^\w {3,20}$
9 中文、英文、数字包括下划线：^[\u4E00-\u9FA5A-Za-z0-9_]+$
10 中文、英文、数字但不包括下划线等符号：^[\u4E00-\u9FA5A-Za-z0-9]+$ 或 ^[\u4E00-\u9FA5A-Za-z0-9]{2,20}$
11 可以输入含有 ^%&amp;’,;=?KaTeX parse error: Can't use function '\"' in math mode at position 1: \̲"̲等字符：`[^%&amp;',;=?\x22]+12 禁止输入含有～的字符：[^~\x22]+ 三、特殊需求表达式 1 Email 地址：^\w+([-+.]\w+)@\w+([-.]\w+).\w+([-.]\w+) KaTeX parse error: Undefined control sequence: \s at position 108: …`[a-zA-z]+://[^\̲s̲]*`或`^http://…4 手机号码：^(13 [0-9]|14 [5|7]|15 [0|1|2|3|5|6|7|8|9]|18 [0|1|2|3|5|6|7|8|9])\d {8}$
5 电话号码 (“XXX-XXXXXXX”、“XXXX-XXXXXXXX”、“XXX-XXXXXXX”、“XXX-XXXXXXXX”、"XXXXXXX" 和 "XXXXXXXX)：^(\(\d {3,4}-)|\d {3.4}-)?\d {7,8}$
6 国内电话号码 (0511-4405222、021-87888822)：\d {3}-\d {8}|\d {4}-\d {7}
7 身份证号 (15 位、18 位数字)：^\d {15}|\d {18}$
8 短身份证号码 (数字、字母 x 结尾)：^([0-9]){7,18}(x|X)?$ 或 ^\d {8,18}|[0-9x]{8,18}|[0-9X]{8,18}?$
9 帐号是否合法 (字母开头，允许 5-16 字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$
10 密码 (以字母开头，长度在 6~18 之间，只能包含字母、数字和下划线)：^[a-zA-Z]\w {5,17}$
11 强密码 (必须包含大小写字母和数字的组合，不能使用特殊字符，长度在 8-10 之间)：^(?=.*\d)(?=.*[a-z])(?=.*[A-Z]).{8,10}$
12 日期格式：^\d {4}-\d {1,2}-\d {1,2}
13 一年的 12 个月 (01～09 和 1～12)：^(0?[1-9]|1 [0-2])$
14 一个月的 31 天 (01～09 和 1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$
15 钱的输入格式：
16 1. 有四种钱的表示形式我们可以接受:“10000.00” 和 “10,000.00”, 和没有 “分” 的 “10000” 和 “10,000”：^[1-9][0-9]*$
17 2. 这表示任意一个不以 0 开头的数字，但是，这也意味着一个字符 "0" 不通过，所以我们采用下面的形式：^(0|[1-9][0-9]*)$
18 3. 一个 0 或者一个不以 0 开头的数字。我们还可以允许开头有一个负号：^(0|-?[1-9][0-9]*)$
19 4. 这表示一个 0 或者一个可能为负的开头不为 0 的数字。让用户以 0 开头好了。把负号的也去掉，因为钱总不能是负的吧。下面我们要加的是说明可能的小数部分：^[0-9]+(.[0-9]+)?$
20 5. 必须说明的是，小数点后面至少应该有 1 位数，所以 "10.“是不通过的，但是 “10” 和 “10.2” 是通过的：^[0-9]+(.[0-9]{2})?$
21 6. 这样我们规定小数点后面必须有两位，如果你认为太苛刻了，可以这样：^[0-9]+(.[0-9]{1,2})?$
22 7. 这样就允许用户只写一位小数。下面我们该考虑数字中的逗号了，我们可以这样：^[0-9]{1,3}(,[0-9]{3})*(.[0-9]{1,2})?$
23 8.1 到 3 个数字，后面跟着任意个 逗号 + 3 个数字，逗号成为可选，而不是必须：^([0-9]+|[0-9]{1,3}(,[0-9]{3})*)(.[0-9]{1,2})?$
24 备注：这就是最终结果了，别忘了”+“可以用”" 替代如果你觉得空字符串也可以接受的话 (奇怪，为什么？) 最后，别忘了在用函数时去掉去掉那个反斜杠，一般的错误都在这里
25 xml 文件：^([a-zA-Z]+-?)+[a-zA-Z0-9]+\\.[x|X][m|M][l|L]$
26 中文字符的正则表达式：[\u4e00-\u9fa5]
27 双字节字符：[^\x00-\xff] (包括汉字在内，可以用来计算字符串的长度 (一个双字节字符长度计 2，ASCII 字符计 1))
28 空白行的正则表达式：\n\s*\r (可以用来删除空白行)
29 HTML 标记的正则表达式：&lt;(\S*?)[^&gt;]*&gt;.*?&lt;/\1&gt;|&lt;.*? /&gt; (网上流传的版本太糟糕，上面这个也仅仅能部分，对于复杂的嵌套标记依旧无能为力)
30 首尾空白字符的正则表达式：^\s*|\s*$ 或 (^\s*)|(\s*$) (可以用来删除行首行尾的空白字符 (包括空格、制表符、换页符等等)，非常有用的表达式)
31 腾讯 QQ 号：[1-9][0-9]{4,} (腾讯 QQ 号从 10000 开始)
32 中国邮政编码：[1-9]\d {5}(?!\d) (中国邮政编码为 6 位数字)
33 IP 地址：\d+\.\d+\.\d+\.\d+ (提取 IP 地址时有用)
</code></pre>
<h2 id="38-mapreduce-开发总结"><a class="header" href="#38-mapreduce-开发总结">3.8 MapReduce 开发总结</a></h2>
<p><strong>1</strong>）输入数据接口：InputFormat</p>
<p>（1）默认使用的实现类是：TextInputFormat</p>
<p>（2）TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为 key，行内容作为 value 返回。</p>
<p>（3）CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</p>
<p><strong>2</strong>）逻辑处理接口：Mapper</p>
<p>用户根据业务需求实现其中三个方法：map ()、setup () 和 cleanup ()</p>
<p><strong>3</strong>）Partitioner 分区</p>
<p>（1）有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；<code>key.hashCode ()&amp;Integer.MAXVALUE % numReduces</code></p>
<p>（2）如果业务上有特别的需求，可以自定义分区。</p>
<p><strong>4</strong>）Comparable 排序</p>
<p>（1）当我们用自定义的对象作为 key 来输出时，就必须要实现 <code>WritableComparable</code> 接口，重写其中的 <code>compareTo ()</code> 方法。</p>
<p>（2）部分排序：对最终输出的每一个文件进行内部排序。</p>
<p>（3）全排序：对所有数据进行排序，通常只有一个 Reduce。</p>
<p>（4）二次排序：排序的条件有两个。</p>
<p><strong>5</strong>）Combiner 合并</p>
<p>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。</p>
<p><strong>6</strong>）逻辑处理接口：Reducer</p>
<p>用户根据业务需求实现其中三个方法：reduce ()、setup () 和 cleanup ()</p>
<p><strong>7</strong>）输出数据接口：OutputFormat</p>
<p>（1）默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</p>
<p>（2）用户还可以自定义 OutputFormat。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="hadoop数据压缩"><a class="header" href="#hadoop数据压缩">Hadoop数据压缩</a></h1>
<ul>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#41-%E6%A6%82%E8%BF%B0">4.1 概述</a></li>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#42-mapreduce%E6%94%AF%E6%8C%81%E7%9A%84%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81">4.2 MapReduce支持的压缩编码</a></li>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#43-%E5%8E%8B%E7%BC%A9%E6%96%B9%E5%BC%8F%E9%80%89%E6%8B%A9">4.3 压缩方式选择</a>
<ul>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#431-gzip%E5%8E%8B%E7%BC%A9">4.3.1 Gzip压缩</a></li>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#432-bzip2%E5%8E%8B%E7%BC%A9">4.3.2 Bzip2压缩</a></li>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#433-lzo%E5%8E%8B%E7%BC%A9">4.3.3 Lzo压缩</a></li>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#434-snappy%E5%8E%8B%E7%BC%A9">4.3.4 Snappy压缩</a></li>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#435-%E5%8E%8B%E7%BC%A9%E4%BD%8D%E7%BD%AE%E9%80%89%E6%8B%A9">4.3.5 压缩位置选择</a></li>
</ul>
</li>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#44-%E5%8E%8B%E7%BC%A9%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE">4.4 压缩参数配置</a></li>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#45-%E5%8E%8B%E7%BC%A9%E5%AE%9E%E6%93%8D%E6%A1%88%E4%BE%8B">4.5 压缩实操案例</a>
<ul>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#451-map%E8%BE%93%E5%87%BA%E7%AB%AF%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9">4.5.1 Map输出端采用压缩</a></li>
<li><a href="Hadoop/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html#452-reduce%E8%BE%93%E5%87%BA%E7%AB%AF%E9%87%87%E7%94%A8%E5%8E%8B%E7%BC%A9">4.5.2 Reduce输出端采用压缩</a></li>
</ul>
</li>
</ul>
<h2 id="41-概述"><a class="header" href="#41-概述">4.1 概述</a></h2>
<p><strong>1</strong>）压缩的好处和坏处</p>
<p>好处：减少磁盘IO、减少磁盘存储空间。</p>
<p>坏处：增加CPU开销。</p>
<p><strong>2</strong>）压缩原则</p>
<p>（1）运算密集型的Job，少用压缩</p>
<p>（2）IO密集型的Job，多用压缩</p>
<h2 id="42-mapreduce支持的压缩编码"><a class="header" href="#42-mapreduce支持的压缩编码">4.2 MapReduce支持的压缩编码</a></h2>
<p>1）压缩算法对比介绍</p>
<div class="table-wrapper"><table><thead><tr><th>压缩格式</th><th>Hadoop自带？</th><th>算法</th><th>文件扩展名</th><th>是否可切片</th><th>换成压缩格式后，原来的程序是否需要修改</th></tr></thead><tbody>
<tr><td>DEFLATE</td><td>是，直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样，不需要修改</td></tr>
<tr><td>Gzip</td><td>是，直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样，不需要修改</td></tr>
<tr><td>bzip2</td><td>是，直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样，不需要修改</td></tr>
<tr><td>LZO</td><td>否，需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引，还需要指定输入格式</td></tr>
<tr><td>Snappy</td><td>是，直接使用</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样，不需要修改</td></tr>
</tbody></table>
</div>
<p>2）压缩性能的比较</p>
<div class="table-wrapper"><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody>
<tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr>
<tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr>
<tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr>
</tbody></table>
</div>
<p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p>
<p>Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger.On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p>
<h2 id="43-压缩方式选择"><a class="header" href="#43-压缩方式选择">4.3 压缩方式选择</a></h2>
<p>​ 压缩方式选择时重点考虑：压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片。</p>
<h3 id="431-gzip压缩"><a class="header" href="#431-gzip压缩">4.3.1 Gzip压缩</a></h3>
<p>优点：压缩率比较高</p>
<p>缺点：不支持Split；压缩/解压速度一般</p>
<h3 id="432-bzip2压缩"><a class="header" href="#432-bzip2压缩">4.3.2 Bzip2压缩</a></h3>
<p>优点：压缩率高；支持Split</p>
<p>缺点：压缩/解压速度慢</p>
<h3 id="433-lzo压缩"><a class="header" href="#433-lzo压缩">4.3.3 Lzo压缩</a></h3>
<p>优点：压缩/解压速度比较快；支持Split</p>
<p>缺点：压缩率一般；想支持切片需要额外创建索引</p>
<h3 id="434-snappy压缩"><a class="header" href="#434-snappy压缩">4.3.4 Snappy压缩</a></h3>
<p>优点：压缩和解压缩速度快</p>
<p>缺点：不支持Split；压缩率一般</p>
<p>注意：使用Snappy需要Hadoop3.x及Linux Kernel 7.5+</p>
<h3 id="435-压缩位置选择"><a class="header" href="#435-压缩位置选择">4.3.5 压缩位置选择</a></h3>
<p>压缩可以在MapReduce作用的任意阶段启用。</p>
<p><img src="https://image.3001.net/images/20221110/16680589476259.png" alt="image-20221110134223786" /></p>
<h2 id="44-压缩参数配置"><a class="header" href="#44-压缩参数配置">4.4 压缩参数配置</a></h2>
<p>1）为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器</p>
<div class="table-wrapper"><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody>
<tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr>
<tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr>
<tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr>
<tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr>
<tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr>
</tbody></table>
</div>
<p>查看相关信息使用<code>hadoop checknative</code>命令</p>
<p>2）要在Hadoop中启用压缩，可以配置如下参数</p>
<div class="table-wrapper"><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody>
<tr><td>io.compression.codecs （在core-site.xml中配置）</td><td>无，这个需要在命令行输入hadoop checknative查看</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr>
<tr><td>mapreduce.map.output.compress（在mapred-site.xml中配置）</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr>
<tr><td>mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td></tr>
<tr><td>mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr>
<tr><td>mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr>
</tbody></table>
</div>
<h2 id="45-压缩实操案例"><a class="header" href="#45-压缩实操案例">4.5 压缩实操案例</a></h2>
<h3 id="451-map输出端采用压缩"><a class="header" href="#451-map输出端采用压缩">4.5.1 Map输出端采用压缩</a></h3>
<p>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。</p>
<p>1）开启并设置map端输出压缩</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.io.compress.BZip2Codec;	  
import org.apache.hadoop.io.compress.CompressionCodec;  
import org.apache.hadoop.io.compress.GzipCodec;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
public class WordCountDriver {  
  
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {  
  
		Configuration conf = new Configuration();  
  
		// 开启map端输出压缩  
		conf.setBoolean("mapreduce.map.output.compress", true);  
  
		// 设置map端输出压缩方式  
		conf.setClass("mapreduce.map.output.compress.codec", BZip2Codec.class,CompressionCodec.class);  
  
		Job job = Job.getInstance(conf);  
  
		job.setJarByClass(WordCountDriver.class);  
  
		job.setMapperClass(WordCountMapper.class);  
		job.setReducerClass(WordCountReducer.class);  
  
		job.setMapOutputKeyClass(Text.class);  
		job.setMapOutputValueClass(IntWritable.class);  
  
		job.setOutputKeyClass(Text.class);  
		job.setOutputValueClass(IntWritable.class);  
  
		FileInputFormat.setInputPaths(job, new Path(args[0]));  
		FileOutputFormat.setOutputPath(job, new Path(args[1]));  
  
		boolean result = job.waitForCompletion(true);  
  
		System.exit(result ? 0 : 1);  
	}  
}
</code></pre>
<p>2）Mapper保持不变</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
  
public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{  
  
	Text k = new Text();  
	IntWritable v = new IntWritable(1);  
  
	@Override  
	protected void map(LongWritable key, Text value, Context context)throws IOException, InterruptedException {  
  
		// 1 获取一行  
		String line = value.toString();  
  
		// 2 切割  
		String[] words = line.split(" ");  
  
		// 3 循环写出  
		for(String word:words){  
			k.set(word);  
			context.write(k, v);  
		}  
	}  
}
</code></pre>
<p>3）Reducer保持不变</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
  
public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{  
  
	IntWritable v = new IntWritable();  
  
	@Override  
	protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,  
			Context context) throws IOException, InterruptedException {  
		  
		int sum = 0;  
  
		// 1 汇总  
		for(IntWritable value:values){  
			sum += value.get();  
		}  
		  
         v.set(sum);  
  
         // 2 输出  
		context.write(key, v);  
	}  
}
</code></pre>
<h3 id="452-reduce输出端采用压缩"><a class="header" href="#452-reduce输出端采用压缩">4.5.2 Reduce输出端采用压缩</a></h3>
<p>基于WordCount案例处理。</p>
<p>1）修改驱动，开启并设置reduce端输出压缩</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.io.compress.BZip2Codec;  
import org.apache.hadoop.io.compress.DefaultCodec;  
import org.apache.hadoop.io.compress.GzipCodec;  
import org.apache.hadoop.io.compress.Lz4Codec;  
import org.apache.hadoop.io.compress.SnappyCodec;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
public class WordCountDriver {  
  
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {  
		  
		Configuration conf = new Configuration();  
		  
		Job job = Job.getInstance(conf);  
		  
		job.setJarByClass(WordCountDriver.class);  
		  
		job.setMapperClass(WordCountMapper.class);  
		job.setReducerClass(WordCountReducer.class);  
		  
		job.setMapOutputKeyClass(Text.class);  
		job.setMapOutputValueClass(IntWritable.class);  
		  
		job.setOutputKeyClass(Text.class);  
		job.setOutputValueClass(IntWritable.class);  
		  
		FileInputFormat.setInputPaths(job, new Path(args[0]));  
		FileOutputFormat.setOutputPath(job, new Path(args[1]));  
		  
		// 开启reduce端输出压缩  
		FileOutputFormat.setCompressOutput(job, true);  
  
		// 设置压缩的方式  
	    FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);   
//	    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);   
//	    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);   
	      
		boolean result = job.waitForCompletion(true);  
		  
		System.exit(result?0:1);  
	}  
}
</code></pre>
<p>2）Mapper保持不变</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Mapper;  
  
public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{  
  
	Text k = new Text();  
	IntWritable v = new IntWritable(1);  
  
	@Override  
	protected void map(LongWritable key, Text value, Context context)throws IOException, InterruptedException {  
  
		// 1 获取一行  
		String line = value.toString();  
  
		// 2 切割  
		String[] words = line.split(" ");  
  
		// 3 循环写出  
		for(String word:words){  
			k.set(word);  
			context.write(k, v);  
		}  
	}  
}
</code></pre>
<p>3）Reducer保持不变</p>
<pre><code class="language-java">package com.atguigu.mapreduce.compress;  
import java.io.IOException;  
import org.apache.hadoop.io.IntWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Reducer;  
  
public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{  
  
	IntWritable v = new IntWritable();  
  
	@Override  
	protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,  
			Context context) throws IOException, InterruptedException {  
		  
		int sum = 0;  
  
		// 1 汇总  
		for(IntWritable value:values){  
			sum += value.get();  
		}  
		  
         v.set(sum);  
  
         // 2 输出  
		context.write(key, v);  
	}  
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="spark概述"><a class="header" href="#spark概述">Spark概述</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="spark核心概念"><a class="header" href="#spark核心概念">Spark核心概念</a></h1>
<ul>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E5%BA%8F%E5%88%97%E5%8C%96">序列化</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#kryo%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%86%E6%9E%B6">Kryo序列化框架</a>
<ul>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E4%BD%BF%E7%94%A8kyro">使用Kyro</a></li>
</ul>
</li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E4%BE%9D%E8%B5%96">依赖</a>
<ul>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E5%AE%BD%E4%BE%9D%E8%B5%96%E5%92%8C%E7%AA%84%E4%BE%9D%E8%B5%96">宽依赖和窄依赖</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E4%BD%9C%E4%B8%9A%E9%98%B6%E6%AE%B5%E5%92%8C%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%85%B3%E7%B3%BB">作业、阶段和任务的关系</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E4%BB%BB%E5%8A%A1%E6%95%B0%E9%87%8F%E5%88%86%E5%8C%BA%E6%95%B0%E9%87%8F">任务数量（分区数量）</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E6%8C%81%E4%B9%85%E5%8C%96%E5%92%8C%E5%BA%8F%E5%88%97%E5%8C%96">持久化和序列化</a>
<ul>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#cache%E9%87%8D%E5%A4%8D%E4%BD%BF%E7%94%A8rdd%E6%97%B6%E4%BD%BF%E7%94%A8">Cache（重复使用RDD时使用）</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#persistfile%E8%90%BD%E7%9B%98">Persist（File，落盘）</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#checkpoint%E5%A4%9A%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%85%B1%E4%BA%AB%E6%95%B0%E6%8D%AE">checkpoint（多个进程共享数据）</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#shuffle%E7%AE%97%E5%AD%90%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96">shuffle算子的持久化</a></li>
</ul>
</li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E5%88%86%E5%8C%BA%E5%99%A8">分区器</a>
<ul>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E5%99%A8">自定义分区器</a></li>
</ul>
</li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">注意事项</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#rdd%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7%E4%B8%A4%E4%B8%AA%E4%BE%8B%E5%AD%90%E5%92%8C%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F">RDD的局限性（两个例子）和广播变量</a>
<ul>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8collect%E5%92%8Cforeach">为什么同时使用collect()和forEach()？</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F">广播变量</a></li>
<li><a href="Spark/Spark%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5.html#%E5%A4%84%E7%90%86json%E6%95%B0%E6%8D%AE">处理JSON数据</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="序列化-1"><a class="header" href="#序列化-1">序列化</a></h1>
<ul>
<li>Kryo——速度快，序列化后体积小；缺点是跨语言支持较复杂</li>
<li>Protostuff——速度快，基于protobuf；缺点是需静态编译</li>
<li>Protostuff-Runtime,无需静态编译，但序列化前需预先传入schema;缺点是不支持无默认构造函数的类，反序列化时需用户自己初始化序列化后的对象，其只负责将该对象进行赋值</li>
<li>Java——使用方便，可序列化所有类；缺点是速度慢，占空间</li>
</ul>
<p>具体的对比可以参考这个基线图：</p>
<p>Results - JVM Serializer Benchmarks</p>
<p>效率对比直观图：</p>
<p>An Introduction and Comparison of Several Common Java Serialization Frameworks - Alibaba Cloud Community</p>
<p>首选序列化：Kryo、Protostuff</p>
<h1 id="kryo序列化框架"><a class="header" href="#kryo序列化框架">Kryo序列化框架</a></h1>
<pre><code class="language-java">package com.zzw;

import com.esotericsoftware.kryo.Kryo;
import com.esotericsoftware.kryo.io.Input;
import com.esotericsoftware.kryo.io.Output;
import com.esotericsoftware.kryo.serializers.BeanSerializer;

import java.io.*;

public class KryoTest {
    public static void main(String[] args) {
        User user = new User("Tom", 20);
        //javaSerial(user, "D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user.dat");
        //kryoSerial(user, "D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user1.kryo");
        User user1 = kryoDeSerial(User.class, "D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user1.kryo");
        System.out.println(user1.getName() + " " + user1.getAge());

    }
    public static void javaSerial(Serializable s, String path) {
        try {
            ObjectOutputStream oos = new ObjectOutputStream(new BufferedOutputStream(new FileOutputStream(path)));
            oos.writeObject(s);
            oos.flush();
            oos.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    public static &lt;T&gt; T kryoDeSerial(Class&lt;T&gt; c, String path) {
        try {
            Kryo kryo = new Kryo();
            kryo.register(c, new BeanSerializer(kryo, c));
            Input input = new Input(new BufferedInputStream(new FileInputStream(path)));
            T obj = kryo.readObject(input, c);
            input.close();
            return obj;
        } catch (Exception e) {
            e.printStackTrace();
        }
        return null;
    }
    public static void kryoSerial(Serializable s, String path) {
        try {
            Kryo kryo = new Kryo();
            kryo.register(s.getClass(), new BeanSerializer(kryo, s.getClass()));
            Output output = new Output(new BufferedOutputStream(new FileOutputStream(path)));
            kryo.writeObject(output, s);
            output.flush();
            output.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
class User implements Serializable {
    private String name;
    private int age;

    public User(String name, int age) {
        this.name = name;
        this.age = age;
    }
    public User() {
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getAge() {
        return age;
    }

    public void setAge(int age) {
        this.age = age;
    }
}
</code></pre>
<h2 id="使用kyro"><a class="header" href="#使用kyro">使用Kyro</a></h2>
<pre><code class="language-java">package com.atguigu.serializable;

import com.atguigu.bean.User;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;

import java.util.Arrays;

public class Test02_Kryo {
    public static void main(String[] args) throws ClassNotFoundException {

        // 1.创建配置对象
        SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("sparkCore")
                // 替换默认的序列化机制
                .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
                // 注册需要使用kryo序列化的自定义类
                .registerKryoClasses(new Class[]{Class.forName("com.atguigu.bean.User")});

        // 2. 创建sparkContext
        JavaSparkContext sc = new JavaSparkContext(conf);

        // 3. 编写代码
        User zhangsan = new User("zhangsan", 13);
        User lisi = new User("lisi", 13);

        JavaRDD&lt;User&gt; userJavaRDD = sc.parallelize(Arrays.asList(zhangsan, lisi), 2);

        JavaRDD&lt;User&gt; mapRDD = userJavaRDD.map(new Function&lt;User, User&gt;() {
            @Override
            public User call(User v1) throws Exception {
                return new User(v1.getName(), v1.getAge() + 1);
            }
        });

        mapRDD. collect().forEach(System.out::println);

        // 4. 关闭sc
        sc.stop();
    }
}

</code></pre>
<h1 id="依赖"><a class="header" href="#依赖">依赖</a></h1>
<p>RDD依赖：Spark中相邻的两个RDD之间存在的依赖关系</p>
<p>连续的依赖关系被称为血缘关系</p>
<p>Spark中的每一个RDD都保存了依赖关系和血缘关系，方便出问题时可以溯源或重试</p>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.dep;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import java.util.Arrays;

public class Spark01_WordCount {
    public static void main(String[] args) throws InterruptedException {
        // 1.创建配置对象
        SparkConf conf = new SparkConf();
        conf.setMaster("local[2]");
        conf.setAppName("sparkCore");

        // 2. 创建sparkContext
        JavaSparkContext sc = new JavaSparkContext(conf);

        // 3. 编写代码

        JavaRDD&lt;String&gt; lineRDD = sc.textFile("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\word.txt");
        System.out.println(lineRDD.toDebugString());
        System.out.println("*************");
        JavaRDD&lt;String&gt; wordRDD = lineRDD.flatMap(line -&gt; Arrays.asList(line.split(" ")).iterator());
        System.out.println(wordRDD.toDebugString());
        System.out.println("*************");
        JavaPairRDD&lt;String, Iterable&lt;String&gt;&gt; wordGroupRDD = wordRDD.groupBy(word -&gt; word);
        System.out.println(wordGroupRDD.toDebugString());
        System.out.println("*************");
        JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordGroupRDD.mapValues(iter -&gt; {
            int count = 0;
            for (String s : iter) {
                count++;
            }
            return count;
        });
        System.out.println(wordCountRDD.toDebugString());
        System.out.println("*************");
        wordCountRDD.collect().forEach(System.out::println);

        // 4. 关闭sc
        sc.stop();

    }
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>(2) D:\Documents\items\BigData\BigDataCode\spark\src\main\resources\data\word.txt MapPartitionsRDD[1] at textFile at Spark01_WordCount.java:22 []
 |  D:\Documents\items\BigData\BigDataCode\spark\src\main\resources\data\word.txt HadoopRDD[0] at textFile at Spark01_WordCount.java:22 []
*************
(2) MapPartitionsRDD[2] at flatMap at Spark01_WordCount.java:25 []
 |  D:\Documents\items\BigData\BigDataCode\spark\src\main\resources\data\word.txt MapPartitionsRDD[1] at textFile at Spark01_WordCount.java:22 []
 |  D:\Documents\items\BigData\BigDataCode\spark\src\main\resources\data\word.txt HadoopRDD[0] at textFile at Spark01_WordCount.java:22 []
*************
(2) MapPartitionsRDD[5] at groupBy at Spark01_WordCount.java:28 []
 |  ShuffledRDD[4] at groupBy at Spark01_WordCount.java:28 []
 +-(2) MapPartitionsRDD[3] at groupBy at Spark01_WordCount.java:28 []
    |  MapPartitionsRDD[2] at flatMap at Spark01_WordCount.java:25 []
    |  D:\Documents\items\BigData\BigDataCode\spark\src\main\resources\data\word.txt MapPartitionsRDD[1] at textFile at Spark01_WordCount.java:22 []
    |  D:\Documents\items\BigData\BigDataCode\spark\src\main\resources\data\word.txt HadoopRDD[0] at textFile at Spark01_WordCount.java:22 []
*************
(2) MapPartitionsRDD[6] at mapValues at Spark01_WordCount.java:31 []
 |  MapPartitionsRDD[5] at groupBy at Spark01_WordCount.java:28 []
 |  ShuffledRDD[4] at groupBy at Spark01_WordCount.java:28 []
 +-(2) MapPartitionsRDD[3] at groupBy at Spark01_WordCount.java:28 []
    |  MapPartitionsRDD[2] at flatMap at Spark01_WordCount.java:25 []
    |  D:\Documents\items\BigData\BigDataCode\spark\src\main\resources\data\word.txt MapPartitionsRDD[1] at textFile at Spark01_WordCount.java:22 []
    |  D:\Documents\items\BigData\BigDataCode\spark\src\main\resources\data\word.txt HadoopRDD[0] at textFile at Spark01_WordCount.java:22 []
*************
(Flink,1)
(Zookeeper,1)
(Kafka,3)
(Cassandra,1)
(Spark,2)
(Flume,2)
(Redis,1)
(HBase,1)
(Hadoop,2)
</code></pre>
<pre><code class="language-java">List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Arrays.asList(t1, t2, t3, t4);  
JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(list);  
  
System.out.println(rdd.rdd().dependencies());
</code></pre>
<h2 id="宽依赖和窄依赖"><a class="header" href="#宽依赖和窄依赖">宽依赖和窄依赖</a></h2>
<p>在Driver端准备计算逻辑（RDD的关系）-&gt;由Spark对关系进行判断决定人物的数量和关系-&gt;计算逻辑是在Executor端执行</p>
<p>RDD中的依赖关系本质上并不是RDD对象的关系，而是RDD对象中分区数据的关系</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250105145756.png" alt="" /></p>
<p>OneToOneDependency 一对一（窄依赖）：计算中上游RDD的一个分区数据被下游RDD的一个分区所独享</p>
<p>不是窄依赖的就是宽依赖</p>
<p>ShuffleDependency （宽依赖）：计算中上游RDD的一个分区数据被下游RDD的多个分区所共享</p>
<p>宽依赖会将分区数据打乱重新组合，所以底层实现存在Shuffle操作</p>
<h2 id="作业阶段和任务的关系"><a class="header" href="#作业阶段和任务的关系">作业、阶段和任务的关系</a></h2>
<p>作业（Job）：行动算子执行时，会触发作业的执行（Active Job）</p>
<p>阶段（Stage）：一个Job中RDD的计算流程，默认就一个完整的阶段，但是如果计算流程中存在shuffle，那么流程就会分为二</p>
<p>分开的每一段就称之为Stage（阶段），前一个阶段不执行完，后一个阶段不允许执行</p>
<p>阶段的数量=1+shuffle依赖的数量</p>
<p>任务（Task）：每个Executor执行的计算单元</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250105154242.png" alt="" /></p>
<p><code>0 until numPartitions</code> 左闭右开，每个分区对应一个<code>new Task</code></p>
<p>任务的数量其实就是每个阶段最后一个RDD分区的数量之和</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250105155313.png" alt="" /></p>
<h2 id="任务数量分区数量"><a class="header" href="#任务数量分区数量">任务数量（分区数量）</a></h2>
<p>怎么设置任务数量？简单来说，任务数量最好等于资源核数</p>
<p>但这样容易出问题——计算资源空置</p>
<p>移动数据不如移动计算</p>
<p>一般推荐分区数量为资源核数的2~3倍</p>
<h2 id="持久化和序列化"><a class="header" href="#持久化和序列化">持久化和序列化</a></h2>
<p>持久化：长时间保存对象</p>
<p>序列化：内存中对象=&gt;byte序列（byte数组）</p>
<h3 id="cache重复使用rdd时使用"><a class="header" href="#cache重复使用rdd时使用">Cache（重复使用RDD时使用）</a></h3>
<p>代码流程设计存在问题：数据重复、计算重复</p>
<p>改变流向没用，因为数据不能倒流转</p>
<p>RDD不保存数据，如果重复使用同一个RDD，那么数据就会从头执行，导致数据重复、计算重复</p>
<p>解决措施：cache</p>
<p>对应代码为<code>mapRDD.cache();</code></p>
<p>未使用cache时</p>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.dep;  
  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.ArrayList;  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Dep {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Tuple2&lt;String, Integer&gt;&gt; nums = new ArrayList&lt;&gt;();  
        nums.add(new Tuple2&lt;&gt;("a", 1));  
        nums.add(new Tuple2&lt;&gt;("a", 2));  
        nums.add(new Tuple2&lt;&gt;("a", 3));  
        nums.add(new Tuple2&lt;&gt;("a", 4));  
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(nums, 2);  
  
        JavaPairRDD&lt;String, Integer&gt; mapRDD = rdd.mapToPair(kv -&gt; {  
            System.out.println("*************");  
            return kv;  
        });  
  
        //mapRDD.cache();  
  
        JavaPairRDD&lt;String, Integer&gt; wordCountRDD = mapRDD.reduceByKey(Integer::sum);  
  
        wordCountRDD.collect();  
        System.out.println("计算1结束");  
        System.out.println("############################");  
  
        JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupRDD = mapRDD.groupByKey();  
        groupRDD.collect();  
        System.out.println("计算2结束");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>*************
*************
*************
*************
计算1结束
############################
*************
*************
*************
*************
计算2结束
</code></pre>
<p>使用cache后</p>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.dep;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

public class Spark02_Dep {
    public static void main(String[] args) throws InterruptedException {
        // 1.创建配置对象
        SparkConf conf = new SparkConf();
        conf.setMaster("local[2]");
        conf.setAppName("sparkCore");

        // 2. 创建sparkContext
        JavaSparkContext sc = new JavaSparkContext(conf);

        // 3. 编写代码
        List&lt;Tuple2&lt;String, Integer&gt;&gt; nums = new ArrayList&lt;&gt;();
        nums.add(new Tuple2&lt;&gt;("a", 1));
        nums.add(new Tuple2&lt;&gt;("a", 2));
        nums.add(new Tuple2&lt;&gt;("a", 3));
        nums.add(new Tuple2&lt;&gt;("a", 4));
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(nums, 2);

        JavaPairRDD&lt;String, Integer&gt; mapRDD = rdd.mapToPair(kv -&gt; {
            System.out.println("*************");
            return kv;
        });

        mapRDD.cache();

        JavaPairRDD&lt;String, Integer&gt; wordCountRDD = mapRDD.reduceByKey(Integer::sum);

        wordCountRDD.collect();
        System.out.println("计算1结束");
        System.out.println("############################");

        JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupRDD = mapRDD.groupByKey();
        groupRDD.collect();
        System.out.println("计算2结束");

        // 4. 关闭sc
        sc.stop();

    }
}
</code></pre>
<pre><code>*************
*************
*************
*************
计算1结束
############################
计算2结束
</code></pre>
<p>cache其实是把数据保存到内存中，所以会受到内存大小的影响</p>
<h3 id="persistfile落盘"><a class="header" href="#persistfile落盘">Persist（File，落盘）</a></h3>
<p>使用<code>mapRDD.persist()</code>，传参<code>StorageLevel</code></p>
<p>例如<code>mapRDD.persist(StorageLevel.MEMORY_ONLY());</code>和<code>mapRDD.cache();</code>等同</p>
<p>使用<code>mapRDD.unpersist();</code>释放缓存</p>
<pre><code class="language-java">@DeveloperApi  
public static StorageLevel fromString(final String s) {  
    return StorageLevel$.MODULE$.fromString(s);  
}  
  
public static StorageLevel OFF_HEAP() {  
    return StorageLevel$.MODULE$.OFF_HEAP();  
}  
  
public static StorageLevel MEMORY_AND_DISK_SER_2() {  
    return StorageLevel$.MODULE$.MEMORY_AND_DISK_SER_2();  
}  
  
public static StorageLevel MEMORY_AND_DISK_SER() {  
    return StorageLevel$.MODULE$.MEMORY_AND_DISK_SER();  
}  
  
public static StorageLevel MEMORY_AND_DISK_2() {  
    return StorageLevel$.MODULE$.MEMORY_AND_DISK_2();  
}  
  
public static StorageLevel MEMORY_AND_DISK() {  
    return StorageLevel$.MODULE$.MEMORY_AND_DISK();  
}  
  
public static StorageLevel MEMORY_ONLY_SER_2() {  
    return StorageLevel$.MODULE$.MEMORY_ONLY_SER_2();  
}  
  
public static StorageLevel MEMORY_ONLY_SER() {  
    return StorageLevel$.MODULE$.MEMORY_ONLY_SER();  
}  
  
public static StorageLevel MEMORY_ONLY_2() {  
    return StorageLevel$.MODULE$.MEMORY_ONLY_2();  
}  
  
public static StorageLevel MEMORY_ONLY() {  
    return StorageLevel$.MODULE$.MEMORY_ONLY();  
}  
  
public static StorageLevel DISK_ONLY_3() {  
    return StorageLevel$.MODULE$.DISK_ONLY_3();  
}  
  
public static StorageLevel DISK_ONLY_2() {  
    return StorageLevel$.MODULE$.DISK_ONLY_2();  
}  
  
public static StorageLevel DISK_ONLY() {  
    return StorageLevel$.MODULE$.DISK_ONLY();  
}  
  
public static StorageLevel NONE() {  
    return StorageLevel$.MODULE$.NONE();  
}
</code></pre>
<p>SER表示将数据序列化之后再保存，2表示备份份数</p>
<h3 id="checkpoint多个进程共享数据"><a class="header" href="#checkpoint多个进程共享数据">checkpoint（多个进程共享数据）</a></h3>
<p>Spark的持久化操作只对当前应用程序有效，其他应用程序无法访问</p>
<p>使用HDFS生成中间件（检查点）</p>
<p>执行<code>mapRDD.checkpoint();</code></p>
<p>需要设定检查点目录，推荐使用HDFS共享文件系统，也可以使用本地文件路径</p>
<p>执行<code>sc.setCheckpointDir();</code></p>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.dep;  
  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import org.apache.spark.storage.StorageLevel;  
import scala.Tuple2;  
  
import java.util.ArrayList;  
import java.util.List;  
  
public class Spark03_Dep {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
        sc.setCheckpointDir("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\checkpoint");  
  
        // 3. 编写代码  
        List&lt;Tuple2&lt;String, Integer&gt;&gt; nums = new ArrayList&lt;&gt;();  
        nums.add(new Tuple2&lt;&gt;("a", 1));  
        nums.add(new Tuple2&lt;&gt;("a", 2));  
        nums.add(new Tuple2&lt;&gt;("a", 3));  
        nums.add(new Tuple2&lt;&gt;("a", 4));  
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(nums, 2);  
  
        JavaPairRDD&lt;String, Integer&gt; mapRDD = rdd.mapToPair(kv -&gt; {  
            System.out.println("*************");  
            return kv;  
        });  
  
        //mapRDD.cache();  
        //mapRDD.persist(StorageLevel.MEMORY_ONLY());        mapRDD.checkpoint();  
  
        JavaPairRDD&lt;String, Integer&gt; wordCountRDD = mapRDD.reduceByKey(Integer::sum);  
  
        wordCountRDD.collect();  
        System.out.println("计算1结束");  
        System.out.println("############################");  
  
        JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupRDD = mapRDD.groupByKey();  
        groupRDD.collect();  
        System.out.println("计算2结束");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>*************
*************
*************
*************
*************
*************
*************
*************
计算1结束
############################
计算2结束
</code></pre>
<p>检查点操作目的是希望RDD结果长时间的保存，所以需要保证数据的安全，会从头再跑一遍，性能比较低。</p>
<p>上面共有八行星号</p>
<p>为了提高效率，Spark推荐再检查点之前，执行cache方法，将数据缓存。</p>
<pre><code class="language-java">mapRDD.cache();  
mapRDD.checkpoint();
</code></pre>
<pre><code>*************
*************
*************
*************
计算1结束
############################
计算2结束
</code></pre>
<h3 id="shuffle算子的持久化"><a class="header" href="#shuffle算子的持久化">shuffle算子的持久化</a></h3>
<p>cache方法会在血缘关系中增加依赖关系</p>
<p>checkpoint方法切断（改变）血缘关系</p>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.dep;  
  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import org.apache.spark.storage.StorageLevel;  
import scala.Tuple2;  
  
import java.util.ArrayList;  
import java.util.List;  
  
public class Spark03_Dep {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
        sc.setCheckpointDir("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\checkpoint");  
  
        // 3. 编写代码  
        List&lt;Tuple2&lt;String, Integer&gt;&gt; nums = new ArrayList&lt;&gt;();  
        nums.add(new Tuple2&lt;&gt;("a", 1));  
        nums.add(new Tuple2&lt;&gt;("a", 2));  
        nums.add(new Tuple2&lt;&gt;("a", 3));  
        nums.add(new Tuple2&lt;&gt;("a", 4));  
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(nums, 2);  
  
        JavaPairRDD&lt;String, Integer&gt; mapRDD = rdd.mapToPair(kv -&gt; {  
            return kv;  
        });  
  
        mapRDD.cache();  
        //mapRDD.persist(StorageLevel.MEMORY_ONLY());  
        //mapRDD.checkpoint();  
        JavaPairRDD&lt;String, Integer&gt; wordCountRDD = mapRDD.reduceByKey(Integer::sum);  
        System.out.println(wordCountRDD.toDebugString());  
        System.out.println("############################");  
  
        wordCountRDD.collect();  
        System.out.println(wordCountRDD.toDebugString());  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>(2) ShuffledRDD[2] at reduceByKey at Spark03_Dep.java:40 []
 +-(2) MapPartitionsRDD[1] at mapToPair at Spark03_Dep.java:32 []
    |  ParallelCollectionRDD[0] at parallelize at Spark03_Dep.java:30 []
############################
(2) ShuffledRDD[2] at reduceByKey at Spark03_Dep.java:40 []
 +-(2) MapPartitionsRDD[1] at mapToPair at Spark03_Dep.java:32 []
    |      CachedPartitions: 2; MemorySize: 304.0 B; DiskSize: 0.0 B
    |  ParallelCollectionRDD[0] at parallelize at Spark03_Dep.java:30 []

</code></pre>
<pre><code>(2) ShuffledRDD[2] at reduceByKey at Spark03_Dep.java:40 []
 +-(2) MapPartitionsRDD[1] at mapToPair at Spark03_Dep.java:32 []
    |  ParallelCollectionRDD[0] at parallelize at Spark03_Dep.java:30 []
############################
(2) ShuffledRDD[2] at reduceByKey at Spark03_Dep.java:40 []
 +-(2) MapPartitionsRDD[1] at mapToPair at Spark03_Dep.java:32 []
    |  ReliableCheckpointRDD[3] at collect at Spark03_Dep.java:44 []
</code></pre>
<p>cache可能丢失，而checkpoint共享</p>
<p>所有的shuffle操作性能是非常低，所以Spark为了提升shuffle算子的性能，每个shuffle算子都是自动含有缓存如果重复调用相同规则的shuffle算子，那么第二次shuffle算子不会有shuffle操作.</p>
<h2 id="分区器"><a class="header" href="#分区器">分区器</a></h2>
<p>reduceBySum 先分区内求和，然后分区间求和</p>
<p>数据分区的规则</p>
<p>计算后数据所在的分区是通过Spark的内部计算（分区）完成，尽可能让数据均衡（散列）一些，但是不是平均分。</p>
<p>reduceByKey方法需要传递两个参数</p>
<p>1.第一个参数表示数据分区的规则，参数可以不用传递，使用时，会使用默认值（默认分区规则Hashpartitioner）</p>
<p>2.第二个参数表示数据聚合的逻辑</p>
<p>HashPartitioner中有个方法getPartition</p>
<p>getPartition需要传递一个参数Key，然后方法需要返回一个值，表示分区编号，分区编号从0开始。</p>
<pre><code class="language-scala">val rawMod = x % mod;
</code></pre>
<p>逻辑：<code>分区编号&lt;=Key.hashCode%partNum（哈希取余）</code></p>
<h3 id="自定义分区器"><a class="header" href="#自定义分区器">自定义分区器</a></h3>
<p>自定义分区器流程：</p>
<ol>
<li>创建自定义类</li>
<li>继承抽象类Partitioner</li>
<li>重写方法（2+2)Partitioner(2)+0bject(2)</li>
<li>构建对象，在算子中使用</li>
</ol>
<pre><code class="language-java">import org.apache.spark.Partitioner;  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.ArrayList;  
import java.util.List;  
  
public class Spark04_Partitioner {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Tuple2&lt;String, Integer&gt;&gt; nums = new ArrayList&lt;&gt;();  
        nums.add(new Tuple2&lt;&gt;("nba", 1));  
        nums.add(new Tuple2&lt;&gt;("wnba", 2));  
        nums.add(new Tuple2&lt;&gt;("nba", 3));  
        nums.add(new Tuple2&lt;&gt;("cba", 4));  
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(nums, 2);  
  
        JavaPairRDD&lt;String, Integer&gt; mapRDD = rdd.mapToPair(kv -&gt; {  
            return kv;  
        });  
  
        JavaPairRDD&lt;String, Integer&gt; wordCountRDD = mapRDD.reduceByKey(new MyPartitioner(), Integer::sum);  
  
        wordCountRDD.saveAsTextFile("output");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}  
  
class MyPartitioner extends Partitioner {  
    @Override  
    public int numPartitions() {  
        return 3;  
    }  
  
    @Override  
    public int getPartition(Object key) {  
        if(key.equals("nba")){  
            return 0;  
        }else if(key.equals("wnba")){  
            return 1;  
        }else{  
            return 2;  
        }  
    }  
}
</code></pre>
<h2 id="注意事项"><a class="header" href="#注意事项">注意事项</a></h2>
<p>没有重写分区器方法时，两次连续reduceByKey()中的shuffle不起作用</p>
<p>则Stage数量=1+1=2</p>
<p>Task数量= 2 + 2 = 4</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250107152628.png" alt="" /></p>
<pre><code class="language-java">import org.apache.spark.Partitioner;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.List;

public class Spark04_Partitioner {
    public static void main(String[] args) throws InterruptedException {
        // 1.创建配置对象
        SparkConf conf = new SparkConf();
        conf.setMaster("local[2]");
        conf.setAppName("sparkCore");

        // 2. 创建sparkContext
        JavaSparkContext sc = new JavaSparkContext(conf);

        // 3. 编写代码
        List&lt;Tuple2&lt;String, Integer&gt;&gt; nums = new ArrayList&lt;&gt;();
        nums.add(new Tuple2&lt;&gt;("nba", 1));
        nums.add(new Tuple2&lt;&gt;("wnba", 2));
        nums.add(new Tuple2&lt;&gt;("nba", 3));
        nums.add(new Tuple2&lt;&gt;("cba", 4));
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(nums, 2);

        JavaPairRDD&lt;String, Integer&gt; mapRDD = rdd.mapToPair(kv -&gt; {
            return kv;
        });

        JavaPairRDD&lt;String, Integer&gt; reduceRDD = mapRDD.reduceByKey(Integer::sum);
        JavaPairRDD&lt;String, Integer&gt; reduceRDD1 = reduceRDD.reduceByKey(Integer::sum);

        reduceRDD1.collect();

        System.out.println("执行结束");

        Thread.sleep(1000000L);

        // 4. 关闭sc
        sc.stop();

    }
}

class MyPartitioner extends Partitioner {
    private Integer numOfPartitioner;

    public MyPartitioner() {}
    public MyPartitioner(Integer numOfPartitioner) {
        this.numOfPartitioner = numOfPartitioner;
    }
    @Override
    public int numPartitions() {
        return this.numOfPartitioner;
    }

    @Override
    public int getPartition(Object key) {
        if(key.equals("nba")){
            return 0;
        }else if(key.equals("wnba")){
            return 1;
        }else{
            return 2;
        }
    }
}
</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250107152357.png" alt="" /></p>
<p>重写之后Stage数量=2+1=3</p>
<p>Task数量=2 + 3  + 3=8</p>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.dep;

import org.apache.spark.Partitioner;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.List;

public class Spark04_Partitioner {
    public static void main(String[] args) throws InterruptedException {
        // 1.创建配置对象
        SparkConf conf = new SparkConf();
        conf.setMaster("local[2]");
        conf.setAppName("sparkCore");

        // 2. 创建sparkContext
        JavaSparkContext sc = new JavaSparkContext(conf);

        // 3. 编写代码
        List&lt;Tuple2&lt;String, Integer&gt;&gt; nums = new ArrayList&lt;&gt;();
        nums.add(new Tuple2&lt;&gt;("nba", 1));
        nums.add(new Tuple2&lt;&gt;("wnba", 2));
        nums.add(new Tuple2&lt;&gt;("nba", 3));
        nums.add(new Tuple2&lt;&gt;("cba", 4));
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(nums, 2);

        JavaPairRDD&lt;String, Integer&gt; mapRDD = rdd.mapToPair(kv -&gt; {
            return kv;
        });

        JavaPairRDD&lt;String, Integer&gt; reduceRDD = mapRDD.reduceByKey(new MyPartitioner(3), Integer::sum);
        JavaPairRDD&lt;String, Integer&gt; reduceRDD1 = reduceRDD.reduceByKey(new MyPartitioner(3), Integer::sum);

        reduceRDD1.collect();

        System.out.println("执行结束");

        Thread.sleep(1000000L);

        // 4. 关闭sc
        sc.stop();

    }
}

class MyPartitioner extends Partitioner {
    private Integer numOfPartitioner;

    public MyPartitioner() {}
    public MyPartitioner(Integer numOfPartitioner) {
        this.numOfPartitioner = numOfPartitioner;
    }
    @Override
    public int numPartitions() {
        return this.numOfPartitioner;
    }

    @Override
    public int getPartition(Object key) {
        if(key.equals("nba")){
            return 0;
        }else if(key.equals("wnba")){
            return 1;
        }else{
            return 2;
        }
    }
}
</code></pre>
<p>解释：</p>
<p><code>PairRDDFunctions.scala</code></p>
<pre><code class="language-scala">if(self.partitioner == Some(partitioner)){
	self.mapPartitions( iter =&gt; {
	val context = TaskContext.get()
	new InterruptibleIterator(context，aggregator.combineValuesByKey(iter，context))}，preservesPartitioning = true)
}else{
	new ShuffledRDD[K，V，C](self，partitioner)
	.setSerializer(serializer)
	.setAggregator(aggregator)
	.setMapSideCombine(mapSideCombine)
	}
}
</code></pre>
<p>判断分区器是否相同<code>self.partitioner == Some(partitioner)</code></p>
<p>要使两次连续reduceByKey()不影响任务数量则需要重写hashcode()和equals()方法</p>
<pre><code class="language-java">import org.apache.spark.Partitioner;  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.ArrayList;  
import java.util.List;  
  
public class Spark04_Partitioner {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Tuple2&lt;String, Integer&gt;&gt; nums = new ArrayList&lt;&gt;();  
        nums.add(new Tuple2&lt;&gt;("nba", 1));  
        nums.add(new Tuple2&lt;&gt;("wnba", 2));  
        nums.add(new Tuple2&lt;&gt;("nba", 3));  
        nums.add(new Tuple2&lt;&gt;("cba", 4));  
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(nums, 2);  
  
        JavaPairRDD&lt;String, Integer&gt; mapRDD = rdd.mapToPair(kv -&gt; {  
            return kv;  
        });  
  
        JavaPairRDD&lt;String, Integer&gt; reduceRDD = mapRDD.reduceByKey(new MyPartitioner(3), Integer::sum);  
        JavaPairRDD&lt;String, Integer&gt; reduceRDD1 = reduceRDD.reduceByKey(new MyPartitioner(3), Integer::sum);  
  
        reduceRDD1.collect();  
  
        System.out.println("执行结束");  
  
        Thread.sleep(1000000L);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}  
  
class MyPartitioner extends Partitioner {  
    private Integer numOfPartitioner;  
  
    public MyPartitioner() {}  
    public MyPartitioner(Integer numOfPartitioner) {  
        this.numOfPartitioner = numOfPartitioner;  
    }  
    @Override  
    public int numPartitions() {  
        return this.numOfPartitioner;  
    }  
  
    @Override  
    public int getPartition(Object key) {  
        if(key.equals("nba")){  
            return 0;  
        }else if(key.equals("wnba")){  
            return 1;  
        }else{  
            return 2;  
        }  
    }  
  
    @Override  
    public int hashCode() {  
        return this.numOfPartitioner.hashCode();  
    }  
  
    @Override  
    public boolean equals(Object obj) {  
        if(obj instanceof MyPartitioner){  
            return ((MyPartitioner)obj).numOfPartitioner.equals(this.numOfPartitioner);  
        }else{  
            return false;  
        }  
    }  
}
</code></pre>
<h2 id="rdd的局限性两个例子和广播变量"><a class="header" href="#rdd的局限性两个例子和广播变量">RDD的局限性（两个例子）和广播变量</a></h2>
<h3 id="为什么同时使用collect和foreach"><a class="header" href="#为什么同时使用collect和foreach">为什么同时使用collect()和forEach()？</a></h3>
<p>RDD在foreach循环时，逻辑代码和操作全部都是在Executor端完成的，那么结果不会拉取回到Driver端</p>
<p>RDD无法实现数据拉取操作</p>
<p>collect</p>
<p>如果Executor端使用了Driver端数据，那么需要从Driver端将数据拉取到Executor端数据拉取的单位是Task（任务）</p>
<p>如果数据不是以Task为传输单位，而是以Executor为单位进行传输，那么性能会提高</p>
<p>RDD不能以Executor为单位进行数据传输</p>
<h3 id="广播变量"><a class="header" href="#广播变量">广播变量</a></h3>
<p>Spark需要采用特殊的数据模型实现数据传输：广播变量</p>
<p>默认数据传输以Task为单位进行传输，如果想要以Executor为单位传输，那么需要进行包装（封装）</p>
<p><code>final Broadcast&lt;List&lt;String&gt;&gt; broadcast = jsc.broadcast(okList);</code></p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import org.apache.spark.broadcast.Broadcast;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark_Broadcast {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        JavaRDD&lt;String&gt; rdd = sc.parallelize(Arrays.asList("hadoop", "spark", "flink", "hive", "hbase"));  
  
        List&lt;String&gt; list = Arrays.asList("hadoop", "spark", "flink");  
  
        Broadcast&lt;List&lt;String&gt;&gt; broadcast = sc.broadcast(list);  
  
        rdd.filter(x -&gt; broadcast.value().contains(x)).collect().forEach(System.out::println);  
  
        // 注意：如果broadcast的value是一个不可序列化的对象，则会报错：  
        // java.io.NotSerializableException: org.apache.spark.sql.catalyst.expressions.UnsafeRow  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>hadoop
spark
flink
</code></pre>
<h3 id="处理json数据"><a class="header" href="#处理json数据">处理JSON数据</a></h3>
<p>数据格式：JSON</p>
<ol>
<li>每一行就是一个JSON格式的数据，而且表示一个对象，对象内容必须包含在中</li>
<li>对象中的多个属性必须采用逗号隔开</li>
<li>每一个属性，属性名和属性值之间采用冒号隔开</li>
<li>属性名必须采用双引号声明，属性值如果为字符串类型，也需要采用双引号包含</li>
</ol>
<p>重点关注RDD的功能和方法，数据格式不是我们的学习重点。</p>
<p>在特殊场景中，Spark对数据处理的逻辑进行了封装来简化开发。</p>
<p>SparkSQL就是对RDD的封装</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="spark-rdd"><a class="header" href="#spark-rdd">Spark RDD</a></h1>
<ul>
<li><a href="Spark/SparkRDD.html#%E6%A6%82%E8%BF%B0">概述</a></li>
<li><a href="Spark/SparkRDD.html#%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D">详细介绍</a>
<ul>
<li><a href="Spark/SparkRDD.html#1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">1. 基本概念</a></li>
<li><a href="Spark/SparkRDD.html#2-%E5%88%9B%E5%BB%BArdd">2. 创建RDD</a></li>
<li><a href="Spark/SparkRDD.html#3-rdd%E6%93%8D%E4%BD%9C">3. RDD操作</a>
<ul>
<li><a href="Spark/SparkRDD.html#%E8%BD%AC%E6%8D%A2transformations">转换（Transformations）</a></li>
<li><a href="Spark/SparkRDD.html#%E8%A1%8C%E5%8A%A8actions">行动（Actions）</a></li>
</ul>
</li>
<li><a href="Spark/SparkRDD.html#4-%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6">4. 容错机制</a></li>
<li><a href="Spark/SparkRDD.html#5-%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B">5. 计算模型</a></li>
<li><a href="Spark/SparkRDD.html#6-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96">6. 性能优化</a></li>
<li><a href="Spark/SparkRDD.html#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
<li><a href="Spark/SparkRDD.html#rdd-%E7%BC%96%E7%A8%8B">RDD 编程</a>
<ul>
<li><a href="Spark/SparkRDD.html#21rdd%E7%9A%84%E5%88%9B%E5%BB%BA">2.1RDD的创建</a></li>
<li><a href="Spark/SparkRDD.html#211idea%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87">2.1.1IDEA环境准备</a></li>
</ul>
</li>
<li><a href="Spark/SparkRDD.html#rdd%E6%96%B9%E6%B3%95">RDD方法</a>
<ul>
<li><a href="Spark/SparkRDD.html#%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E6%96%B9%E6%B3%95%E7%9A%84%E5%88%86%E7%B1%BB">处理数据方法的分类</a></li>
</ul>
</li>
<li><a href="Spark/SparkRDD.html#spark-on-yarn-%E9%83%A8%E7%BD%B2%E6%90%AD%E5%BB%BA%E8%AF%A6%E7%BB%86%E5%9B%BE%E6%96%87%E6%95%99%E7%A8%8B">Spark on YARN 部署搭建详细图文教程</a>
<ul>
<li><a href="Spark/SparkRDD.html#1-function-%E6%8E%A5%E5%8F%A3%E7%9A%84%E7%94%A8%E6%B3%95">1. Function 接口的用法</a></li>
<li><a href="Spark/SparkRDD.html#2-%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E7%B1%BB%E5%9E%8B">2. 选择合适的类型</a></li>
<li><a href="Spark/SparkRDD.html#3-%E4%BF%AE%E6%AD%A3-map-%E7%9A%84-lambda-%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%BD%BF%E7%94%A8">3. 修正 <code>map</code> 的 lambda 表达式使用</a></li>
<li><a href="Spark/SparkRDD.html#filter">Filter</a></li>
<li><a href="Spark/SparkRDD.html#flatmap">FlatMap</a></li>
<li><a href="Spark/SparkRDD.html#groupby%E6%96%B9%E6%B3%95">groupby方法</a></li>
<li><a href="Spark/SparkRDD.html#distinct">distinct</a></li>
<li><a href="Spark/SparkRDD.html#sortby">sortBy</a></li>
</ul>
</li>
<li><a href="Spark/SparkRDD.html#kv%E7%B1%BB%E5%9E%8B">KV类型</a>
<ul>
<li><a href="Spark/SparkRDD.html#%E5%8D%95%E5%80%BC%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BAkv%E7%B1%BB%E5%9E%8B">单值类型转换为KV类型</a></li>
<li><a href="Spark/SparkRDD.html#groupby%E5%92%8Ckv%E7%B1%BB%E5%9E%8B">groupBy和KV类型</a></li>
<li><a href="Spark/SparkRDD.html#wordcount">WordCount</a></li>
<li><a href="Spark/SparkRDD.html#groupbykey%E6%8C%89key%E5%AF%B9value%E8%BF%9B%E8%A1%8C%E5%88%86%E7%BB%84">GroupByKey（按Key对Value进行分组）</a></li>
<li><a href="Spark/SparkRDD.html#reducebykey%E6%8C%89%E7%85%A7key%E5%AF%B9value%E4%B8%A4%E4%B8%A4%E8%81%9A%E5%90%88">ReduceByKey（按照key对value两两聚合）</a></li>
<li><a href="Spark/SparkRDD.html#sortbykey%E6%8C%89%E7%85%A7key%E6%8E%92%E5%BA%8F">sortByKey（按照key排序）</a></li>
<li><a href="Spark/SparkRDD.html#%E4%BC%98%E5%8C%96shuffle%E6%80%A7%E8%83%BD">优化shuffle性能</a></li>
<li><a href="Spark/SparkRDD.html#%E7%BC%A9%E5%87%8F%E5%88%86%E5%8C%BAcoalesce">缩减分区（coalesce）</a></li>
<li><a href="Spark/SparkRDD.html#repartition%E6%89%A9%E5%A4%A7%E5%88%86%E5%8C%BA">repartition（扩大分区）</a></li>
</ul>
</li>
<li><a href="Spark/SparkRDD.html#action%E7%AE%97%E5%AD%90">Action算子</a>
<ul>
<li><a href="Spark/SparkRDD.html#%E5%A6%82%E4%BD%95%E5%8C%BA%E5%88%86%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90%E5%92%8C%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90">如何区分转换算子和行动算子？</a></li>
<li><a href="Spark/SparkRDD.html#collect%E6%96%B9%E6%B3%95">collect()方法</a></li>
<li><a href="Spark/SparkRDD.html#%E5%85%B6%E4%BB%96%E6%96%B9%E6%B3%95">其他方法</a>
<ul>
<li><a href="Spark/SparkRDD.html#saveastextfilevssaveasobjectfile"><code>saveAsTextFile</code>VS<code>saveAsObjectFile</code></a></li>
<li><a href="Spark/SparkRDD.html#collect%E5%92%8Cforeach%E8%81%94%E5%8A%A8">collect()和foreach()联动</a></li>
</ul>
</li>
<li><a href="Spark/SparkRDD.html#%E5%BA%8F%E5%88%97%E5%8C%96">序列化</a></li>
</ul>
</li>
</ul>
<h2 id="概述"><a class="header" href="#概述">概述</a></h2>
<p>在Apache Spark中，RDD（Resilient Distributed Dataset，弹性分布式数据集）是Spark的核心概念之一，是一种不可变的分布式数据集。RDD为大规模数据处理提供了一种高效且容错的方式。</p>
<p>RDD：分布式计算模型</p>
<ol>
<li>一定是一个对象</li>
<li>一定封装了大量方法和属性（计算逻辑）</li>
<li>一定需要适合进行分布式处理（降低数据规模，实现并行计算）</li>
</ol>
<p>分布式集群中心化基础架构——主从架构（Master-Slave）</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20241126153006.png" alt="" /></p>
<h2 id="详细介绍"><a class="header" href="#详细介绍">详细介绍</a></h2>
<p>以下是对RDD的详细介绍：</p>
<h3 id="1-基本概念"><a class="header" href="#1-基本概念">1. 基本概念</a></h3>
<ul>
<li><strong>弹性</strong>：RDD支持在节点故障时自动恢复丢失的数据。它通过记录数据的来源（血统信息）来实现这一点。</li>
<li><strong>分布式</strong>：RDD的数据会被分散存储在集群的多个节点上，利用并行计算提高数据处理的效率。</li>
<li><strong>不可变性</strong>：一旦创建，RDD中的数据不能被改变。这意味着对RDD的操作会返回一个新的RDD，而不是修改原有的RDD。</li>
</ul>
<h3 id="2-创建rdd"><a class="header" href="#2-创建rdd">2. 创建RDD</a></h3>
<p>RDD可以通过以下几种方式创建：</p>
<ul>
<li><strong>从已有的集合</strong>：可以直接从本地的Python、Scala等集合创建RDD。例如，在Scala中使用<code>sc.parallelize()</code>方法。</li>
<li><strong>从外部存储</strong>：可以从HDFS、S3等外部存储系统读取数据，比如使用<code>sc.textFile()</code>方法来读取文本文件。</li>
</ul>
<h3 id="3-rdd操作"><a class="header" href="#3-rdd操作">3. RDD操作</a></h3>
<p>RDD支持两类操作：转换和行动。</p>
<h4 id="转换transformations"><a class="header" href="#转换transformations">转换（Transformations）</a></h4>
<p>转换是指对RDD进行的一种操作，生成一个新的RDD。转换是惰性执行的，即只有在需要结果时才会被计算。常见的转换操作包括：</p>
<ul>
<li><code>map(func)</code>：对RDD中的每个元素应用<code>func</code>函数，返回一个新的RDD。</li>
<li><code>filter(func)</code>：根据给定的<code>func</code>函数进行筛选，返回满足条件的元素的新RDD。</li>
<li><code>flatMap(func)</code>：类似于<code>map</code>，但每个输入元素可以映射到零个或多个输出元素。</li>
<li><code>union(otherRDD)</code>：返回一个新的RDD，包括两个RDD的所有元素。</li>
<li><code>join(otherRDD)</code>：对两个RDD进行连接操作，返回一个包含键值对的新RDD。</li>
</ul>
<h4 id="行动actions"><a class="header" href="#行动actions">行动（Actions）</a></h4>
<p>行动是指对RDD进行的操作，会触发计算并返回结果。不像转换，行动会立即执行。常见的行动操作包括：</p>
<ul>
<li><code>collect()</code>：从所有节点收集数据到驱动程序，返回一个包含所有数据的数组。</li>
<li><code>count()</code>：返回RDD中元素的数量。</li>
<li><code>first()</code>：返回RDD中的第一个元素。</li>
<li><code>take(n)</code>：返回RDD中的前n个元素。</li>
<li><code>saveAsTextFile(path)</code>：将RDD的数据保存到指定路径的文本文件中。</li>
</ul>
<h3 id="4-容错机制"><a class="header" href="#4-容错机制">4. 容错机制</a></h3>
<p>RDD的容错机制是通过血统（Lineage）来实现的。每个RDD都maintains了其转换操作的一个有向无环图。在发生节点故障时，Spark可以根据这个血统信息重新计算丢失的分区，确保数据的完整性和可靠性。</p>
<h3 id="5-计算模型"><a class="header" href="#5-计算模型">5. 计算模型</a></h3>
<p>RDD采用延迟计算的模型，实际的计算只有在执行行动操作时才会发生。这种特性可以优化job的执行效率，使得Spark能够更好地利用集群的资源。</p>
<h3 id="6-性能优化"><a class="header" href="#6-性能优化">6. 性能优化</a></h3>
<p>Spark中的RDD通过各种特性来提升性能，包括：</p>
<ul>
<li>数据的分区（Partitioning）：合理的分区可以提高并行度，减少任务之间的Shuffle。</li>
<li>缓存（Caching）：经常使用的RDD可以被缓存到内存中，以提速后续计算。</li>
</ul>
<h3 id="总结"><a class="header" href="#总结">总结</a></h3>
<p>RDD是Apache Spark的基础数据结构，提供了强大的分布式计算能力和容错特性。它的设计使得开发者可以用简洁的操作来处理大规模数据，从而提高了数据处理的效率和可扩展性。在使用Spark进行大数据处理时，了解RDD的原理和操作是非常重要的。</p>
<p>RDD模型可以封装数据的处理逻辑，但是这个逻辑不能太复杂，类似于字符串。</p>
<p>RDD的功能类似于字符串的功能，需要通过大量的RDD对象组合在一起实现复杂的功能。</p>
<p>RDD和字符串的区别：</p>
<ol>
<li>字符串的功能是单点操作，功能一旦调用，就会马上执行</li>
<li>RDD的功能是分布式操作，功能调用但不会马上执行</li>
</ol>
<h1 id="rdd-编程"><a class="header" href="#rdd-编程">RDD 编程</a></h1>
<h2 id="21rdd的创建"><a class="header" href="#21rdd的创建">2.1RDD的创建</a></h2>
<p>在Spark中创建RDD的创建方式可以分为三种：从集合中创建RDD、从外部存储创
建RDD、从其他RDD创建。</p>
<h2 id="211idea环境准备"><a class="header" href="#211idea环境准备">2.1.1IDEA环境准备</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import java.util.Arrays;
import java.util.List;

public class Spark02_RDD_Memory_Partition {
    public static void main(String[] args) {
        // 1.创建配置对象
        SparkConf conf = new SparkConf();
        conf.setAppName("Spark01_Env");
        conf.setMaster("local[*]");
        conf.set("spark.default.parallelism", "4");

        // 2. 创建sparkContext
        JavaSparkContext jsc = new JavaSparkContext(conf);

        // 3. 编写代码
        List&lt;String&gt; names = Arrays.asList("zhangsan", "lisi", "wangwu", "zhaoliu");
        JavaRDD&lt;String&gt; rdd = jsc.parallelize(names, 3);
        rdd.saveAsTextFile("output");
        // 4. 关闭sc
        jsc.stop();
    }
}

</code></pre>
<p>parallelize方法可以传递2个参数的</p>
<p>第一个参数表示对接的数据源集合</p>
<p>第二个参数表示切片（分区）的数量</p>
<p>可以不需要指定，spark会采用默认值进行分区（切片）</p>
<p>numSlices =scheduler.conf.getInt("spark.default.parallelism",totalcores)</p>
<p>从配置对象中获取配置参数：spark.default.parallelism（默认并行度）</p>
<p>如果配置参数不存在，那么默认取值为totalCores（当前环境总的虚拟核数），</p>
<p>Kafka可以将数据进行切片（减小规模），也称之为分区，这个分区操作是底层完成的。</p>
<p>Local环境中，分区数量和环境核数相关，但是一般不推荐分区数量需要手动设定</p>
<p>Spark在读取集合数据时，分区设定存在3种不同场合</p>
<ol>
<li>优先使用方法参数</li>
<li>使用配置参数：spark.default.parallelism</li>
<li>采用环境默认值</li>
</ol>
<p>TOD0文件数据源分区设定也存在多个位置</p>
<p>1.textFile可以传递第二个参数：minPartitions（最小分区数）</p>
<p>参数可以不需要传递的，那么Spark会采用默认值</p>
<p><code>minPartitions =math.min(defaultParallelism，2)</code></p>
<p>2.使用配置参数：<code>spark.default.parallelism=&gt;1=&gt;math.min（参数，2）</code></p>
<p>3.采用环境默认总核值=&gt;math.min（总核数，2）</p>
<p>Spark框架基于MR开发的。</p>
<p>Spark框架文件的操作没有自已的实现的。采用MR库（Hadoop）来实现当前读取文件的切片数量不是由Spark决定的，而是由Hadoop决定</p>
<p>Hadoop切片规则：</p>
<pre><code>totalsize:7byte
goalsize:totalsize/min-part-num=&gt;7/3=2byte
part-num:totalsize/goalsize=&gt;7/2=&gt;3....1
</code></pre>
<pre><code>finalList&lt;Integer&gt;names =Arrays.asList（1,2,3,4,5,6);

/*
【1】【2,3】【4】【5，6】

len=6,partnum=4

(0 until 4) =&gt; [0，1,2,3]

θ=&gt;((i*length)/numSlices，(((i+1）*length)/numSlices)) 
=&gt;（(0*6）/4，（((0+1）*6）/4))
（0，1）=&gt;1

1 =&gt;（(i*length）/numSlices，（((i+1）*length）/numSlices))
=&gt; =&gt;
=&gt; 3=&gt;
=&gt;
（(1*6）/4，（((2)*6)/4))(1，3）=&gt;2
（(i*length）/numSlices，（((i+1）*length）/numSlices))
/4，（((3）*6）/4))
*6)((2
4）=&gt;1(3,
*length）/numSlices，（((i+1）*length）/numSlices))((i
（((4）*6）/4))
((3
6)
/4,
*
(4, 6) =&gt; 2
=&gt;
</code></pre>
<p>Spark进行分区处理时，需要对每个分区的数据尽快能地平均分配</p>
<p>totalsize=7
goalsize=totalsize/minpartnum=7/2=3
partnum=totalsize/goalsize =7/3=2...1=&gt;2+1=3</p>
<p>Spark不支持文件操作的。文件操作都是由Hadoop完成的</p>
<p>Hadoop进行文件切片数量的计算和文件数据存储计算规则不一样。</p>
<p>1.分区数量计算的时候，考虑的是尽可能的平均：按字节来计算</p>
<p>2.分区数据的存储是考虑业务数据的完整性：按照行来读取</p>
<p>读取数据时，还需要考虑数据偏移量，偏移量从0开始的，相同偏移量对应的数据不能重复读取。</p>
<pre><code>1．分区数量
goalsize =14/4=&gt;3
= 14/3=4...2=&gt;4+1=5
partnum 2．分区数据
[0,3][3，6][6，9][9，12][12, 14]
110 =&gt;
=&gt; =&gt; =&gt; =&gt; =&gt;
0123
4567
22
=&gt;
【11】【22】【33】【44】 4
891011
33@@
=&gt;
44
1213
</code></pre>
<p>注意避免数据倾斜问题，以行为单位存储业务</p>
<h1 id="rdd方法"><a class="header" href="#rdd方法">RDD方法</a></h1>
<p>注意方法名、IN、OUT</p>
<p>两大类</p>
<p>RDD类似于数据管道，可以流转数据，但是不能保存数据</p>
<p>RDD有很多的方法可以将数据向下流转，称之为转换</p>
<p>RDD有很多的方法可以控制数据流转，称之为行动</p>
<p>算子（operate）：操作、方法</p>
<h2 id="处理数据方法的分类"><a class="header" href="#处理数据方法的分类">处理数据方法的分类</a></h2>
<p>两大类：单值、键值</p>
<p>Scala中()构成元组</p>
<p>Scala语言中可以将无关的数据封装在一起，形成一个整体，称之为元素的组合，简称为【元组】</p>
<p>如果想要访问元组中的数据，必须采用顺序号</p>
<p>如果元组中的数据只有2个的场合，称之为对偶元组，也称之为键值对</p>
<p>马丁将Scala融合到Java，形成的JDK1.8中的Tuple类</p>
<pre><code class="language-scala">var kV=（0，1) 
var kv1 =("zhangsan", 20,1001) 
</code></pre>
<pre><code class="language-java">import scala.Tuple1;  
import scala.Tuple2;



Tuple2&lt;String, Integer&gt; tuple = new Tuple2&lt;&gt;("zhangsan", 18);  
System.out.println(tuple._1 + " " + tuple._2;  
  
Tuple1&lt;String&gt; tuple1 = new Tuple1&lt;&gt;("zhangsan");  
System.out.println(tuple1._1());
</code></pre>
<p>Java中元组的最大数据容量为22</p>
<h1 id="spark-on-yarn-部署搭建详细图文教程"><a class="header" href="#spark-on-yarn-部署搭建详细图文教程">Spark on YARN 部署搭建详细图文教程</a></h1>
<p>https://blog.csdn.net/weixin_46560589/article/details/132898417</p>
<p>在您提供的代码中，最主要的问题出现在 <code>rdd.map</code> 调用中的 <code>Function</code> 接口的实现上。<code>Function</code> 接口是用于定义一个返回值的函数，但在 Spark 中的 <code>map</code> 操作中采用的是 <code>Function&lt;T, R&gt;</code> 接口类型。因此，您只需要实现 <code>apply</code> 方法，而没有必要实现 <code>call</code> 方法。</p>
<p>以下是您代码中需要修改的地方：</p>
<h3 id="1-function-接口的用法"><a class="header" href="#1-function-接口的用法">1. <strong>Function 接口的用法</strong></a></h3>
<p>在 Spark Java API 中 <code>Function</code> 接口只有一个方法 <code>apply</code>，并且 <code>apply</code> 方法参数与返回类型匹配。这是不需要实现 <code>call</code> 方法的。</p>
<h3 id="2-选择合适的类型"><a class="header" href="#2-选择合适的类型">2. <strong>选择合适的类型</strong></a></h3>
<p>为了使 <code>apply</code> 方法返回 <code>Integer</code> 而不是 <code>Object</code>，你应该将 <code>mapRdd</code> 的类型定义为 <code>JavaRDD&lt;Integer&gt;</code>，而不是 <code>JavaRDD&lt;Object&gt;</code>。</p>
<h3 id="3-修正-map-的-lambda-表达式使用"><a class="header" href="#3-修正-map-的-lambda-表达式使用">3. <strong>修正 <code>map</code> 的 lambda 表达式使用</strong></a></h3>
<p>可以考虑使用 lambda 表达式来简化代码。</p>
<p>修正后的代码如下：</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import java.util.Arrays;
import java.util.List;

public class Spark02_Operate_Transform_Map {
    public static void main(String[] args) {
        // 1.创建配置对象
        SparkConf conf = new SparkConf();
        conf.setMaster("local[*]");
        conf.setAppName("sparkCore");

        // 2. 创建sparkContext
        JavaSparkContext sc = new JavaSparkContext(conf);

        // 3. 编写代码
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4, 5);
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);

        // map操作，返回类型应为 JavaRDD&lt;Integer&gt;
        JavaRDD&lt;Integer&gt; mapRdd = rdd.map(integer -&gt; integer * 2);
        
        // collect() 返回的是 List 类型，所以后续不能直接调用 foreach，要使用方法调用
        mapRdd.collect().forEach(System.out::println);

        System.out.println("---------------------------");
        System.out.println(nums);

        // 4. 关闭sc
        sc.stop();
    }
}
</code></pre>
<pre><code>ToDo如果Java中接口采用注解oFunctionaLInterface声明，那么接口的使用就可以采用JDK提供的函数式编程的语法实现（lambda表达式）
1.return可以省略：map方法就需要返回值，所以不需要return
2.分号可以省略：可以采用换行的方式表示代码逻辑 
3.大括号可以省略：如果逻辑代码只有一行
4.小括号可以省略：参数列表中的参数只有一个
5.参数和箭头可以省略：参数在逻辑中只使用了一次（需要有对象来实现功能）
</code></pre>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
import java.util.function.Function;  
  
public class Spark02_Operate_Transform_Map {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[*]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4, 5);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
  
        // map操作  
        //JavaRDD&lt;Integer&gt; mapRdd = rdd.map(integer -&gt; integer * 2);  
        JavaRDD&lt;Integer&gt; mapRdd = rdd.map(NumberTest::doubleNumber);  
        mapRdd.collect().forEach(System.out::println);  
  
        System.out.println("---------------------------");  
        System.out.println(nums);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}  
class NumberTest {  
    public static int doubleNumber(int num) {  
        return num * 2;  
    }  
}
</code></pre>
<p>函数式编程写法</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
  
public class Spark02_Operate_Transform_Map {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[*]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        sc.parallelize(Arrays.asList(1, 2, 3, 4, 5), 2)  
                .map(NumberTest::doubleNumber)  
                .collect()  
                .forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}  
class NumberTest {  
    public static int doubleNumber(int num) {  
        return num * 2;  
    }  
}
</code></pre>
<p>Map转换方法，将传递的数据转换为其他数据返回</p>
<p>默认情况下，新创建的RDD的分区数量和之前旧的RDD的数量保持一致</p>
<p>数据流转过程中，数据所在分区会如何变化？</p>
<p>默认情况下，数据流转，所在的分区编号不变。分组处理后会发生变化。</p>
<p>Spark处理不同分区数据时：分区内有序，分区间无序</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
  
public class Spark02_Operate_Transform_Map2 {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        sc.parallelize(Arrays.asList(1, 2, 3, 4), 2)  
                .map(NumberTest2::doubleNumber)  
                .collect();  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}  
class NumberTest2 {  
    public static int doubleNumber(int num) {  
        System.out.println("doubleNumber: @" + num);  
        return num * 2;  
    }  
}
</code></pre>
<pre><code>doubleNumber: @3
doubleNumber: @1
doubleNumber: @4
doubleNumber: @2
</code></pre>
<p>一个RDD中处理完所有数据后，下一个RDD才会继续处理数据。</p>
<p>原因：RDD不保存数据</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_Map2 {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
        JavaRDD&lt;Integer&gt; mapRdd1 = rdd.map(num -&gt; {  
            System.out.println("map1: @" + num);  
            return num;  
        });  
        JavaRDD&lt;Integer&gt; mapRdd2 = mapRdd1.map(num -&gt; {  
            System.out.println("map1: #####" + num);  
            return num;  
        });  
        mapRdd2.foreach(num -&gt; System.out.println("foreach: " + num));  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>map1: @3
map1: @1
map1: #####3
map1: #####1
foreach: 1
foreach: 3
map1: @2
map1: @4
map1: #####4
map1: #####2
foreach: 4
foreach: 2
</code></pre>
<h2 id="filter"><a class="header" href="#filter">Filter</a></h2>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.operate;  
  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_Filter {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[*]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
        rdd.filter(num -&gt; num % 2 == 0).collect().forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<h2 id="flatmap"><a class="header" href="#flatmap">FlatMap</a></h2>
<p>将整体数据拆分成个体来使用的操作称为扁平化操作</p>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.operate;  
  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import org.apache.spark.api.java.function.FlatMapFunction;  
  
import java.util.ArrayList;  
import java.util.Arrays;  
import java.util.Iterator;  
import java.util.List;  
  
public class Spark02_Operate_Transform_FlatMap {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;List&lt;Integer&gt;&gt; data = Arrays.asList(Arrays.asList(1, 2, 3), Arrays.asList(4, 5, 6));  
        JavaRDD&lt;List&lt;Integer&gt;&gt; rdd = sc.parallelize(data, 2);  
        //rdd.flatMap(list -&gt; list.iterator()).collect().forEach(System.out::println);  
        JavaRDD&lt;Integer&gt; flatMapRdd = rdd.flatMap(new FlatMapFunction&lt;List&lt;Integer&gt;, Integer&gt;(){  
  
            public Iterator&lt;Integer&gt; call(List&lt;Integer&gt; list) throws Exception {  
                List&lt;Integer&gt; nums = new ArrayList&lt;&gt;();  
                list.forEach(num -&gt; nums.add(num * 2));  
                return nums.iterator();  
            }  
        });  
        flatMapRdd.collect().forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>2
4
6
8
10
12
</code></pre>
<p>map方法只负责转换数据（A-&gt;B[B1，B2，B3])，不能将数据拆分后独立使用</p>
<p>flatMap方法可以将数据拆分后独立使用（A-&gt;B1，B2，B3）</p>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.operate;  
  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import org.apache.spark.api.java.function.FlatMapFunction;  
  
import java.util.ArrayList;  
import java.util.Arrays;  
import java.util.Iterator;  
import java.util.List;  
  
public class Spark02_Operate_Transform_FlatMap1 {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
  
        JavaRDD&lt;String&gt; rdd = sc.textFile("spark/src/main/resources/data/test.txt");  
        JavaRDD&lt;String&gt; flatMapRdd = rdd.flatMap(line -&gt; Arrays.asList(line.split(" ")).iterator());  
        flatMapRdd.collect().forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>hadoop
flume
spark
hive
mysql
navicat
oracle
bazaar
git
</code></pre>
<h2 id="groupby方法"><a class="header" href="#groupby方法">groupby方法</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_Groupby {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
        rdd.groupBy(num -&gt; num % 5).collect().forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>(4,[4, 9])
(0,[5, 10])
(2,[2, 7])
(1,[1, 6])
(3,[3, 8])
</code></pre>
<p>如何解决数据倾斜？</p>
<p>重新分组？</p>
<p>默认情况下，数据处理后，所在的分区不会发生变化，但是groupBy方法例外</p>
<p>Spark在数据处理中，要求同一个组的数据必须在同一个分区中</p>
<p>所以分组操作会将<strong>数据分区中的数据</strong>打乱重新组合，在Spark中这个操作被称为Shuffle</p>
<p>一个分区可以存放多个组</p>
<p>Spark要求所有的数据必须分组后才能继续执行后续操作</p>
<p>RDD对象不能保存数据</p>
<p>当前groupBy操作会将数据保存到磁盘文件中，保证数据全部分组后执行后续操作</p>
<p>Shuffle操作一定会落盘，但可能会导致资源浪费</p>
<p>Spark中含有Shuffle操作的方法都有改变分区的能力</p>
<p>RDD的分区和Task之间有关系。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20241231155313.png" alt="" /></p>
<p>Shuffle会将完整的计算流程一分为二，其中一部分任务会写磁盘，另外一部分的任务会读磁盘</p>
<p>写磁盘的操作不完成，不允许读磁盘</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_Groupby {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
        rdd.groupBy(num -&gt; num % 5).collect().forEach(System.out::println);  
  
        System.out.println("执行结束");  
  
  
        //监控页面：http://localhost:4040  
        Thread.sleep(300000);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<h2 id="distinct"><a class="header" href="#distinct">distinct</a></h2>
<p>distinct 分布式去重，采用了分组+Shuffle的处理方式</p>
<p>hashSet 单点去重</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_Distinct {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 1,1,12,2,2);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 3);  
        rdd.distinct().collect().forEach(System.out::println);  
  
        System.out.println("执行结束");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<h2 id="sortby"><a class="header" href="#sortby">sortBy</a></h2>
<p>传递三个参数</p>
<p>第一个参数表示排序规则——Spark会为每一个数据增加一个标记，然后按照标记对数据进行排序</p>
<p>第二个参数表示排序的方式</p>
<p>第三个参数表示分区数量</p>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.operate;  
  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_SortBy {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 4,9,12,2,2);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 3);  
        rdd.distinct().sortBy(num -&gt; num, true, 2).collect().forEach(System.out::println);  
  
        System.out.println("执行结束");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code class="language-java">package com.zzw.bigdata.spark.rdd.operate;  
  
import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_SortBy {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 4,9,12,2,2);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 3);  
        rdd.distinct().sortBy(num -&gt; "" + num, true, 2).collect().forEach(System.out::println);  
  
        System.out.println("执行结束");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>1
12
2
4
9
执行结束
</code></pre>
<p>字符串按照字典顺序排列</p>
<h1 id="kv类型"><a class="header" href="#kv类型">KV类型</a></h1>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_KV {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        //KV类型一般表示二元组，如(key, value)，SparkRDD中对数据整体的处理称为单值操作，对数据中的每个元素进行操作称为KV操作。  
        Tuple2&lt;String, Integer&gt; t1 = new Tuple2&lt;String, Integer&gt;("zhangsan", 18);  
        Tuple2&lt;String, Integer&gt; t2 = new Tuple2&lt;String, Integer&gt;("lisi", 20);  
        Tuple2&lt;String, Integer&gt; t3 = new Tuple2&lt;String, Integer&gt;("wangwu", 22);  
        Tuple2&lt;String, Integer&gt; t4 = new Tuple2&lt;String, Integer&gt;("zhaoliu", 25);  
  
        List&lt;Tuple2&lt;String, Integer&gt;&gt; list = Arrays.asList(t1, t2, t3, t4);  
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(list);  
  
        //单值处理  
        rdd.map(t -&gt; new Tuple2&lt;&gt;(t._1, t._2 * 2)).collect().forEach(System.out::println);  
  
        // 3.1 KV操作  
        // 3.1.1 mapValues  
        JavaPairRDD&lt;String, Integer&gt; pairRdd = sc.parallelizePairs(list);  
        pairRdd.mapValues(value -&gt; value * 2).collect().forEach(System.out::println);  
  
        System.out.println("执行结束");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>(zhangsan,36)
(lisi,40)
(wangwu,44)
(zhaoliu,50)
(zhangsan,36)
(lisi,40)
(wangwu,44)
(zhaoliu,50)
执行结束
</code></pre>
<h2 id="单值类型转换为kv类型"><a class="header" href="#单值类型转换为kv类型">单值类型转换为KV类型</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_KV_1 {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4, 5);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums);  
  
        rdd.mapToPair(num -&gt; new Tuple2&lt;&gt;(num, num * 2)).mapValues(num -&gt; num * 2).collect().forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>(1,4)
(2,8)
(3,12)
(4,16)
(5,20)
</code></pre>
<h2 id="groupby和kv类型"><a class="header" href="#groupby和kv类型">groupBy和KV类型</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_KV_GroupBy {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
  
        JavaPairRDD&lt;Integer, Iterable&lt;Integer&gt;&gt; groupRDD = rdd.groupBy(num -&gt; num % 2);  
        groupRDD.mapValues(iter -&gt; {  
            int sum = 0;  
            for (Integer num : iter) {  
                sum += num;  
            }  
            return sum;  
        }).collect().forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>(0,6)
(1,4)
</code></pre>
<h2 id="wordcount"><a class="header" href="#wordcount">WordCount</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_KV_WordCount {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
  
        JavaRDD&lt;String&gt; lineRDD = sc.textFile("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\word.txt");  
        JavaRDD&lt;String&gt; wordRDD = lineRDD.flatMap(line -&gt; Arrays.asList(line.split(" ")).iterator());  
        JavaPairRDD&lt;String, Iterable&lt;String&gt;&gt; wordGroupRDD = wordRDD.groupBy(word -&gt; word);  
        JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordGroupRDD.mapValues(iter -&gt; {  
            int count = 0;  
            for (String s : iter) {  
                count++;  
            }  
            return count;  
        });  
        wordCountRDD.collect().forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>(Flink,1)
(Zookeeper,1)
(Kafka,3)
(Cassandra,1)
(Spark,2)
(Flume,2)
(Redis,1)
(HBase,1)
(Hadoop,2)
</code></pre>
<h2 id="groupbykey按key对value进行分组"><a class="header" href="#groupbykey按key对value进行分组">GroupByKey（按Key对Value进行分组）</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_KV_GroupByKey {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; rdd = sc.parallelize(Arrays.asList(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("b", 2), new Tuple2&lt;&gt;("a", 3), new Tuple2&lt;&gt;("c", 4)));  
        JavaPairRDD&lt;String, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt;&gt; groupRDD = rdd.groupBy(t -&gt; t._1);  
        groupRDD.collect().forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>(b,[(b,2)])
(a,[(a,1), (a,3)])
(c,[(c,4)])
</code></pre>
<p>简单写法如下所示：</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_KV_GroupByKey {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        sc.parallelizePairs(Arrays.asList(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("b", 2), new Tuple2&lt;&gt;("a", 3), new Tuple2&lt;&gt;("c", 4))).groupByKey().collect().forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>(b,[2])
(a,[1, 3])
(c,[4])
</code></pre>
<p>加入求和逻辑：</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_KV_GroupByKey {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        sc.parallelizePairs(Arrays.asList(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("b", 2), new Tuple2&lt;&gt;("a", 3), new Tuple2&lt;&gt;("c", 4)))  
                .groupByKey().mapValues(iter -&gt; {  
                    int sum = 0;  
                    for (int i : iter) {  
                        sum += i;  
                    }  
                    return sum;  
                })  
                .collect()  
                .forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>(b,2)
(a,4)
(c,4)
</code></pre>
<h2 id="reducebykey按照key对value两两聚合"><a class="header" href="#reducebykey按照key对value两两聚合">ReduceByKey（按照key对value两两聚合）</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
  
public class Spark02_Operate_Transform_KV_ReduceByKey {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        sc.parallelize(Arrays.asList(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("b", 2), new Tuple2&lt;&gt;("a", 3), new Tuple2&lt;&gt;("c", 4)))  
                .mapToPair(t -&gt; t).reduceByKey(Integer::sum)  
                .collect()  
                .forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>(b,2)
(a,4)
(c,4)
</code></pre>
<h2 id="sortbykey按照key排序"><a class="header" href="#sortbykey按照key排序">sortByKey（按照key排序）</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
  
public class Spark02_Operate_Transform_KV_SortByKey {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        sc.parallelize(Arrays.asList(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("b", 2), new Tuple2&lt;&gt;("a", 3), new Tuple2&lt;&gt;("c", 4)))  
                .mapToPair(t -&gt; t).sortByKey(false)  
                .collect()  
                .forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>(c,4)
(b,2)
(a,1)
(a,3)
</code></pre>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
  
public class Spark02_Operate_Transform_KV_SortByKey {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        sc.parallelize(Arrays.asList(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("a", 2), new Tuple2&lt;&gt;("a", 3), new Tuple2&lt;&gt;("a", 4)))  
                .mapToPair(t -&gt; new Tuple2&lt;&gt;(t._2, t)).sortByKey(false).map(t -&gt; t._2)  
                .collect()  
                .forEach(System.out::println);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>(a,4)
(a,3)
(a,2)
(a,1)
</code></pre>
<h2 id="优化shuffle性能"><a class="header" href="#优化shuffle性能">优化shuffle性能</a></h2>
<ol>
<li>花钱</li>
<li>增加磁盘读写缓冲区</li>
<li>如果不影响最终结果的话，那么磁盘读写的数据越少，性能越高。（reduceByKey可以在Shuffle之前就预先聚合Combine，极大地提高性能）</li>
</ol>
<h2 id="缩减分区coalesce"><a class="header" href="#缩减分区coalesce">缩减分区（coalesce）</a></h2>
<p>该方法没有Shuffle功能，所以数据不会被打乱然后重新组合。</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_Partition {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4, 5, 6);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
        JavaRDD&lt;Integer&gt; filterRDD = rdd.filter(num -&gt; num % 2 == 0);  
  
        JavaRDD&lt;Integer&gt; coalesceRDD = filterRDD.coalesce(1);  
        coalesceRDD.saveAsTextFile("./output");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<p>该方法默认无法扩大分区，只能缩减分区。</p>
<p>设定shuffle参数为true，可以实现扩大分区</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_Partition {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4, 5, 6);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
        JavaRDD&lt;Integer&gt; filterRDD = rdd.filter(num -&gt; num % 2 == 0);  
  
        JavaRDD&lt;Integer&gt; coalesceRDD = filterRDD.coalesce(3, true);  
        coalesceRDD.saveAsTextFile("./output");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<h2 id="repartition扩大分区"><a class="header" href="#repartition扩大分区">repartition（扩大分区）</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Transform_Partition {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4, 5, 6);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
        JavaRDD&lt;Integer&gt; filterRDD = rdd.filter(num -&gt; num % 2 == 0);  
  
        filterRDD.repartition(3).saveAsTextFile("./output");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<h1 id="action算子"><a class="header" href="#action算子">Action算子</a></h1>
<h2 id="如何区分转换算子和行动算子"><a class="header" href="#如何区分转换算子和行动算子">如何区分转换算子和行动算子？</a></h2>
<p>不能用是否启动Job来区分</p>
<p>转换算子目的：将旧的RDD转换成新的RDD来组合多个RDD的功能，格式：RDD（In）-&gt;RDD(Out)</p>
<p>行动算子目的：返回具体执行结果</p>
<p>sortBy()方法会sample，然后提前执行一下collect()方法，故而有Job进行</p>
<h2 id="collect方法"><a class="header" href="#collect方法">collect()方法</a></h2>
<p>将Executor端执行的结果按照分区的顺序拉取（采集）到Driver端，将结果组合成集合对象</p>
<p>特殊情况：文件内容存在于HDFS</p>
<p>collect()方法可能会将多个Executor的数据大量拉取到Driver端，导致内存溢出。生产环境下慎用</p>
<h2 id="其他方法"><a class="header" href="#其他方法">其他方法</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
  
import java.util.Arrays;  
import java.util.List;  
  
public class Spark02_Operate_Action {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
  
        List&lt;Integer&gt; collect = rdd.collect();  
        rdd.count();  
        Integer first = rdd.first();  
        List&lt;Integer&gt; take = rdd.take(3);  
        System.out.println("collect: " + collect);  
        System.out.println("count: " + rdd.count());  
        System.out.println("first: " + first);  
        System.out.println("take: " + take);  
  
        System.out.println("执行结束");  
  
  
        //监控页面：http://localhost:4040  
        //Thread.sleep(300000);  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<pre><code>collect: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
first: 1
take: [1, 2, 3]
执行结束
</code></pre>
<pre><code class="language-java">import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.util.Arrays;
import java.util.List;
import java.util.Map;

public class Spark02_Operate_Action {
    public static void main(String[] args) throws InterruptedException {
        // 1.创建配置对象
        SparkConf conf = new SparkConf();
        conf.setMaster("local[2]");
        conf.setAppName("sparkCore");

        // 2. 创建sparkContext
        JavaSparkContext sc = new JavaSparkContext(conf);

        // 3. 编写代码
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4);
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);

        JavaPairRDD&lt;Integer, Integer&gt; pairRDD = rdd.mapToPair((x) -&gt; new Tuple2&lt;&gt;(x, x * x));
        Map&lt;Integer, Long&gt; integerLongMap = pairRDD.countByKey();
        System.out.println(integerLongMap);

        // 4. 关闭sc
        sc.stop();

    }
}
</code></pre>
<pre><code>{4=1, 2=1, 1=1, 3=1}
</code></pre>
<h3 id="saveastextfilevssaveasobjectfile"><a class="header" href="#saveastextfilevssaveasobjectfile"><code>saveAsTextFile</code>VS<code>saveAsObjectFile</code></a></h3>
<p>后者用于保存对象比如User</p>
<h3 id="collect和foreach联动"><a class="header" href="#collect和foreach联动">collect()和foreach()联动</a></h3>
<p>foreach()在Executor端输出，分布式无序输出</p>
<p>collect()和foreach()联动之后在Driver端单点有序输出</p>
<p>foreachPartition执行效率高，但是受到内存大小限制</p>
<pre><code class="language-java">SparkConf conf = new SparkConf();  
conf.setMaster("local[2]");  
conf.setAppName("sparkCore");  
  
// 2. 创建sparkContext  
JavaSparkContext sc = new JavaSparkContext(conf);  
  
// 3. 编写代码  
List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4);  
JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
  
rdd.collect().forEach(System.out::println);  
System.out.println("--------------------------");  
rdd.foreach(System.out::println);  
System.out.println("------------------");  
rdd.foreachPartition(  
        list -&gt; {  
            System.out.println(list);  
        }  
);  
  
// 4. 关闭sc  
sc.stop();
</code></pre>
<p>执行会报错<code>Exception in thread "main" org.apache.spark.SparkException: Task not serializable</code></p>
<p>错误原因：foreach里面使用了lambda表达式</p>
<p>正确代码如下所示：</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
import java.util.List;  
import java.util.Map;  
  
public class Spark02_Operate_Action {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
  
        rdd.collect().forEach(System.out::println);  
        System.out.println("--------------------------");  
        rdd.foreach(num -&gt; System.out.println(num));  
        System.out.println("------------------");  
        rdd.foreachPartition(  
                list -&gt; {  
                    System.out.println(list);  
                }  
        );  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>1
2
3
4
--------------------------
1
3
4
2
------------------
IteratorWrapper(&lt;iterator&gt;)
IteratorWrapper(&lt;iterator&gt;)
</code></pre>
<h2 id="序列化-2"><a class="header" href="#序列化-2">序列化</a></h2>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.io.Serializable;  
import java.util.Arrays;  
import java.util.List;  
import java.util.Map;  
  
public class Spark02_Operate_Action {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4);  
        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
  
        Student student = new Student();  
  
        rdd.foreach(num -&gt; System.out.println(student.name + " " + num));  
        System.out.println("------------------");  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}  
class Student implements Serializable {  
    public String name = "baozi";  
}
</code></pre>
<p>在Executor端循环遍历的时候使用到了Driver端对象</p>
<p>运行过程中就需要将Driver端的对象通过网络传递到Executor端，否则无法使用</p>
<p>这里传输的对象必须要实现可序列化接口，否则无法传递</p>
<p>RDD算子的逻辑代码是在Executor端执行的，其他的代码都在Driver端执行</p>
<p>下面两份代码都是正确的：</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.io.Serializable;  
import java.util.Arrays;  
import java.util.List;  
import java.util.Map;  
  
public class Spark02_Operate_Action {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;String&gt; list = Arrays.asList("Hive", "Hadoop", "Spark", "Flink");  
        JavaRDD&lt;String&gt; rdd = sc.parallelize(list, 2);  
  
        new Search("H").match(rdd);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}  
class Search implements Serializable {  
    private String query;  
    public Search(String query){  
        this.query = query;  
    }  
    public void match(JavaRDD&lt;String&gt; rdd){  
        rdd.filter(line -&gt; line.startsWith(query)).collect().forEach(System.out::println);  
    }  
}
</code></pre>
<pre><code>Hive
Hadoop
</code></pre>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import scala.Tuple2;  
  
import java.io.Serializable;  
import java.util.Arrays;  
import java.util.List;  
import java.util.Map;  
  
public class Spark02_Operate_Action {  
    public static void main(String[] args) throws InterruptedException {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setMaster("local[2]");  
        conf.setAppName("sparkCore");  
  
        // 2. 创建sparkContext  
        JavaSparkContext sc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        List&lt;String&gt; list = Arrays.asList("Hive", "Hadoop", "Spark", "Flink");  
        JavaRDD&lt;String&gt; rdd = sc.parallelize(list, 2);  
  
        new Search("H").match(rdd);  
  
        // 4. 关闭sc  
        sc.stop();  
  
    }  
}  
class Search{  
    private String query;  
    public Search(String query){  
        this.query = query;  
    }  
    public void match(JavaRDD&lt;String&gt; rdd){  
        String keyword = this.query;  
        rdd.filter(line -&gt; line.startsWith(keyword)).collect().forEach(System.out::println);  
    }  
}
</code></pre>
<p>String默认实现了Serializable接口</p>
<pre><code class="language-java">SparkConf conf = new SparkConf();  
conf.setMaster("local[2]");  
conf.setAppName("sparkCore");  
  
// 2. 创建sparkContext  
JavaSparkContext sc = new JavaSparkContext(conf);  
  
// 3. 编写代码  
List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4);  
JavaRDD&lt;Integer&gt; rdd = sc.parallelize(nums, 2);  
  
rdd.collect().forEach(System.out::println);  
System.out.println("--------------------------");  
rdd.foreach(System.out::println);  
System.out.println("------------------");  
rdd.foreachPartition(  
        list -&gt; {  
            System.out.println(list);  
        }  
);  
  
// 4. 关闭sc  
sc.stop();
</code></pre>
<p>执行会报错<code>Exception in thread "main" org.apache.spark.SparkException: Task not serializable</code></p>
<p>错误原因：foreach中System.out是一个Driver端的对象</p>
<pre><code class="language-java">PrintStream out = System.out;  
rdd.foreachPartition(  
        out::println  
);
</code></pre>
<p>Java1.8的函数式编程是通过对象模拟出来的，不是真正的函数式编程。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="spark内核"><a class="header" href="#spark内核">Spark内核</a></h1>
<ul>
<li><a href="Spark/SparkCore.html#%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B">运行流程</a></li>
<li><a href="Spark/SparkCore.html#%E6%A0%B8%E5%BF%83%E5%AF%B9%E8%B1%A1">核心对象</a></li>
<li><a href="Spark/SparkCore.html#%E6%A0%B8%E5%BF%83%E5%AF%B9%E8%B1%A1%E9%80%9A%E4%BF%A1%E6%B5%81%E7%A8%8Bnetty">核心对象通信流程——Netty</a></li>
<li><a href="Spark/SparkCore.html#task%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%B0%83%E5%BA%A6%E6%89%A7%E8%A1%8C">Task任务的调度执行</a></li>
<li><a href="Spark/SparkCore.html#shuffle%E5%BA%95%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86">Shuffle底层的实现原理</a></li>
<li><a href="Spark/SparkCore.html#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86">内存管理</a></li>
</ul>
<h2 id="运行流程"><a class="header" href="#运行流程">运行流程</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116141936.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116150729.png" alt="" /></p>
<p><code>YarnClusterApplication.class</code></p>
<pre><code class="language-java">package org.apache.spark.deploy.yarn;  
  
import org.apache.spark.SparkConf;  
import org.apache.spark.deploy.SparkApplication;  
import org.apache.spark.internal.config.package.;  
import org.apache.spark.rpc.RpcEnv;  
import scala.reflect.ScalaSignature;  
  
@ScalaSignature(  
    bytes = "\u0006\u0001e2Qa\u0001\u0003\u0001\u00119AQ!\u0007\u0001\u0005\u0002mAQA\b\u0001\u0005B}\u0011a#W1s]\u000ecWo\u001d;fe\u0006\u0003\b\u000f\\5dCRLwN\u001c\u0006\u0003\u000b\u0019\tA!_1s]*\u0011q\u0001C\u0001\u0007I\u0016\u0004Hn\\=\u000b\u0005%Q\u0011!B:qCJ\\'BA\u0006\r\u0003\u0019\t\u0007/Y2iK*\tQ\"A\u0002pe\u001e\u001c2\u0001A\b\u0016!\t\u00012#D\u0001\u0012\u0015\u0005\u0011\u0012!B:dC2\f\u0017B\u0001\u000b\u0012\u0005\u0019\te.\u001f*fMB\u0011acF\u0007\u0002\r%\u0011\u0001D\u0002\u0002\u0011'B\f'o[!qa2L7-\u0019;j_:\fa\u0001P5oSRt4\u0001\u0001\u000b\u00029A\u0011Q\u0004A\u0007\u0002\t\u0005)1\u000f^1siR\u0019\u0001eI\u001a\u0011\u0005A\t\u0013B\u0001\u0012\u0012\u0005\u0011)f.\u001b;\t\u000b\u0011\u0012\u0001\u0019A\u0013\u0002\t\u0005\u0014xm\u001d\t\u0004!\u0019B\u0013BA\u0014\u0012\u0005\u0015\t%O]1z!\tI\u0003G\u0004\u0002+]A\u00111&amp;E\u0007\u0002Y)\u0011QFG\u0001\u0007yI|w\u000e\u001e \n\u0005=\n\u0012A\u0002)sK\u0012,g-\u0003\u00022e\t11\u000b\u001e:j]\u001eT!aL\t\t\u000bQ\u0012\u0001\u0019A\u001b\u0002\t\r|gN\u001a\t\u0003m]j\u0011\u0001C\u0005\u0003q!\u0011\u0011b\u00159be.\u001cuN\u001c4"  
)  
public class YarnClusterApplication implements SparkApplication {  
    public void start(final String[] args, final SparkConf conf) {  
        conf.remove(.MODULE$.JARS());  
        conf.remove(.MODULE$.FILES());  
        conf.remove(.MODULE$.ARCHIVES());  
        (new Client(new ClientArguments(args), conf, (RpcEnv)null)).run();  
    }  
  
    public YarnClusterApplication() {  
    }  
}
</code></pre>
<h2 id="核心对象"><a class="header" href="#核心对象">核心对象</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116153352.png" alt="" /></p>
<h2 id="核心对象通信流程netty"><a class="header" href="#核心对象通信流程netty">核心对象通信流程——Netty</a></h2>
<p>Java通信方式</p>
<p>Java io=&gt; BIO（阻塞式IO）</p>
<ul>
<li>FileInputStream</li>
<li>FileOutputStream</li>
<li>Socket</li>
</ul>
<p>Java nio =&gt; NIO + (Linux Epoll)（非阻塞式IO+模拟异步）</p>
<ul>
<li>Kafka
<ul>
<li>FileChannel</li>
<li>Selector</li>
</ul>
</li>
</ul>
<p>Java AIO =&gt; AIO（异步）</p>
<p>Netty使用的就是NIO + (Linux Epoll)方式。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116153308.png" alt="" /></p>
<pre><code class="language-java">public class Inbox implements Logging {  
    private final String endpointName;  
    private final RpcEndpoint endpoint;  
    @GuardedBy("this")  
    private final LinkedList&lt;InboxMessage&gt; messages;  
    @GuardedBy("this")  
    private boolean stopped;  
    @GuardedBy("this")  
    private boolean enableConcurrent;  
    @GuardedBy("this")  
    private int numActiveThreads;  
    private transient Logger org$apache$spark$internal$Logging$$log_;
}
</code></pre>
<pre><code class="language-java">public class Outbox {  
    public final NettyRpcEnv org$apache$spark$rpc$netty$Outbox$$nettyEnv;  
    private final RpcAddress address;  
    @GuardedBy("this")  
    private final LinkedList&lt;OutboxMessage&gt; messages;  
    @GuardedBy("this")  
    private TransportClient org$apache$spark$rpc$netty$Outbox$$client;  
    @GuardedBy("this")  
    private Future&lt;BoxedUnit&gt; org$apache$spark$rpc$netty$Outbox$$connectFuture;  
    @GuardedBy("this")  
    private boolean org$apache$spark$rpc$netty$Outbox$$stopped;  
    @GuardedBy("this")  
    private boolean draining;
}
</code></pre>
<h2 id="task任务的调度执行"><a class="header" href="#task任务的调度执行">Task任务的调度执行</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116155452.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116160035.png" alt="" /></p>
<p>DAGSheduler.java</p>
<pre><code class="language-java">private ResultStage createResultStage(final RDD&lt;?&gt; rdd, final Function2&lt;TaskContext, Iterator&lt;?&gt;, ?&gt; func, final int[] partitions, final int jobId, final CallSite callSite) {
        Tuple2 var8 = this.getShuffleDependenciesAndResourceProfiles(rdd);
        if (var8 != null) {
            HashSet shuffleDeps = (HashSet)var8._1();
            HashSet resourceProfiles = (HashSet)var8._2();
            Tuple2 var7 = new Tuple2(shuffleDeps, resourceProfiles);
            HashSet shuffleDeps = (HashSet)var7._1();
            HashSet resourceProfiles = (HashSet)var7._2();
            ResourceProfile resourceProfile = this.mergeResourceProfilesForStage(resourceProfiles);
            this.checkBarrierStageWithDynamicAllocation(rdd);
            this.checkBarrierStageWithNumSlots(rdd, resourceProfile);
            this.checkBarrierStageWithRDDChainPattern(rdd, (new ArrayOps.ofInt(scala.Predef..MODULE$.intArrayOps(partitions))).toSet().size());
            List parents = this.getOrCreateParentStages(shuffleDeps, jobId);
            int id = this.nextStageId().getAndIncrement();
            ResultStage stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite, resourceProfile.id());
            this.stageIdToStage().update(BoxesRunTime.boxToInteger(id), stage);
            this.updateJobIdStageIdMaps(jobId, stage);
            return stage;
        } else {
            throw new MatchError(var8);
        }
    }



public void handleJobSubmitted(final int jobId, final RDD&lt;?&gt; finalRDD, final Function2&lt;TaskContext, Iterator&lt;?&gt;, ?&gt; func, final int[] partitions, final CallSite callSite, final JobListener listener, final JobArtifactSet artifacts, final Properties properties) {
        ObjectRef finalStage = ObjectRef.create((Object)null);

        try {
            finalStage.elem = this.createResultStage(finalRDD, func, partitions, jobId, callSite);
        } catch (BarrierJobSlotsNumberCheckFailed var18) {
            int numCheckFailures = BoxesRunTime.unboxToInt(this.barrierJobIdToNumTasksCheckFailures().compute(BoxesRunTime.boxToInteger(jobId), (x$22, value) -&gt; BoxesRunTime.boxToInteger($anonfun$handleJobSubmitted$1(BoxesRunTime.unboxToInt(x$22), BoxesRunTime.unboxToInt(value)))));
            this.logWarning(() -&gt; (new StringBuilder(92)).append("Barrier stage in job ").append(jobId).append(" requires ").append(var18.requiredConcurrentTasks()).append(" slots, ").append("but only ").append(var18.maxConcurrentTasks()).append(" are available. ").append("Will retry up to ").append(this.maxFailureNumTasksCheck() - numCheckFailures + 1).append(" more times").toString());
            if (numCheckFailures &lt;= this.maxFailureNumTasksCheck()) {
                this.messageScheduler().schedule(new Runnable(jobId, finalRDD, func, partitions, callSite, listener, artifacts, properties) {
                    private final int jobId$3;
                    private final RDD finalRDD$1;
                    private final Function2 func$1;
                    private final int[] partitions$1;
                    private final CallSite callSite$2;
                    private final JobListener listener$1;
                    private final JobArtifactSet artifacts$1;
                    private final Properties properties$1;

                    public void run() {
                        this.$outer.eventProcessLoop().post(new JobSubmitted(this.jobId$3, this.finalRDD$1, this.func$1, this.partitions$1, this.callSite$2, this.listener$1, this.artifacts$1, this.properties$1));
                    }

                    public {
                        if (DAGScheduler.this == null) {
                            throw null;
                        } else {
                            this.$outer = DAGScheduler.this;
                            this.jobId$3 = jobId$3;
                            this.finalRDD$1 = finalRDD$1;
                            this.func$1 = func$1;
                            this.partitions$1 = partitions$1;
                            this.callSite$2 = callSite$2;
                            this.listener$1 = listener$1;
                            this.artifacts$1 = artifacts$1;
                            this.properties$1 = properties$1;
                        }
                    }
                }, this.timeIntervalNumTasksCheck(), TimeUnit.SECONDS);
                return;
            }

            this.barrierJobIdToNumTasksCheckFailures().remove(BoxesRunTime.boxToInteger(jobId));
            listener.jobFailed(var18);
            return;
        } catch (Exception var19) {
            this.logWarning(() -&gt; (new StringBuilder(50)).append("Creating new stage failed due to exception - job: ").append(jobId).toString(), var19);
            listener.jobFailed(var19);
            return;
        }

        this.barrierJobIdToNumTasksCheckFailures().remove(BoxesRunTime.boxToInteger(jobId));
        ActiveJob job = new ActiveJob(jobId, (ResultStage)finalStage.elem, callSite, listener, artifacts, properties);
        this.clearCacheLocs();
        this.logInfo(() -&gt; (new StringOps(scala.Predef..MODULE$.augmentString("Got job %s (%s) with %d output partitions"))).format(scala.Predef..MODULE$.genericWrapArray(new Object[]{BoxesRunTime.boxToInteger(job.jobId()), callSite.shortForm(), BoxesRunTime.boxToInteger(partitions.length)})));
        this.logInfo(() -&gt; (new StringBuilder(16)).append("Final stage: ").append((ResultStage)finalStage.elem).append(" (").append(((ResultStage)finalStage.elem).name()).append(")").toString());
        this.logInfo(() -&gt; (new StringBuilder(24)).append("Parents of final stage: ").append(((ResultStage)finalStage.elem).parents()).toString());
        this.logInfo(() -&gt; (new StringBuilder(17)).append("Missing parents: ").append(this.getMissingParentStages((ResultStage)finalStage.elem)).toString());
        long jobSubmissionTime = this.clock.getTimeMillis();
        this.jobIdToActiveJob().update(BoxesRunTime.boxToInteger(jobId), job);
        this.activeJobs().$plus$eq(job);
        ((ResultStage)finalStage.elem).setActiveJob(job);
        int[] stageIds = (int[])((TraversableOnce)this.jobIdToStageIds().apply(BoxesRunTime.boxToInteger(jobId))).toArray(scala.reflect.ClassTag..MODULE$.Int());
        StageInfo[] stageInfos = (StageInfo[])(new ArrayOps.ofInt(scala.Predef..MODULE$.intArrayOps(stageIds))).flatMap((id) -&gt; $anonfun$handleJobSubmitted$8(this, BoxesRunTime.unboxToInt(id)), scala.Array..MODULE$.canBuildFrom(scala.reflect.ClassTag..MODULE$.apply(StageInfo.class)));
        this.listenerBus.post(new SparkListenerJobStart(job.jobId(), jobSubmissionTime, scala.Predef..MODULE$.wrapRefArray((Object[])stageInfos), org.apache.spark.util.Utils..MODULE$.cloneProperties(properties)));
        this.submitStage((ResultStage)finalStage.elem);
    }
</code></pre>
<p><code>Tuple2 var8 = this.getShuffleDependenciesAndResourceProfiles(rdd);</code></p>
<p>Pool.class</p>
<pre><code class="language-java">public Pool(final String poolName, final Enumeration.Value schedulingMode, final int initMinShare, final int initWeight) {
        label30: {
            Object var11;
            label29: {
                label32: {
                    this.poolName = poolName;
                    this.schedulingMode = schedulingMode;
                    super();
                    Logging.$init$(this);
                    this.schedulableQueue = new ConcurrentLinkedQueue();
                    this.schedulableNameToSchedulable = new ConcurrentHashMap();
                    this.weight = initWeight;
                    this.minShare = initMinShare;
                    this.runningTasks = 0;
                    this.priority = 0;
                    this.stageId = -1;
                    this.name = poolName;
                    this.parent = null;
                    Enumeration.Value var10001 = org.apache.spark.scheduler.SchedulingMode..MODULE$.FAIR();
                    if (var10001 == null) {
                        if (schedulingMode == null) {
                            break label32;
                        }
                    } else if (var10001.equals(schedulingMode)) {
                        break label32;
                    }

                    var10001 = org.apache.spark.scheduler.SchedulingMode..MODULE$.FIFO();
                    if (var10001 == null) {
                        if (schedulingMode != null) {
                            break label30;
                        }
                    } else if (!var10001.equals(schedulingMode)) {
                        break label30;
                    }

                    var11 = new FIFOSchedulingAlgorithm();
                    break label29;
                }

                var11 = new FairSchedulingAlgorithm();
            }

            this.taskSetSchedulingAlgorithm = (SchedulingAlgorithm)var11;
            return;
        }
</code></pre>
<h2 id="shuffle底层的实现原理"><a class="header" href="#shuffle底层的实现原理">Shuffle底层的实现原理</a></h2>
<p>以Task为单位可能存在数据安全问题、小文件过多问题。</p>
<p>以CPU核为单位也存在小文件过多问题。</p>
<p>实际：以Task为单位，但尽量避免可能存在的数据安全问题。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116164756.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116164729.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116163109.png" alt="" /></p>
<p>SortShuffleManager.class</p>
<pre><code class="language-java">public &lt;K, V, C&gt; ShuffleHandle registerShuffle(final int shuffleId, final ShuffleDependency&lt;K, V, C&gt; dependency) {
        if (.MODULE$.shouldBypassMergeSort(this.conf, dependency)) {
            return new BypassMergeSortShuffleHandle(shuffleId, dependency);
        } else {
            return (ShuffleHandle)(SortShuffleManager$.MODULE$.canUseSerializedShuffle(dependency) ? new SerializedShuffleHandle(shuffleId, dependency) : new BaseShuffleHandle(shuffleId, dependency));
        }
    }
</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116164652.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116164841.png" alt="" /></p>
<p><code>reduceByKey()</code>有预聚合</p>
<p><code>groupByKey()</code>没有预聚合</p>
<h2 id="内存管理"><a class="header" href="#内存管理">内存管理</a></h2>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116165936.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250116181019.png" alt="" /></p>
<p>堆外内存：JVM管理不了（操作系统）的内存，称之为堆外内存(Unsafe)</p>
<p>堆内内存：JVM管理的内存，称之为堆内内存</p>
<p>Spark封装了堆外内存和堆内内存。</p>
<p>非堆内存：栈内存，方法区内存</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="spark-core实战"><a class="header" href="#spark-core实战">Spark Core实战</a></h1>
<p>思路</p>
<p>开发原则</p>
<ol>
<li>多什么，删什么（减小数据规模）</li>
<li>缺什么，补什么</li>
<li>功能实现中尽可能少用Shuffle</li>
</ol>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.api.java.JavaPairRDD;  
import org.apache.spark.api.java.JavaRDD;  
import org.apache.spark.api.java.JavaSparkContext;  
import org.jetbrains.annotations.NotNull;  
import scala.Tuple2;  
  
import java.io.Serializable;  
import java.util.ArrayList;  
import java.util.Arrays;  
import java.util.List;  
  
public class SparkHandsOn {  
    public static void main(String[] args) {  
        // 1.创建配置对象  
        SparkConf conf = new SparkConf();  
        conf.setAppName("Spark01_Env");  
        conf.setMaster("local[*]");  
  
        // 2. 创建sparkContext  
        JavaSparkContext jsc = new JavaSparkContext(conf);  
  
        // 3. 编写代码  
        JavaRDD&lt;String&gt; rdd = jsc.textFile("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user_visit_action.txt");  
        // 过滤掉搜索数据，减小数据规模  
        JavaRDD&lt;String&gt; filterRDD = rdd.filter(line -&gt; {  
            String[] items = line.split("_");  
            return "null".equals(items[5]);  
        });  
  
        // 分组统计  
        JavaRDD&lt;CategoryCountInfo&gt; categoryCountInfoJavaRDD = filterRDD.flatMap(line -&gt; {  
            String[] items = line.split("_");  
            if (!"-1".equals(items[6])) {  
                return Arrays.asList(new CategoryCountInfo(items[6], 1L, 0L, 0L)).iterator();  
            } else if (!"null".equals(items[8])) {  
                List&lt;CategoryCountInfo&gt; categoryCountInfoList = new ArrayList&lt;&gt;();  
                String[] ids = items[8].split(",");  
                for (String categoryId : ids) {  
                    categoryCountInfoList.add(new CategoryCountInfo(categoryId, 0L, 1L, 0L));  
                }  
                return categoryCountInfoList.iterator();  
            } else if (!"null".equals(items[10])) {  
                List&lt;CategoryCountInfo&gt; categoryCountInfoList = new ArrayList&lt;&gt;();  
                String[] ids = items[10].split(",");  
                for (String categoryId : ids) {  
                    categoryCountInfoList.add(new CategoryCountInfo(categoryId, 0L, 0L, 1L));  
                }  
                return categoryCountInfoList.iterator();  
            } else {  
                System.out.println("数据格式错误：" + line);  
                return null;  
            }  
        });  
  
        JavaPairRDD&lt;String, CategoryCountInfo&gt; stringCategoryCountInfoJavaPairRDD = categoryCountInfoJavaRDD.  
                mapToPair(pair -&gt; new Tuple2&lt;&gt;(pair.getCategoryId(), pair))  
                .reduceByKey((a, b) -&gt; {  
                    a.setClickCount(a.getClickCount() + b.getClickCount());  
                    a.setOrderCount(a.getOrderCount() + b.getOrderCount());  
                    a.setPayCount(a.getPayCount() + b.getPayCount());  
                    return a;  
                });  
  
        JavaRDD&lt;CategoryCountInfo&gt; mapRDD = stringCategoryCountInfoJavaPairRDD.map(pair -&gt; pair._2);  
        JavaRDD&lt;CategoryCountInfo&gt; sortRDD = mapRDD.sortBy(obj -&gt; obj, false, 2);  
        List&lt;CategoryCountInfo&gt; takeSort = sortRDD.take(10);  
        for (CategoryCountInfo categoryCountInfo : takeSort) {  
            System.out.println(categoryCountInfo);  
        }  
  
        // 4. 关闭sc  
        jsc.stop();  
    }  
}  
  
class CategoryCountInfo implements Comparable&lt;CategoryCountInfo&gt;, Serializable {  
    private String categoryId;  
    private Long clickCount;  
    private Long orderCount;  
    private Long payCount;  
  
    public CategoryCountInfo() {  
    }  
  
    public CategoryCountInfo(String categoryId, Long clickCount, Long orderCount, Long payCount) {  
        this.categoryId = categoryId;  
        this.clickCount = clickCount;  
        this.orderCount = orderCount;  
        this.payCount = payCount;  
    }  
  
    public String getCategoryId() {  
        return categoryId;  
    }  
  
    public void setCategoryId(String categoryId) {  
        this.categoryId = categoryId;  
    }  
  
    public Long getClickCount() {  
        return clickCount;  
    }  
  
    public void setClickCount(Long clickCount) {  
        this.clickCount = clickCount;  
    }  
  
    public Long getOrderCount() {  
        return orderCount;  
    }  
  
    public void setOrderCount(Long orderCount) {  
        this.orderCount = orderCount;  
    }  
  
    public Long getPayCount() {  
        return payCount;  
    }  
  
    public void setPayCount(Long payCount) {  
        this.payCount = payCount;  
    }  
  
    @Override  
    public String toString() {  
        return "CategoryCountInfo{" +  
                "categoryId='" + categoryId + '\'' +  
                ", clickCount=" + clickCount +  
                ", orderCount=" + orderCount +  
                ", payCount=" + payCount +  
                '}';  
    }  
  
    @Override  
    public int compareTo(@NotNull CategoryCountInfo other) {  
        if (this.clickCount &gt; other.clickCount) {  
            return 1;  
        } else if (this.clickCount &lt; other.clickCount) {  
            return -1;  
        } else {  
            if (this.orderCount &gt; other.orderCount) {  
                return 1;  
            } else if (this.orderCount &lt; other.orderCount) {  
                return -1;  
            } else {  
                return this.payCount.compareTo(other.payCount);  
            }  
        }  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>CategoryCountInfo{categoryId='15', clickCount=6120, orderCount=1672, payCount=1259}
CategoryCountInfo{categoryId='2', clickCount=6119, orderCount=1767, payCount=1196}
CategoryCountInfo{categoryId='20', clickCount=6098, orderCount=1776, payCount=1244}
CategoryCountInfo{categoryId='12', clickCount=6095, orderCount=1740, payCount=1218}
CategoryCountInfo{categoryId='11', clickCount=6093, orderCount=1781, payCount=1202}
CategoryCountInfo{categoryId='17', clickCount=6079, orderCount=1752, payCount=1231}
CategoryCountInfo{categoryId='7', clickCount=6074, orderCount=1796, payCount=1252}
CategoryCountInfo{categoryId='9', clickCount=6045, orderCount=1736, payCount=1230}
CategoryCountInfo{categoryId='19', clickCount=6044, orderCount=1722, payCount=1158}
CategoryCountInfo{categoryId='13', clickCount=6036, orderCount=1781, payCount=1161}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="sparksql"><a class="header" href="#sparksql">SparkSQL</a></h1>
<ul>
<li><a href="Spark/SparkSQL.html#sparksql-%E7%AE%80%E4%BB%8B">SparkSQL 简介</a>
<ul>
<li><a href="Spark/SparkSQL.html#1-sparksql-%E7%9A%84%E7%89%B9%E7%82%B9">1. SparkSQL 的特点</a></li>
<li><a href="Spark/SparkSQL.html#2-sparksql-%E7%9A%84%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86">2. SparkSQL 的组成部分</a>
<ul>
<li><a href="Spark/SparkSQL.html#21-dataframe">2.1 DataFrame</a></li>
<li><a href="Spark/SparkSQL.html#22-dataset">2.2 Dataset</a></li>
<li><a href="Spark/SparkSQL.html#23-catalyst-optimizer">2.3 Catalyst Optimizer</a></li>
<li><a href="Spark/SparkSQL.html#24-tungsten-%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E">2.4 Tungsten 执行引擎</a></li>
</ul>
</li>
<li><a href="Spark/SparkSQL.html#3-%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95">3. 基本用法</a>
<ul>
<li><a href="Spark/SparkSQL.html#31-%E5%88%9B%E5%BB%BA-sparksession">3.1 创建 SparkSession</a></li>
<li><a href="Spark/SparkSQL.html#32-%E5%88%9B%E5%BB%BA-dataframe">3.2 创建 DataFrame</a></li>
<li><a href="Spark/SparkSQL.html#33-sql-%E6%9F%A5%E8%AF%A2">3.3 SQL 查询</a></li>
<li><a href="Spark/SparkSQL.html#34-dataframe-%E6%93%8D%E4%BD%9C">3.4 DataFrame 操作</a></li>
<li><a href="Spark/SparkSQL.html#35-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE">3.5 写入数据</a></li>
</ul>
</li>
<li><a href="Spark/SparkSQL.html#4-%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81">4. 示例代码</a></li>
<li><a href="Spark/SparkSQL.html#5-%E7%BB%93%E8%AE%BA">5. 结论</a></li>
</ul>
</li>
<li><a href="Spark/SparkSQL.html#sparksql-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">SparkSQl 环境搭建</a>
<ul>
<li><a href="Spark/SparkSQL.html#%E6%9D%A5%E6%BA%90">来源</a></li>
<li><a href="Spark/SparkSQL.html#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">环境搭建</a></li>
<li><a href="Spark/SparkSQL.html#%E6%A8%A1%E5%9E%8B">模型</a></li>
<li><a href="Spark/SparkSQL.html#%E8%A7%A3%E5%86%B3%E6%8A%A5%E9%94%99job-aborted-due-to-stage-failure">解决报错<code>Job aborted due to stage failure</code></a></li>
</ul>
</li>
<li><a href="Spark/SparkSQL.html#%E4%B8%8D%E5%90%8C%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%8E%AF%E5%A2%83%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%BD%AC%E6%8D%A2">不同场景下环境对象的转换</a>
<ul>
<li><a href="Spark/SparkSQL.html#%E7%8E%AF%E5%A2%83%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2">环境之间的转换</a></li>
<li><a href="Spark/SparkSQL.html#coresparkcontext--sqlsparksession">Core:SparkContext-&gt; SQL:SparkSession</a>
<ul>
<li><a href="Spark/SparkSQL.html#sqlsparksession--coresparkcontext">SQL:SparkSession-&gt; Core:SparkContext</a></li>
<li><a href="Spark/SparkSQL.html#sqlsparksession---corejavasparkcontext">SQL:SparkSession -&gt; Core:JavaSparkContext</a></li>
</ul>
</li>
<li><a href="Spark/SparkSQL.html#%E4%B8%8D%E5%90%8C%E5%9C%BA%E6%99%AF%E4%B8%8B%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%BD%AC%E6%8D%A2">不同场景下模型数据对象的转换</a>
<ul>
<li><a href="Spark/SparkSQL.html#rdd">RDD</a></li>
<li><a href="Spark/SparkSQL.html#dataframe">DataFrame</a></li>
<li><a href="Spark/SparkSQL.html#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B1%BB%E5%9E%8B%E4%BB%A3%E7%A0%81%E6%9A%82%E6%97%B6%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E8%BF%90%E8%A1%8C">自定义类型（代码暂时无法正常运行）</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="Spark/SparkSQL.html#%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%AE%BF%E9%97%AE">模型对象的访问</a>
<ul>
<li><a href="Spark/SparkSQL.html#%E4%BD%BF%E7%94%A8sql%E8%AF%AD%E6%B3%95">使用SQL语法</a></li>
<li><a href="Spark/SparkSQL.html#%E4%BD%BF%E7%94%A8dsl%E8%AF%AD%E6%B3%95">使用DSL语法</a></li>
<li><a href="Spark/SparkSQL.html#sql%E6%96%87%E7%9A%84%E7%BC%BA%E9%99%B7concat%E5%92%8C">SQL文的缺陷（concat和+）</a></li>
</ul>
</li>
<li><a href="Spark/SparkSQL.html#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%B9%E6%B3%95udf%E5%92%8Cudaf">自定义方法（UDF和UDAF）</a>
<ul>
<li><a href="Spark/SparkSQL.html#udf">UDF</a></li>
<li><a href="Spark/SparkSQL.html#udaf%E5%8E%9F%E7%90%86%E5%92%8C%E8%87%AA%E5%AE%9A%E4%B9%89%E5%AE%9E%E7%8E%B0">UDAF原理和自定义实现</a></li>
</ul>
</li>
</ul>
<h1 id="sparksql-简介"><a class="header" href="#sparksql-简介">SparkSQL 简介</a></h1>
<p>SparkSQL 是 Spark 生态系统的重要组成部分，用于处理结构化数据。它允许使用 SQL 查询语言访问和管理数据，同时结合了 Spark 的强大分布式计算能力，支持处理大量数据集。以下将详细介绍 SparkSQL 的关键特点、组成部分、基本用法以及一些示例代码。</p>
<h2 id="1-sparksql-的特点"><a class="header" href="#1-sparksql-的特点">1. SparkSQL 的特点</a></h2>
<ul>
<li><strong>统一的数据处理</strong>：既可以使用 DataFrame 和 Dataset API 进行数据操作，也可以用 SQL 语句直接查询数据。</li>
<li><strong>与 Hive 集成</strong>：支持访问 Hive 表和查询，能够与现有 Hive 生态系统无缝集成。</li>
<li><strong>高效的查询执行</strong>：通过使用 Catalyst 查询优化器，提供高效的查询执行计划。</li>
<li><strong>与多种数据源兼容</strong>：支持读取和写入多种数据源，包括 JSON、Parquet、ORC、JDBC、Kafka 等。</li>
<li><strong>可扩展性</strong>：支持分布式计算，可扩展到大规模数据集。</li>
</ul>
<h2 id="2-sparksql-的组成部分"><a class="header" href="#2-sparksql-的组成部分">2. SparkSQL 的组成部分</a></h2>
<h3 id="21-dataframe"><a class="header" href="#21-dataframe">2.1 DataFrame</a></h3>
<ul>
<li>DataFrame 是 SparkSQL 中的核心数据结构，类似于 Pandas 的 DataFrame，具有行和列的结构，表格形式的数据表示。可以通过各种方式创建 DataFrame，例如从 RDD、JSON 文件、Hive 表等。</li>
</ul>
<h3 id="22-dataset"><a class="header" href="#22-dataset">2.2 Dataset</a></h3>
<ul>
<li>Dataset 是结合了 DataFrame 的优点与强类型的 API，属于 SparkSQL 的扩展。它提供了编译时类型检查，借此确保数据的类型安全性。</li>
</ul>
<h3 id="23-catalyst-optimizer"><a class="header" href="#23-catalyst-optimizer">2.3 Catalyst Optimizer</a></h3>
<ul>
<li>Catalyst 是 SparkSQL 的查询优化器，它对 SQL 查询进行分析、优化和编译，确保有效率高的执行计划。</li>
</ul>
<h3 id="24-tungsten-执行引擎"><a class="header" href="#24-tungsten-执行引擎">2.4 Tungsten 执行引擎</a></h3>
<ul>
<li>Tungsten 是 Spark 的一个执行引擎，针对内存管理、物理计划生成等方面进行了优化，能够提高 SparkSQL 的执行性能。</li>
</ul>
<h2 id="3-基本用法"><a class="header" href="#3-基本用法">3. 基本用法</a></h2>
<h3 id="31-创建-sparksession"><a class="header" href="#31-创建-sparksession">3.1 创建 SparkSession</a></h3>
<p>要使用 SparkSQL，首先需要创建一个 <code>SparkSession</code> 对象，它是 Spark 应用程序的入口点。</p>
<pre><code class="language-scala">import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("Spark SQL Example")
  .config("spark.some.config.option", "config-value")
  .getOrCreate()
</code></pre>
<h3 id="32-创建-dataframe"><a class="header" href="#32-创建-dataframe">3.2 创建 DataFrame</a></h3>
<p>可以从多种数据源创建 DataFrame。</p>
<pre><code class="language-scala">// 从 JSON 文件创建 DataFrame
val jsonDF = spark.read.json("path/to/data.json")

// 从 CSV 文件创建 DataFrame
val csvDF = spark.read.option("header", "true").csv("path/to/data.csv")

// 从 RDD 创建 DataFrame
import spark.implicits._
val rdd = spark.sparkContext.parallelize(Seq(("Alice", 1), ("Bob", 2)))
val dfFromRDD = rdd.toDF("name", "id")
</code></pre>
<h3 id="33-sql-查询"><a class="header" href="#33-sql-查询">3.3 SQL 查询</a></h3>
<p>在 DataFrame 上使用 SQL 查询，首先需将 DataFrame 注册为临时视图。</p>
<pre><code class="language-scala">// 注册一个临时视图
jsonDF.createOrReplaceTempView("people")

// 使用 SQL 查询数据
val resultDF = spark.sql("SELECT * FROM people WHERE age &gt; 21")
resultDF.show()
</code></pre>
<h3 id="34-dataframe-操作"><a class="header" href="#34-dataframe-操作">3.4 DataFrame 操作</a></h3>
<p>可以使用 DataFrame API 对数据进行多种操作，包括选择、过滤、分组、聚合等。</p>
<pre><code class="language-scala">// 选择特定列
val selectedDF = jsonDF.select("name", "age")

// 过滤数据
val filteredDF = jsonDF.filter($"age" &gt; 21)

// 分组聚合
val groupByDF = jsonDF.groupBy("age").count()
</code></pre>
<h3 id="35-写入数据"><a class="header" href="#35-写入数据">3.5 写入数据</a></h3>
<p>可以将处理后的 DataFrame 写入不同的数据源。</p>
<pre><code class="language-scala">// 写入为 Parquet 格式
resultDF.write.parquet("path/to/output.parquet")

// 写入为 CSV 格式
resultDF.write.option("header", "true").csv("path/to/output.csv")
</code></pre>
<h2 id="4-示例代码"><a class="header" href="#4-示例代码">4. 示例代码</a></h2>
<p>以下是一个完整的 SparkSQL 示例，涵盖从数据读取、处理到结果写入的全过程：</p>
<pre><code class="language-scala">import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("Spark SQL Example")
  .getOrCreate()

// 1. 读取 JSON 数据
val jsonDF = spark.read.json("path/to/data.json")

// 2. 显示 DataFrame 内容
jsonDF.show()

// 3. 创建临时视图以便使用 SQL
jsonDF.createOrReplaceTempView("people")

// 4. 使用 SQL 查询
val resultDF = spark.sql("SELECT name, age FROM people WHERE age &gt; 21")
resultDF.show()

// 5. 使用 DataFrame API 进行处理
val groupedDF = jsonDF.groupBy("age").count()
groupedDF.show()

// 6. 写入处理结果
groupedDF.write.parquet("path/to/output.parquet")

spark.stop()
</code></pre>
<h2 id="5-结论"><a class="header" href="#5-结论">5. 结论</a></h2>
<p>SparkSQL 是处理结构化数据的一种灵活、高效的方式。它结合了 SQL 的易用性和 Spark 的高性能计算能力，非常适合大数据分析和实时数据处理。</p>
<h1 id="sparksql-环境搭建"><a class="header" href="#sparksql-环境搭建">SparkSQl 环境搭建</a></h1>
<h2 id="来源"><a class="header" href="#来源">来源</a></h2>
<p>Spark + Hive =&gt; Shark =&gt; Spark On Hive =&gt; SparkSQL
Spark + Hive =&gt; Shark =&gt; Hive On Spark =&gt; Hive -&gt; SQL -&gt; RDD</p>
<p>Shark =&gt;Spark On Hive=&gt;SparkSQL=&gt; Spark parse SQL</p>
<p>Shark =&gt;HiveOnSpark=&gt;数据仓库=&gt;Hive parse SQL</p>
<h2 id="环境搭建"><a class="header" href="#环境搭建">环境搭建</a></h2>
<p>没有JavaSparkSession，只有sparkSession</p>
<p>sparkSession底层使用的仍旧是Scala语言</p>
<p>推荐使用构建器模式构建SparkSQL环境对象，少用new</p>
<pre><code class="language-java">import org.apache.spark.sql.SparkSession;  
  
public class SQL01_Env {  
    public static void main(String[] args) {  
        SparkSession sparkSession = SparkSession.builder().appName("SQL01_Env").master("local").getOrCreate();  
        System.out.println("Spark SQL 环境创建成功！");  
        sparkSession.stop();  
        System.out.println("Spark SQL 环境已停止！");  
    }  
}
</code></pre>
<p>思考：close()和stop()有何区别？</p>
<h2 id="模型"><a class="header" href="#模型">模型</a></h2>
<p>SparkSQL中对数据模型也进行了封装：RDD-&gt;Dataset</p>
<p>对接文件数据源时，会将文件中的一行数据封装为Row对象</p>
<pre><code class="language-java">import org.apache.spark.rdd.RDD;  
import org.apache.spark.sql.Dataset;  
import org.apache.spark.sql.Row;  
import org.apache.spark.sql.SparkSession;  
  
public class SQL01_Model {  
    public static void main(String[] args) {  
        SparkSession sparkSession = SparkSession.builder().appName("SQL01_Model").master("local[2]").getOrCreate();  
        System.out.println("Spark SQL 环境创建成功！");  
  
        Dataset&lt;Row&gt; dataset = sparkSession.read().json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user.json");  
        RDD&lt;Row&gt; rdd = dataset.rdd();  
          
        sparkSession.stop();  
        System.out.println("Spark SQL 环境已停止！");  
    }  
}
</code></pre>
<h2 id="解决报错job-aborted-due-to-stage-failure"><a class="header" href="#解决报错job-aborted-due-to-stage-failure">解决报错<code>Job aborted due to stage failure</code></a></h2>
<pre><code>Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (TH executor driver): java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/StreamReadConstraints
	......
</code></pre>
<pre><code>Exception in thread "main" org.apache.spark.sql.AnalysisException: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the
referenced columns only include the internal corrupt record column
(named _corrupt_record by default).
</code></pre>
<p>https://stackoverflow.org.cn/questions/54517775</p>
<p><a href="https://stackoverflow.org.cn/questions/54517775">java - 在 Apache Spark 中解析 JSON 时出现奇怪的错误</a></p>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class SQL03_SQL {
    public static void main(String[] args) {
        // 创建SparkSession
        SparkSession sparkSession = SparkSession.builder()
                .appName("SQL01_Model")
                .master("local[2]")
                .getOrCreate();

        System.out.println("Spark SQL 环境创建成功！");

        // 读取 JSON 数据
        Dataset&lt;Row&gt; dataset = sparkSession.read().format("json").option("multiline", "true")
                .load("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user.json");

        // 注册临时视图
        dataset.createOrReplaceTempView("users");

        // 执行 SQL 查询
        String sqlText = "SELECT * FROM users";
        Dataset&lt;Row&gt; result = sparkSession.sql(sqlText);

        // 展示结果
        result.show();

        // 停止 SparkSession
        sparkSession.stop();
        System.out.println("Spark SQL 环境已停止！");
    }
}
</code></pre>
<pre><code>Spark SQL 环境创建成功！
+---+----+----+
| id|姓名|年龄|
+---+----+----+
|  1|张三|  25|
|  2|李四|  30|
|  3|王五|  22|
+---+----+----+

Spark SQL 环境已停止！
</code></pre>
<p>因为我的JSON格式是多行的，只需要改为一行即可</p>
<pre><code>**{
  "name": "Michael",
  "age": 12
}
{
  "name": "Andy",
  "age": 13
}
{
  "name": "Justin",
  "age": 8
}**
</code></pre>
<p>修改为：</p>
<pre><code>**{"name": "Michael",  "age": 12}
{"name": "Andy",  "age": 13}
{"name": "Justin",  "age": 8}
</code></pre>
<p>我这创建的user.json内容如下所示：</p>
<pre><code class="language-json">[
  {
    "id": 1,
    "姓名": "张三",
    "年龄": 25
  },
  {
    "id": 2,
    "姓名": "李四",
    "年龄": 30
  },
  {
    "id": 3,
    "姓名": "王五",
    "年龄": 22
  }
]
</code></pre>
<p>读取单行</p>
<pre><code class="language-java">Dataset&lt;Row&gt; dataset = sparkSession.read().json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user2.json");
</code></pre>
<pre><code class="language-json">{"id":1,"姓名":"张三","年龄":25}  
{"id":2,"姓名":"李四","年龄":30}  
{"id":3,"姓名":"王五","年龄":22}
</code></pre>
<p>row背后是数组</p>
<pre><code class="language-java">dataset.foreach(  
        row -&gt; {  
            System.out.println(row.getLong(0) + ", " + row.getString(1) + ", " + row.getLong(2));  
        }  
);  
  
//row背后是数组
</code></pre>
<pre><code>Spark SQL 环境创建成功！
1, 张三, 25
2, 李四, 30
3, 王五, 22
+---+----+----+
| id|姓名|年龄|
+---+----+----+
|  1|张三|  25|
|  2|李四|  30|
|  3|王五|  22|
+---+----+----+
</code></pre>
<h1 id="不同场景下环境对象的转换"><a class="header" href="#不同场景下环境对象的转换">不同场景下环境对象的转换</a></h1>
<h2 id="环境之间的转换"><a class="header" href="#环境之间的转换">环境之间的转换</a></h2>
<h2 id="coresparkcontext--sqlsparksession"><a class="header" href="#coresparkcontext--sqlsparksession">Core:SparkContext-&gt; SQL:SparkSession</a></h2>
<pre><code class="language-java">new SparkSession(new SparkContext(conf));
</code></pre>
<h3 id="sqlsparksession--coresparkcontext"><a class="header" href="#sqlsparksession--coresparkcontext">SQL:SparkSession-&gt; Core:SparkContext</a></h3>
<pre><code class="language-java">final SparkContext sparkContext = sparkSession.sparkContext();
sparkcontext.parallelize();
</code></pre>
<h3 id="sqlsparksession---corejavasparkcontext"><a class="header" href="#sqlsparksession---corejavasparkcontext">SQL:SparkSession -&gt; Core:JavaSparkContext</a></h3>
<pre><code class="language-java">final SparkContext sparkContext= sparkSession.sparkContext();
final JavaSparkContext jsc = new JavaSparkContext(sparkContext); 
jsc.parallelize（Arrays.asList(1,2,3,4));
</code></pre>
<h2 id="不同场景下模型数据对象的转换"><a class="header" href="#不同场景下模型数据对象的转换">不同场景下模型数据对象的转换</a></h2>
<h3 id="rdd"><a class="header" href="#rdd">RDD</a></h3>
<pre><code class="language-java">Dataset&lt;Row&gt; dataset = sparkSession.read().json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user.json");  
RDD&lt;Row&gt; rdd = dataset.rdd();
</code></pre>
<h3 id="dataframe"><a class="header" href="#dataframe">DataFrame</a></h3>
<p>读取单行</p>
<pre><code class="language-java">Dataset&lt;Row&gt; dataset = sparkSession.read().json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user2.json");
</code></pre>
<pre><code class="language-json">{"id":1,"姓名":"张三","年龄":25}  
{"id":2,"姓名":"李四","年龄":30}  
{"id":3,"姓名":"王五","年龄":22}
</code></pre>
<p>row背后是数组</p>
<pre><code class="language-java">dataset.foreach(  
        row -&gt; {  
            System.out.println(row.getLong(0) + ", " + row.getString(1) + ", " + row.getLong(2));  
        }  
);  
  
//row背后是数组
</code></pre>
<pre><code>Spark SQL 环境创建成功！
1, 张三, 25
2, 李四, 30
3, 王五, 22
+---+----+----+
| id|姓名|年龄|
+---+----+----+
|  1|张三|  25|
|  2|李四|  30|
|  3|王五|  22|
+---+----+----+
</code></pre>
<h3 id="自定义类型代码暂时无法正常运行"><a class="header" href="#自定义类型代码暂时无法正常运行">自定义类型（代码暂时无法正常运行）</a></h3>
<p>将数据模型中的数据类型进行转换，将Row转换成其他对象进行处理</p>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;  
import org.apache.spark.sql.Encoders;  
import org.apache.spark.sql.Row;  
import org.apache.spark.sql.SparkSession;  
  
import java.io.Serializable;  
  
public class SQL01_Model {  
    public static void main(String[] args) {  
        SparkSession sparkSession = SparkSession.builder().appName("SQL01_Model").master("local[2]").getOrCreate();  
        System.out.println("Spark SQL 环境创建成功！");  
  
        Dataset&lt;Row&gt; dataset = sparkSession.read().json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user2.json");  
  
        Dataset&lt;User&gt; userDataset = dataset.as(Encoders.bean(User.class));  
        userDataset.foreach(  
                user -&gt; {  
                    System.out.println(user.getName() );  
                }  
        );  
  
        sparkSession.stop();  
        System.out.println("Spark SQL 环境已停止！");  
    }  
}  
class User implements Serializable {  
    private String name;  
    private Long age;  
    private Long id;  
  
    public User(String name, Long age, Long id) {  
        this.name = name;  
        this.age = age;  
        this.id = id;  
    }  
    public User() {  
    }  
  
    public String getName() {  
        return name;  
    }  
  
    public void setName(String name) {  
        this.name = name;  
    }  
  
    public Long getAge() {  
        return age;  
    }  
  
    public void setAge(Long age) {  
        this.age = age;  
    }  
  
    public Long getId() {  
        return id;  
    }  
  
    public void setId(Long id) {  
        this.id = id;  
    }  
}
</code></pre>
<p>报错信息：</p>
<pre><code>Caused by: java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 41, Column 8: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 41, Column 8: "com.zzw.bigdata.spark.sparksql.User" is inaccessible from this package
	......

</code></pre>
<p>可能原因：User类不是public的。</p>
<p>具体实践中错误地方：</p>
<pre><code class="language-java">userDataset.foreach(
                user -&gt; {
                    System.out.println(user.getName() );
                }
        );
</code></pre>
<p>执行这个语句就会报错。</p>
<h1 id="模型对象的访问"><a class="header" href="#模型对象的访问">模型对象的访问</a></h1>
<h2 id="使用sql语法"><a class="header" href="#使用sql语法">使用SQL语法</a></h2>
<p>将数据模型转换为二维的结构（行，列），可以通过SQL文进行访问视图：是表的查询结果集。表可以增加，修改，删除，查询。</p>
<p>视图不能增加，不能修改，不能删除，只能查询</p>
<p><code>ds.createOrReplaceTempView(viewName:"user");</code></p>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;  
import org.apache.spark.sql.Encoders;  
import org.apache.spark.sql.Row;  
import org.apache.spark.sql.SparkSession;  
  
public class SQL01_Model_1 {  
    public static void main(String[] args) {  
        SparkSession sparkSession = SparkSession.builder().appName("SQL01_Model_1").master("local[2]").getOrCreate();  
        System.out.println("Spark SQL 环境创建成功！");  
  
        Dataset&lt;Row&gt; dataset = sparkSession.read().json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user2.json");  
  
        dataset.createOrReplaceTempView("user");  
  
        Dataset&lt;Row&gt; result = sparkSession.sql("SELECT * FROM user");  
  
        result.show();  
  
        sparkSession.stop();  
        System.out.println("Spark SQL 环境已停止！");  
    }  
}
</code></pre>
<p>JDK1.8字符串不能跨行</p>
<h2 id="使用dsl语法"><a class="header" href="#使用dsl语法">使用DSL语法</a></h2>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;  
import org.apache.spark.sql.Encoders;  
import org.apache.spark.sql.Row;  
import org.apache.spark.sql.SparkSession;  
  
public class SQL01_Model_1 {  
    public static void main(String[] args) {  
        SparkSession sparkSession = SparkSession.builder().appName("SQL01_Model_1").master("local[2]").getOrCreate();  
        System.out.println("Spark SQL 环境创建成功！");  
  
        Dataset&lt;Row&gt; dataset = sparkSession.read().json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user2.json");  
  
        dataset.select("*").show();  
  
        sparkSession.stop();  
        System.out.println("Spark SQL 环境已停止！");  
    }  
}
</code></pre>
<h2 id="sql文的缺陷concat和"><a class="header" href="#sql文的缺陷concat和">SQL文的缺陷（concat和+）</a></h2>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class SQL03_SQL3 {
    public static void main(String[] args) {
        // 创建SparkSession
        SparkSession sparkSession = SparkSession.builder()
                .appName("SQL01_Model3")
                .master("local[2]")
                .getOrCreate();

        System.out.println("Spark SQL 环境创建成功！");

        // 读取 JSON 数据
        Dataset&lt;Row&gt; dataset = sparkSession.read()
                .json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user2.json");

        // 注册临时视图
        dataset.createOrReplaceTempView("users");

        // 执行 SQL 查询
        String sqlText = "SELECT concat('first_name',name) AS full_name FROM users";
        Dataset&lt;Row&gt; result = sparkSession.sql(sqlText);

        // 展示结果
        result.show();

        // 停止 SparkSession
        sparkSession.stop();
        System.out.println("Spark SQL 环境已停止！");
    }
}
</code></pre>
<p>concat不通用</p>
<pre><code class="language-java">String sql="select concat('Name:',name) from user";
//String sql="select'Name:'Ilname from user";//mysql，oracle(ll)，db2，sqlserver

final Dataset&lt;Row&gt; sqlDS = sparkSession.sql(sql); sqLDS.show();
</code></pre>
<h1 id="自定义方法udf和udaf"><a class="header" href="#自定义方法udf和udaf">自定义方法（UDF和UDAF）</a></h1>
<p>SparkSQL提供了一种特殊的方式，可以在SQL中增加自定义方法来实现复杂的逻辑</p>
<h2 id="udf"><a class="header" href="#udf">UDF</a></h2>
<p>如果想要自定义的方法能够在SQL中使用，那么必须在SPark中进行声明和注册</p>
<p>register方法需要传递3个参数</p>
<p>第一个参数表示SQL中使用的方法名</p>
<p>第二个参数表示逻辑：IN=&gt;OUT</p>
<p>第三个参数表示返回的数据类型：DataType类型数据，需要使用scala语法操作，需要特殊的使用方式。</p>
<pre><code class="language-java">sparkSession.udf().register(name:"prefixName",new UDF1&lt;String, String&gt;() {
@Override
public String call(String name) throws Exception {
return "Name:"+ name;}
}，StringType$.MODULE$);

String sql="select prefixName(name) from user"; 
final Dataset&lt;Row&gt; sqlDS = sparkSession.sql(sql); 
sqLDS.show();
</code></pre>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;  
import org.apache.spark.sql.Row;  
import org.apache.spark.sql.SparkSession;  
import org.apache.spark.sql.types.StringType$;  
  
public class SQL03_SQL3 {  
    public static void main(String[] args) {  
        // 创建SparkSession  
        SparkSession sparkSession = SparkSession.builder()  
                .appName("SQL01_Model3")  
                .master("local[2]")  
                .getOrCreate();  
  
        System.out.println("Spark SQL 环境创建成功！");  
  
        // 读取 JSON 数据  
        Dataset&lt;Row&gt; dataset = sparkSession.read()  
                .json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user2.json");  
  
        // 注册临时视图  
        dataset.createOrReplaceTempView("users");  
  
        sparkSession.udf().register("prefixName", (String name) -&gt; "first_name" + name, StringType$.MODULE$);  
  
        // 执行 SQL 查询  
        String sqlText = "SELECT prefixName(name) AS full_name FROM users";  
        Dataset&lt;Row&gt; result = sparkSession.sql(sqlText);  
  
        // 展示结果  
        result.show();  
  
        // 停止 SparkSession        sparkSession.stop();  
        System.out.println("Spark SQL 环境已停止！");  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>Spark SQL 环境创建成功！
+--------------+
|     full_name|
+--------------+
|first_name张三|
|first_name李四|
|first_name王五|
+--------------+

Spark SQL 环境已停止！
</code></pre>
<p><code>sparkSession.udf().register("prefixName", (String name) -&gt; "first_name" + name, StringType$.MODULE$); </code>另一种写法<code>sparkSession.udf().register("prefixName", (String name) -&gt; "first_name" + name, DataTypes.StringType);</code></p>
<p>也可以静态导入</p>
<h2 id="udaf原理和自定义实现"><a class="header" href="#udaf原理和自定义实现">UDAF原理和自定义实现</a></h2>
<p>每行数据都用一次UDF函数，类似map</p>
<p>所有数据共用一次UDAF函数，类似reduce</p>
<p>UDAF函数底层实现中需要存在一个缓冲区，用于临时存放数据</p>
<p>SparkSQL采用特殊的方式将UDAF转换成UDF使用</p>
<p>UDAF使用时需要创建自定义聚合对象 udaf方法需要传递2个参数</p>
<p>第一个参数表示UDAF对象</p>
<p>第二个参数表示UDAF对象</p>
<pre><code class="language-java">sparkSession.udf().register(name:"avgAge",functions.udaf(
new MyAvgAgeUDAF()，Encoders.LoNG()
);
String sql ="select avgAge(age) from user";
final Dataset&lt;Row&gt; sqlDS = sparkSession.sql(sql);
sqLDS.show();
</code></pre>
<p>自定义UDAF函数，实现年龄的平均值</p>
<p>1.创建自定义的类</p>
<p>2.继承 <code>org.apache.spark.sql.expressions.Aggregator</code></p>
<p>3．设定泛型</p>
<p>IN：输入数据类型</p>
<p>BUF：缓冲区的数据类型</p>
<p>OUT：输出数据类型</p>
<p>4.重写方法（4（计算）+2（状态））</p>
<p>这里需要使用<code>import static org.apache.spark.sql.functions.udaf;</code></p>
<p>文件写在一块会报错<code>Caused by: java.lang.IllegalAccessException: Class org.apache.spark.sql.catalyst.expressions.objects.InitializeJavaBean can not access a member of class com.zzw.bigdata.spark.sparksql.AvgAgeBuffer with modifiers "public</code></p>
<pre><code class="language-java">import org.apache.spark.sql.*;  
  
import static org.apache.spark.sql.functions.udaf;  
  
public class SQL03_SQL_UDAF {  
    public static void main(String[] args) {  
        // 创建SparkSession  
        SparkSession sparkSession = SparkSession.builder()  
                .appName("SQL01_Model_API")  
                .master("local[2]")  
                .getOrCreate();  
  
        System.out.println("Spark SQL 环境创建成功！");  
  
        // 读取 JSON 数据  
        Dataset&lt;Row&gt; dataset = sparkSession.read()  
                .json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user2.json");  
  
        // 注册临时视图  
        dataset.createOrReplaceTempView("users");  
  
        sparkSession.udf().register("avg_age", udaf(new MyAvgAgeUDAF(), Encoders.LONG()));  
  
        // 执行 SQL 查询  
        String sqlText = "SELECT avg_age(age) AS avg_age FROM users";  
        Dataset&lt;Row&gt; result = sparkSession.sql(sqlText);  
  
        // 展示结果  
        result.show();  
  
        // 停止 SparkSession        sparkSession.stop();  
        System.out.println("Spark SQL 环境已停止！");  
    }  
}
</code></pre>
<pre><code class="language-java">import org.apache.spark.sql.Encoder;  
import org.apache.spark.sql.Encoders;  
import org.apache.spark.sql.expressions.Aggregator;  
  
public class MyAvgAgeUDAF extends Aggregator&lt;Long, AvgAgeBuffer, Long&gt; {  
    @Override  
    //初始化缓冲区  
    public AvgAgeBuffer zero() {  
        return new AvgAgeBuffer(0L, 0L);  
    }  
  
    @Override  
    //聚合输入的年龄和缓冲区中的数据，更新缓冲区  
    public AvgAgeBuffer reduce(AvgAgeBuffer buffer, Long value) {  
        buffer.setSum(buffer.getSum() + value);  
        buffer.setCount(buffer.getCount() + 1);  
        return buffer;  
    }  
  
    @Override  
    //合并两个缓冲区，将两个缓冲区中的数据合并到一起  
    public AvgAgeBuffer merge(AvgAgeBuffer buffer1, AvgAgeBuffer buffer2) {  
        buffer1.setSum(buffer1.getSum() + buffer2.getSum());  
        buffer1.setCount(buffer1.getCount() + buffer2.getCount());  
        return buffer1;  
    }  
  
    @Override  
    //计算最终结果  
    public Long finish(AvgAgeBuffer buffer) {  
        return buffer.getSum()/buffer.getCount();  
    }  
    @Override  
    public Encoder&lt;AvgAgeBuffer&gt; bufferEncoder() {  
        return Encoders.bean(AvgAgeBuffer.class);  
    }  
  
    @Override  
    public Encoder&lt;Long&gt; outputEncoder() {  
        return Encoders.LONG();  
    }  
}
</code></pre>
<pre><code class="language-java">import java.io.Serializable;  
  
public class AvgAgeBuffer  implements Serializable {  
    private long sum = 0;  
    private long count = 0;  
  
    public AvgAgeBuffer() {  
    }  
  
    public AvgAgeBuffer(long sum, long count) {  
        this.sum = sum;  
        this.count = count;  
    }  
  
    public long getSum() {  
        return sum;  
    }  
  
    public void setSum(long sum) {  
        this.sum = sum;  
    }  
  
    public long getCount() {  
        return count;  
    }  
  
    public void setCount(long count) {  
        this.count = count;  
    }  
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="sparksql-实战各区域热门商品-top3"><a class="header" href="#sparksql-实战各区域热门商品-top3">SparkSQL 实战：各区域热门商品 Top3</a></h1>
<ul>
<li><a href="Spark/SparkSQL%E5%AE%9E%E6%88%98.html#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90">需求分析</a></li>
<li><a href="Spark/SparkSQL%E5%AE%9E%E6%88%98.html#%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE">用户行为数据</a></li>
<li><a href="Spark/SparkSQL%E5%AE%9E%E6%88%98.html#%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88sql--udaf">实现方案：SQL + UDAF</a></li>
<li><a href="Spark/SparkSQL%E5%AE%9E%E6%88%98.html#%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4">详细步骤</a></li>
<li><a href="Spark/SparkSQL%E5%AE%9E%E6%88%98.html#sql-groupby-%E8%AF%AD%E6%B3%95">SQL GroupBy 语法</a></li>
<li><a href="Spark/SparkSQL%E5%AE%9E%E6%88%98.html#%E8%A1%8C%E8%BD%AC%E5%88%97">行转列</a></li>
<li><a href="Spark/SparkSQL%E5%AE%9E%E6%88%98.html#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
<h2 id="需求分析"><a class="header" href="#需求分析"><strong>需求分析</strong></a></h2>
<p>该需求旨在统计各个区域内最受欢迎的商品，并列出每个区域点击量 Top 3 的商品。解决此问题需要用到以下 SparkSQL 技术：</p>
<ol>
<li><strong>分组聚合 (GroupBy)</strong>：当需求中出现 "各个 XXXX" 的描述时，通常意味着需要按照某个维度对数据进行分组。<code>GroupBy</code> 操作能够将具有相同属性的数据划分到同一个组中，为后续的聚合计算奠定基础。</li>
<li><strong>热门商品统计</strong>：从用户行为数据中，提取点击行为，并统计每个商品的点击量。统计结果可以表示为（商品 ID，点击数量）的键值对。</li>
<li><strong>Top N 算法</strong>：在每个分组内，按照点击量对商品进行排序，并选取前 N 名作为热门商品。这里 N=3，即 Top3。组内排序通常需要使用开窗函数，它可以在不改变现有行的基础上，为每行增加排序信息（例如 <code>RANK()</code>、<code>ROW_NUMBER()</code> 等）。</li>
<li><strong>数据补全</strong>：原始数据可能不包含所有需要的字段信息，或者某些字段存在缺失值。需要通过 Join 操作连接相关表，或者使用 Union 操作合并多个数据集，以补全数据。例如，可以将商品 ID 与商品名称进行关联，确保结果包含完整的商品信息。</li>
<li><strong>结果展示</strong>：最终结果需要以清晰、易读的方式呈现。如果需要将多行数据合并为单行，或者对结果进行格式化，可以使用自定义 UDF (User Defined Function) 函数来实现。</li>
</ol>
<h2 id="用户行为数据"><a class="header" href="#用户行为数据"><strong>用户行为数据</strong></a></h2>
<p>在电商场景中，典型的用户行为包括：</p>
<ul>
<li><strong>点击 (Click)</strong>：用户浏览商品的行为。</li>
<li><strong>加购 (Add to Cart)</strong>：用户将商品添加到购物车的行为。</li>
<li><strong>下单 (Order)</strong>：用户提交订单购买商品的行为。</li>
<li><strong>支付 (Payment)</strong>：用户完成支付的行为。</li>
</ul>
<p>在本案例中，我们主要关注点击行为，通过统计点击量来衡量商品的热度。</p>
<h2 id="实现方案sql--udaf"><a class="header" href="#实现方案sql--udaf"><strong>实现方案：SQL + UDAF</strong></a></h2>
<p>考虑到 SparkSQL 的表达能力和执行效率，我们采用 SQL 作为主要的实现方式。对于 SQL 不擅长或者难以实现的功能，可以考虑使用自定义 UDF (User Defined Function) 或 UDAF (User Defined Aggregate Function) 函数来辅助。</p>
<h2 id="详细步骤"><a class="header" href="#详细步骤"><strong>详细步骤</strong></a></h2>
<ol>
<li><strong>数据读取</strong>：使用 SparkSQL 读取包含用户行为数据的 JSON 文件。注意，如果 JSON 文件中的数据为整型，SparkSQL 通常会将其封装为 <code>BigInt</code> 类型（即 Long 类型）。
<pre><code class="language-scala">val df = spark.read.json("path/to/user_behavior.json")
df.printSchema()
</code></pre>
</li>
<li><strong>数据转换</strong>：将原始数据转换为方便后续处理的格式。例如，可以提取用户 ID、商品 ID、行为类型、时间戳等字段。
<pre><code class="language-scala">val behaviorDF = df.select("user_id", "item_id", "behavior_type", "timestamp")
</code></pre>
</li>
<li><strong>点击行为过滤</strong>：筛选出点击行为的数据。
<pre><code class="language-scala">val clickDF = behaviorDF.filter($"behavior_type" === "click")
</code></pre>
</li>
<li><strong>分组聚合</strong>：按照区域和商品 ID 进行分组，统计每个区域内每个商品的点击量。
<pre><code class="language-scala">val itemClickCounts = clickDF.groupBy("region", "item_id")
  .agg(count("*").alias("click_count"))
</code></pre>
</li>
<li><strong>开窗函数</strong>：使用开窗函数在每个区域内按照点击量进行排序。
<pre><code class="language-scala">val rankedItems = itemClickCounts.withColumn(
  "rank",
  dense_rank().over(Window.partitionBy("region").orderBy(desc("click_count")))
)
</code></pre>
</li>
<li><strong>Top3 筛选</strong>：筛选出每个区域内点击量排名前 3 的商品。
<pre><code class="language-scala">val top3Items = rankedItems.filter($"rank" &lt;= 3)
</code></pre>
</li>
<li><strong>结果展示</strong>：将结果以表格或其他形式展示出来。如果需要对结果进行格式化，可以使用 UDF 函数。
<pre><code class="language-scala">top3Items.show()
</code></pre>
</li>
</ol>
<h2 id="sql-groupby-语法"><a class="header" href="#sql-groupby-语法"><strong>SQL GroupBy 语法</strong></a></h2>
<p>在使用 <code>GroupBy</code> 子句时，需要注意以下几点：</p>
<ol>
<li><strong>Select 子句限制</strong>：如果 <code>Select</code> 子句中包含聚合函数（例如 <code>COUNT()</code>、<code>SUM()</code>、<code>AVG()</code> 等），那么查询字段必须满足以下条件之一：
<ul>
<li>常量</li>
<li>在聚合函数中使用</li>
<li>参与分组 ( <code>GroupBy</code> 子句中指定的字段)</li>
</ul>
</li>
<li><strong>分组字段关系</strong>：
<ul>
<li>如果分组字段存在上下级或从属关系，那么统计结果与下级字段有关，与上级字段无关。例如，如果按照省份和城市进行分组，统计结果会受到城市的影响。增加上级字段（例如省份）的目的是为了补全数据，确保每个省份都有对应的 Top3 商品。</li>
<li>如果分组字段存在关联关系（例如商品 ID 和商品名称），那么统计结果与具备唯一性的字段有关（例如商品 ID）。其他字段（例如商品名称）的作用是补全数据，方便结果展示。</li>
<li>如果分组字段没有任何关系，那么统计结果与所有字段有关。</li>
</ul>
</li>
</ol>
<h2 id="行转列"><a class="header" href="#行转列"><strong>行转列</strong></a></h2>
<p>如果需要将多行数据合并为单行，可以使用 <code>pivot</code> 函数或者自定义 UDAF 函数来实现。</p>
<h2 id="总结-1"><a class="header" href="#总结-1"><strong>总结</strong></a></h2>
<p>本案例演示了如何使用 SparkSQL 解决实际业务问题。通过分组聚合、开窗函数、数据补全等技术，我们可以高效地统计出各个区域的热门商品 Top3。同时，我们也需要注意 SQL 语法和分组字段之间的关系，确保结果的准确性和完整性。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="spark数据源"><a class="header" href="#spark数据源">Spark数据源</a></h1>
<ul>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#spark-%E6%95%B0%E6%8D%AE%E6%BA%90%E6%A6%82%E8%A7%88">Spark 数据源概览</a>
<ul>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#1-%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE">1. 内存数据</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#2-%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE">2. 文件数据</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#3-json-%E6%95%B0%E6%8D%AE">3. JSON 数据</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#4-parquet-%E6%96%87%E4%BB%B6">4. Parquet 文件</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#5-hive-%E8%A1%A8">5. Hive 表</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#6-jdbc-%E6%95%B0%E6%8D%AE%E5%BA%93">6. JDBC 数据库</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#7-kafka">7. Kafka</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#8-%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E6%BA%90">8. 其他数据源</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81%E6%80%BB%E7%BB%93">读取数据示例代码总结</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#%E6%80%BB%E7%BB%93">总结</a></li>
</ul>
</li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#%E9%99%84%E5%BD%95">附录</a>
<ul>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#csv%E6%96%87%E4%BB%B6">CSV文件</a>
<ul>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#read">read</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#write">write</a></li>
</ul>
</li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#json%E6%96%87%E4%BB%B6">JSON文件</a>
<ul>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#read-1">read</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#json%E5%92%8Ccsv%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2">JSON和CSV相互转换</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8%E7%9A%84%E6%96%87%E4%BB%B6primary-key">行式存储的文件（Primary Key）</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E7%9A%84%E6%96%87%E4%BB%B6">列式存储的文件</a></li>
</ul>
</li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#parquet%E6%96%87%E4%BB%B6">Parquet文件</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#mysqljdbc">MySQL&amp;JDBC</a></li>
<li><a href="Spark/Spark%E6%95%B0%E6%8D%AE%E6%BA%90.html#hive">HIVE</a></li>
</ul>
</li>
</ul>
<h1 id="spark-数据源概览"><a class="header" href="#spark-数据源概览">Spark 数据源概览</a></h1>
<p>Spark 是一个强大的分布式计算框架，能够从多种数据源读取、处理和写入数据。下面详细介绍 Spark 支持的数据源以及相关的示例代码。</p>
<h2 id="1-内存数据"><a class="header" href="#1-内存数据">1. 内存数据</a></h2>
<ul>
<li><strong>RDD</strong>（Resilient Distributed Dataset）：Spark 的基础数据结构，可以从已有的集合（如 List、Array、等）创建。</li>
<li><strong>DataFrame</strong> 和 <strong>Dataset</strong>：更高级的抽象和结构化的数据表示。</li>
</ul>
<h2 id="2-文件数据"><a class="header" href="#2-文件数据">2. 文件数据</a></h2>
<ul>
<li>
<p><strong>文本文件</strong>：Spark 能够读取文本文件（如 CSV、JSON、Parquet 等），通常使用 SparkContext 的 <code>textFile</code> 或 <code>spark.read</code> 方法。</p>
<p>示例代码：</p>
<pre><code class="language-scala">// 读取文本文件
val textRDD = spark.sparkContext.textFile("path/to/textfile.txt")
val textDF = spark.read.text("path/to/textfile.txt")
</code></pre>
</li>
</ul>
<h2 id="3-json-数据"><a class="header" href="#3-json-数据">3. JSON 数据</a></h2>
<ul>
<li>
<p>Spark 支持读取 JSON 格式的数据，使用 DataFrame API 进行加载。</p>
<p>示例代码：</p>
<pre><code class="language-scala">// 读取 JSON 文件
val jsonDF = spark.read.json("path/to/data.json")
jsonDF.show()
</code></pre>
</li>
</ul>
<h2 id="4-parquet-文件"><a class="header" href="#4-parquet-文件">4. Parquet 文件</a></h2>
<ul>
<li>
<p>Parquet 是一种列式存储格式，具有高压缩率和高性能，适合与 Spark 一起使用。</p>
<p>示例代码：</p>
<pre><code class="language-scala">// 读取 Parquet 文件
val parquetDF = spark.read.parquet("path/to/data.parquet")
parquetDF.show()
</code></pre>
</li>
</ul>
<h2 id="5-hive-表"><a class="header" href="#5-hive-表">5. Hive 表</a></h2>
<ul>
<li>
<p>Spark 能够直接访问存储在 Hive 的表，需要配置 Hive 支持。</p>
<p>示例代码：</p>
<pre><code class="language-scala">// 读取 Hive 表
spark.sql("USE database_name")
val hiveDF = spark.sql("SELECT * FROM table_name")
hiveDF.show()
</code></pre>
</li>
</ul>
<h2 id="6-jdbc-数据库"><a class="header" href="#6-jdbc-数据库">6. JDBC 数据库</a></h2>
<ul>
<li>
<p>Spark 支持通过 JDBC 从关系型数据库（如 MySQL、PostgreSQL、Oracle 等）读取和写入数据。</p>
<p>示例代码：</p>
<pre><code class="language-scala">// JDBC 读取数据
val jdbcDF = spark.read
  .format("jdbc")
  .option("url", "jdbc:mysql://hostname:port/dbname")
  .option("dbtable", "table_name")
  .option("user", "username")
  .option("password", "password")
  .load()

jdbcDF.show()
</code></pre>
</li>
</ul>
<h2 id="7-kafka"><a class="header" href="#7-kafka">7. Kafka</a></h2>
<ul>
<li>
<p>Spark Streaming 可以与 Kafka 集成，读取实时流数据。</p>
<p>示例代码：</p>
<pre><code class="language-scala">// 从 Kafka 读取数据
val kafkaDF = spark.read
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic_name")
  .load()

kafkaDF.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").show()
</code></pre>
</li>
</ul>
<h2 id="8-其他数据源"><a class="header" href="#8-其他数据源">8. 其他数据源</a></h2>
<p>Spark 还支持其他多种数据源，如：</p>
<ul>
<li>ORC 文件</li>
<li>Avro 文件</li>
<li>Cassandra 数据库</li>
<li>Redis</li>
<li>HBase</li>
<li>以及其他可以通过 Spark 的 DataSource API 进行扩展的源</li>
</ul>
<h2 id="读取数据示例代码总结"><a class="header" href="#读取数据示例代码总结">读取数据示例代码总结</a></h2>
<p>以下示例代码整合了不同数据源的读取方法：</p>
<pre><code class="language-scala">import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("Spark Data Sources Example")
  .getOrCreate()

// 1. 从文本文件读取
val textDF = spark.read.text("path/to/textfile.txt")
textDF.show()

// 2. 从 JSON 文件读取
val jsonDF = spark.read.json("path/to/data.json")
jsonDF.show()

// 3. 从 Parquet 文件读取
val parquetDF = spark.read.parquet("path/to/data.parquet")
parquetDF.show()

// 4. 从 Hive 表读取
spark.sql("USE database_name")
val hiveDF = spark.sql("SELECT * FROM table_name")
hiveDF.show()

// 5. 从 JDBC 数据库读取
val jdbcDF = spark.read
  .format("jdbc")
  .option("url", "jdbc:mysql://hostname:port/dbname")
  .option("dbtable", "table_name")
  .option("user", "username")
  .option("password", "password")
  .load()
jdbcDF.show()

// 6. 从 Kafka 读取
val kafkaDF = spark.read
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic_name")
  .load()
kafkaDF.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").show()

spark.stop()
</code></pre>
<h2 id="总结-2"><a class="header" href="#总结-2">总结</a></h2>
<p>Spark 提供强大的数据源连接能力，涵盖了从传统的文件读取到实时流处理的广泛场景。使用 Spark 的 DataFrame 和 SQL API，可以方便地对数据进行处理和分析。</p>
<h1 id="附录"><a class="header" href="#附录">附录</a></h1>
<h2 id="csv文件"><a class="header" href="#csv文件">CSV文件</a></h2>
<p>CSV文件就是将数据采用逗号分隔的数据文件。</p>
<h3 id="read"><a class="header" href="#read">read</a></h3>
<p>读取示例代码：</p>
<pre><code class="language-java">final Dataset&lt;Row&gt; csv= sparkSession.read()
.option("header"，"true")//配置
.option("sep","_")// 配置：It=&gt;tsv，csv.csv(path:"data/user.csv");
.csv.show();
</code></pre>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;  
import org.apache.spark.sql.Row;  
import org.apache.spark.sql.SparkSession;  
  
public class SQL03_SQL_Source_CSV {  
    public static void main(String[] args) {  
        // 创建SparkSession  
        SparkSession sparkSession = SparkSession.builder()  
                .appName("SQL01_Model")  
                .master("local[2]")  
                .getOrCreate();  
  
        // 读取CSV文件  
        Dataset&lt;Row&gt; csvDF = sparkSession.read()  
                .format("csv")  
                .option("header", "true").option("sep", ",")
                .csv("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user.csv");  
  
        // 显示数据  
        csvDF.show();  
  
  
        sparkSession.stop();  
  
    }  
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>+---+----+---+
| id|name|age|
+---+----+---+
|  1|John| 25|
|  2|Jane| 30|
|  3| Bob| 40|
+---+----+---+
</code></pre>
<h3 id="write"><a class="header" href="#write">write</a></h3>
<p>保存文件示例代码：</p>
<pre><code class="language-java">csvDF.write().csv("output");
</code></pre>
<p>含有分区文件<code>part-00000-7122d0ef-0e2e-4507-a9cd-74b297e635f2-c000.csv</code>的目录</p>
<p>设置mode，实现覆盖写或是追加，相关源代码如下所示：</p>
<pre><code class="language-scala">public DataFrameWriter&lt;T&gt; mode(final String saveMode) {
        String var4 = saveMode.toLowerCase(Locale.ROOT);
        if ("overwrite".equals(var4)) {
            return this.mode(SaveMode.Overwrite);
        } else if ("append".equals(var4)) {
            return this.mode(SaveMode.Append);
        } else if ("ignore".equals(var4)) {
            return this.mode(SaveMode.Ignore);
        } else if ("error".equals(var4) ? true : ("errorifexists".equals(var4) ? true : "default".equals(var4))) {
            return this.mode(SaveMode.ErrorIfExists);
        } else {
            throw new IllegalArgumentException((new StringBuilder(114)).append("Unknown save mode: ").append(saveMode).append(". Accepted ").append("save modes are 'overwrite', 'append', 'ignore', 'error', 'errorifexists', 'default'.").toString());
        }
    }
</code></pre>
<p>完整向CSV文件写入内容的代码如下所示：</p>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class SQL03_SQL_Source_CSV {
    public static void main(String[] args) {
        // 创建SparkSession
        SparkSession sparkSession = SparkSession.builder()
                .appName("SQL01_Model")
                .master("local[2]")
                .getOrCreate();

        // 读取CSV文件
        Dataset&lt;Row&gt; csvDF = sparkSession.read()
                .format("csv")
                .option("header", "true")
                .option("sep", ",")
                .csv("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user.csv");

        // 显示数据
        csvDF.show();

        csvDF.write()
                .format("csv")
                .option("header", "true")
                .mode("overwrite")
                .csv("output");


        sparkSession.stop();

    }
}
</code></pre>
<h2 id="json文件"><a class="header" href="#json文件">JSON文件</a></h2>
<p>JSON:JavaScript ObjectNotation</p>
<p>对象：{}</p>
<p>数组：[]</p>
<p>JSON文件：整个文件的数据格式符合JSON格式，不是一行数据符合JSON格式</p>
<p>SparkSQL其实就是对SparkCoreRDD的封装。RDD读取文件采用的是Hadoop，Hadoop按行读取文件内容。SparkSQL只需要保证JSON文件中一行数据符合JSON格式即可，无需整个文件符合JSON格式。</p>
<p>Spark基于HDFS按行读取JSON格式文件，所以使用的JSON格式要按行存储，不能保存为一个完整的JSON文件，否则读取的大部分数据都为空。相关报错信息如下所示：</p>
<pre><code>org.apache.spark.sql.AnalysisException:

SinceSpark2.3，thequeriesfromrawJsoN/csVfilesaredisallowedwhenthe referencedcolumnsonlyincludetheinternalcorruptrecord column
(named _corrupt_record by default)
</code></pre>
<h3 id="read-1"><a class="header" href="#read-1">read</a></h3>
<pre><code class="language-java">final Dataset&lt;Row&gt;json =sparkSession.read().json(path:"data/user.json"); json.show();
</code></pre>
<p>完整代码如下所示：</p>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;  
import org.apache.spark.sql.Row;  
import org.apache.spark.sql.SparkSession;  
  
public class SQL03_SQL_Source_JSON {  
    public static void main(String[] args) {  
        // 创建SparkSession  
        SparkSession sparkSession = SparkSession.builder()  
                .appName("SQL01_Model")  
                .master("local[2]")  
                .getOrCreate();  
  
        // 读取CSV文件  
        Dataset&lt;Row&gt; jsonDF = sparkSession.read()  
                .json("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user2.json");  
  
        // 显示数据  
        jsonDF.show();  
  
        jsonDF.write().parquet("people.parquet");  
  
  
  
        sparkSession.stop();  
  
    }  
}
</code></pre>
<h3 id="json和csv相互转换"><a class="header" href="#json和csv相互转换">JSON和CSV相互转换</a></h3>
<p>通过Dataset</p>
<h3 id="行式存储的文件primary-key"><a class="header" href="#行式存储的文件primary-key">行式存储的文件（Primary Key）</a></h3>
<p>查询快，统计慢</p>
<h3 id="列式存储的文件"><a class="header" href="#列式存储的文件">列式存储的文件</a></h3>
<p>查询快，统计快</p>
<p>HBase使用列式存储</p>
<h2 id="parquet文件"><a class="header" href="#parquet文件">Parquet文件</a></h2>
<p>列式存储的数据自带列分割。</p>
<p><code>part-00000-6b7cb14a-b49f-4b68-b24e-05ed6955c863-c000.snappy.parquet</code>中snappy表示压缩格式（Hadoop）</p>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;  
import org.apache.spark.sql.Row;  
import org.apache.spark.sql.SparkSession;  
  
public class SQL03_SQL_Source_Parquet {  
    public static void main(String[] args) {  
        // 创建SparkSession  
        SparkSession sparkSession = SparkSession.builder()  
                .appName("SQL01_Model")  
                .master("local[2]")  
                .getOrCreate();  
  
        // 读取CSV文件  
        Dataset&lt;Row&gt; parquetDF = sparkSession.read()  
                .parquet("D:\\Documents\\items\\BigData\\BigDataCode\\spark\\src\\main\\resources\\data\\user.parquet");  
  
        // 显示数据  
        parquetDF.show();  
  
        //parquetDF.write().parquet("people.parquet");  
  
  
  
        sparkSession.stop();  
  
    }  
}
</code></pre>
<h2 id="mysqljdbc"><a class="header" href="#mysqljdbc">MySQL&amp;JDBC</a></h2>
<pre><code class="language-java">import org.apache.spark.sql.Dataset;  
import org.apache.spark.sql.Row;  
import org.apache.spark.sql.SparkSession;  
  
import java.util.Properties;  
  
public class SQL03_SQL_Source_MySQL {  
    public static void main(String[] args) {  
        // 创建SparkSession  
        SparkSession sparkSession = SparkSession.builder()  
                .appName("SQL01_Model")  
                .master("local[2]")  
                .getOrCreate();  
  
        Properties properties = new Properties();  
        properties.setProperty("user", "root");  
        properties.setProperty("password", "root123");  
  
        // 读取MySQL数据  
        Dataset&lt;Row&gt; dataset = sparkSession.read()  
               .jdbc("jdbc:mysql://localhost:3306/tiandi", "users", properties);  
  
        dataset.show();  
		dataset.write().jdbc("jdbc:mysql://localhost:3306/tiandi", "users2", properties);
  
  
        sparkSession.stop();  
  
    }  
}
</code></pre>
<h2 id="hive"><a class="header" href="#hive">HIVE</a></h2>
<p>SparkSQL可以采用内嵌Hive（spark开箱即用的 hive），也可以采用外部 Hive。企业开发中，通常采用外部Hive。</p>
<p>2）拷贝 hive-site.xml到resources 目录（如果需要操作Hadoop，需要拷贝 hdfs-site.xml、 core-site.xml、 yarn-site.xml）</p>
<pre><code class="language-xml">&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
    &lt;!--配置Hive保存元数据信息所需的 MySQL URL地址--&gt;
    &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
        &lt;value&gt;jdbc:mysql://localhost:3306/metastore?useSSL=false&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8&amp;amp;allowPublicKeyRetrieval=true&lt;/value&gt;
    &lt;/property&gt;

    &lt;!--配置Hive连接MySQL的驱动全类名--&gt;
    &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
        &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;
    &lt;/property&gt;

    &lt;!--配置Hive连接MySQL的用户名 --&gt;
    &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
        &lt;value&gt;root&lt;/value&gt;
    &lt;/property&gt;

    &lt;!--配置Hive连接MySQL的密码 --&gt;
    &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
        &lt;value&gt;root123&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
        &lt;value&gt;/user/hive/warehouse&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
    &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;
        &lt;value&gt;hadoop100&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
    &lt;/property&gt;
    
    &lt;property&gt;
        &lt;name&gt;hive.cli.print.header&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;


</code></pre>
<p>hive-site.xml完整配置文件内容如下所示：</p>
<pre><code class="language-xml">&lt;configuration&gt;
  &lt;!-- Hive元数据存储的URI --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.uris&lt;/name&gt;
    &lt;value&gt;thrift://myhost:9083&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Hive元数据客户端套接字超时时间（以毫秒为单位） --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.client.socket.timeout&lt;/name&gt;
    &lt;value&gt;300&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Hive数据仓库目录 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 子目录是否继承权限 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.warehouse.subdir.inherit.perms&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 自动转换连接类型的Join操作 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.auto.convert.join&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 自动转换连接类型的Join操作时条件不满足的最大数据量（以字节为单位） --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.auto.convert.join.noconditionaltask.size&lt;/name&gt;
    &lt;value&gt;20971520&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否优化Bucket Map Join的Sorted Merge --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.optimize.bucketmapjoin.sortedmerge&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- SMB Join操作缓存的行数 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.smbjoin.cache.rows&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否启用Hive Server2日志记录操作 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.server2.logging.operation.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Hive Server2操作日志的存储位置 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;
    &lt;value&gt;/var/log/hive/operation_logs&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- MapReduce作业的Reduce任务数 --&gt;
  &lt;property&gt;
    &lt;name&gt;mapred.reduce.tasks&lt;/name&gt;
    &lt;value&gt;-1&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 每个Reduce任务的数据量（以字节为单位） --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.exec.reducers.bytes.per.reducer&lt;/name&gt;
    &lt;value&gt;67108864&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 最大允许复制文件的大小（以字节为单位） --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.exec.copyfile.maxsize&lt;/name&gt;
    &lt;value&gt;33554432&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 同时运行的最大Reduce任务数 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.exec.reducers.max&lt;/name&gt;
    &lt;value&gt;1099&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Vectorized Group By操作的检查间隔 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.groupby.checkinterval&lt;/name&gt;
    &lt;value&gt;4096&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Vectorized Group By操作的Flush比例 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.groupby.flush.percent&lt;/name&gt;
    &lt;value&gt;0.1&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否使用统计信息来优化查询计划 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.compute.query.using.stats&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否启用向量化执行引擎 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.execution.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否在Reduce阶段启用向量化执行 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.execution.reduce.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否使用向量化输入格式 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.use.vectorized.input.format&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否使用检查表达式的向量化执行 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.use.checked.expressions&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否使用向量化序列化和反序列化 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.use.vector.serde.deserialize&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 向量化适配器的使用模式 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.adaptor.usage.mode&lt;/name&gt;
    &lt;value&gt;chosen&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 排除的向量化输入格式列表 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.input.format.excludes&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否合并Map输出的小文件 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.mapfiles&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否合并MapReduce输出的小文件 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.mapredfiles&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否启用CBO优化 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.cbo.enable&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Fetch任务转换级别 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;
    &lt;value&gt;minimal&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 触发Fetch任务转换的数据量阈值（以字节为单位） --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.fetch.task.conversion.threshold&lt;/name&gt;
    &lt;value&gt;268435456&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Limit操作的内存使用百分比 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.limit.pushdown.memory.usage&lt;/name&gt;
    &lt;value&gt;0.1&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否合并Spark任务输出的小文件 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.sparkfiles&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 合并小文件时的平均大小（以字节为单位） --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.smallfiles.avgsize&lt;/name&gt;
    &lt;value&gt;16777216&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 每个任务合并的数据量（以字节为单位） --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.size.per.task&lt;/name&gt;
    &lt;value&gt;268435456&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否启用重复消除优化 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.optimize.reducededuplication&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 最小Reduce任务数以启用重复消除优化 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.optimize.reducededuplication.min.reducer&lt;/name&gt;
    &lt;value&gt;4&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否启用Map端聚合 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.map.aggr&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Map端聚合的哈希表内存比例 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.map.aggr.hash.percentmemory&lt;/name&gt;
    &lt;value&gt;0.5&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否优化动态分区排序 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.optimize.sort.dynamic.partition&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Hive执行引擎类型（mr、tez、spark） --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.execution.engine&lt;/name&gt;
    &lt;value&gt;mr&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Spark Executor的内存大小 --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.executor.memory&lt;/name&gt;
    &lt;value&gt;2572261785b&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Spark Driver的内存大小 --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.driver.memory&lt;/name&gt;
    &lt;value&gt;3865470566b&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 每个Spark Executor的核心数 --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.executor.cores&lt;/name&gt;
    &lt;value&gt;4&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Spark Driver的内存Overhead --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.yarn.driver.memoryOverhead&lt;/name&gt;
    &lt;value&gt;409m&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Spark Executor的内存Overhead --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.yarn.executor.memoryOverhead&lt;/name&gt;
    &lt;value&gt;432m&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否启用动态资源分配 --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.dynamicAllocation.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 动态资源分配的初始Executor数量 --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.dynamicAllocation.initialExecutors&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 动态资源分配的最小Executor数量 --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.dynamicAllocation.minExecutors&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 动态资源分配的最大Executor数量 --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.dynamicAllocation.maxExecutors&lt;/name&gt;
    &lt;value&gt;2147483647&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否在Hive元数据存储中执行setugi操作 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否支持并发操作 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.support.concurrency&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- ZooKeeper服务器列表 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;myhost04,myhost03,myhost02&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- ZooKeeper客户端端口号 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt;
    &lt;value&gt;2181&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Hive使用的ZooKeeper命名空间 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.zookeeper.namespace&lt;/name&gt;
    &lt;value&gt;hive_zookeeper_namespace_hive&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 集群委派令牌存储类 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.cluster.delegation.token.store.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.hive.thrift.MemoryTokenStore&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否启用Hive Server2用户代理模式 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否启用Hive元数据存储的SASL认证 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Hive Server2的认证方式 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.server2.authentication&lt;/name&gt;
    &lt;value&gt;kerberos&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Hive元数据存储的Kerberos主体名称 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt;
    &lt;value&gt;hive/_HOST@MY.COM&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Hive Server2的Kerberos主体名称 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt;
    &lt;value&gt;hive/_HOST@MY.COM&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否启用Spark Shuffle服务 --&gt;
  &lt;property&gt;
    &lt;name&gt;spark.shuffle.service.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否在没有Limit操作的OrderBy语句中执行严格检查 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.strict.checks.orderby.no.limit&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否在没有分区过滤条件的查询中执行严格检查 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.strict.checks.no.partition.filter&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否执行严格的类型安全性检查 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.strict.checks.type.safety&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否执行严格的笛卡尔积检查 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.strict.checks.cartesian.product&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- 是否执行严格的桶排序检查 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.strict.checks.bucketing&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="spark-streaming"><a class="header" href="#spark-streaming">Spark Streaming</a></h1>
<ul>
<li><a href="Spark/SparkStreaming.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8spark-streaming">为什么使用Spark Streaming？</a>
<ul>
<li><a href="Spark/SparkStreaming.html#%E6%9C%89%E7%95%8C%E6%95%B0%E6%8D%AE%E6%B5%81%E5%92%8C%E6%97%A0%E7%95%8C%E6%95%B0%E6%8D%AE%E6%B5%81">有界数据流和无界数据流</a></li>
<li><a href="Spark/SparkStreaming.html#%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E7%A6%BB%E7%BA%BF%E8%AE%A1%E7%AE%97%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%89%B9%E9%87%8F%E8%AE%A1%E7%AE%97">实时计算、离线计算、流式计算、批量计算</a></li>
<li><a href="Spark/SparkStreaming.html#%E4%BB%80%E4%B9%88%E6%98%AFdstream">什么是DStream？</a></li>
<li><a href="Spark/SparkStreaming.html#%E6%9E%B6%E6%9E%84%E5%9B%BE">架构图</a></li>
<li><a href="Spark/SparkStreaming.html#%E6%96%B9%E6%B3%95vs%E7%AE%97%E5%AD%90">方法VS算子</a></li>
<li><a href="Spark/SparkStreaming.html#sparkstreaming-vs-rdd%E7%AE%97%E5%AD%90">SparkStreaming VS RDD算子</a></li>
</ul>
</li>
<li><a href="Spark/SparkStreaming.html#spark-streaming%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">Spark Streaming环境搭建</a></li>
<li><a href="Spark/SparkStreaming.html#socket-vs-kafka">Socket VS Kafka</a>
<ul>
<li><a href="Spark/SparkStreaming.html#%E7%BD%91%E7%BB%9Csocket%E6%95%B0%E6%8D%AE%E6%B5%81%E5%A4%84%E7%90%86">网络（socket）数据流处理</a></li>
<li><a href="Spark/SparkStreaming.html#kafka%E6%95%B0%E6%8D%AE%E6%B5%81%E5%A4%84%E7%90%86">Kafka数据流处理</a></li>
<li><a href="Spark/SparkStreaming.html#%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90">案例解析</a></li>
</ul>
</li>
<li><a href="Spark/SparkStreaming.html#dstream%E8%BD%AC%E6%8D%A2">DStream转换</a>
<ul>
<li><a href="Spark/SparkStreaming.html#%E6%97%A0%E7%8A%B6%E6%80%81%E8%BD%AC%E5%8C%96%E6%93%8D%E4%BD%9C">无状态转化操作</a></li>
<li><a href="Spark/SparkStreaming.html#dstream%E5%92%8Crdd">DStream和RDD</a></li>
</ul>
</li>
<li><a href="Spark/SparkStreaming.html#%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C">窗口操作</a>
<ul>
<li><a href="Spark/SparkStreaming.html#%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E5%8F%8A%E7%AA%97%E5%8F%A3%E5%8F%82%E6%95%B0">滑动窗口及窗口参数</a></li>
<li><a href="Spark/SparkStreaming.html#windowoperations">WindowOperations</a></li>
<li><a href="Spark/SparkStreaming.html#window">Window</a></li>
<li><a href="Spark/SparkStreaming.html#reducebykeyandwindow">reduceByKeyAndWindow</a></li>
</ul>
</li>
<li><a href="Spark/SparkStreaming.html#dstream%E8%BE%93%E5%87%BA">DStream输出</a></li>
<li><a href="Spark/SparkStreaming.html#%E5%85%B3%E9%97%ADmain%E7%BA%BF%E7%A8%8B">关闭main线程</a>
<ul>
<li><a href="Spark/SparkStreaming.html#%E6%96%B0%E5%A2%9E%E7%BA%BF%E7%A8%8B%E6%96%B9%E5%BC%8F">新增线程方式</a></li>
<li><a href="Spark/SparkStreaming.html#%E8%BE%83%E4%B8%BA%E4%BC%98%E9%9B%85%E7%9A%84%E6%96%B9%E5%BC%8Fstop">较为优雅的方式（stop）</a></li>
<li><a href="Spark/SparkStreaming.html#%E4%BD%BF%E7%94%A8kafka%E6%97%B6%E4%BC%98%E9%9B%85%E5%85%B3%E9%97%AD">使用kafka时优雅关闭</a></li>
</ul>
</li>
</ul>
<h2 id="为什么使用spark-streaming"><a class="header" href="#为什么使用spark-streaming">为什么使用Spark Streaming？</a></h2>
<p>Spark Streaming用于流式数据的处理。</p>
<p>Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、HDFS等。</p>
<p>数据输入后可以用Spark的高度抽象原语如：map丶reduce、join、window等进行运算。</p>
<p>结果也能保存在很多地方，如HDFS、数据库等。</p>
<h3 id="有界数据流和无界数据流"><a class="header" href="#有界数据流和无界数据流">有界数据流和无界数据流</a></h3>
<p>有界数据流可以计算</p>
<p>无界数据流无法计算</p>
<p>Spark Streaming通过数据采集器将无界数据流切分成有界数据流，方便计算</p>
<p>SparkStreaming底层还是SparkCore，就是在流式数据处理中进行的封装</p>
<h3 id="实时计算离线计算流式计算批量计算"><a class="header" href="#实时计算离线计算流式计算批量计算">实时计算、离线计算、流式计算、批量计算</a></h3>
<p>从数据处理方式的角度</p>
<ul>
<li>流式数据处理：一个数据一个数据的处理</li>
<li>微批量数据处理：一小批数据一小批数据的处理</li>
<li>批量数据处理：一批数据一批数据的处理</li>
</ul>
<p>从数据处理延迟的角度</p>
<ul>
<li>实时数据处理：数据处理的延迟以毫秒为单位</li>
<li>准实时数据处理：数据处理的延迟以秒、分钟为单位</li>
<li>离线数据处理：数据处理的延迟以小时，天为单位</li>
</ul>
<p>Spark批量、离线</p>
<p>Spark Streaming微批量、准实时</p>
<p>一小批不是按个数而是按时间来定义比如3s内数据作为一小批，对应数据模型为离散化流dstream</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250114162016.png" alt="" /></p>
<h3 id="什么是dstream"><a class="header" href="#什么是dstream">什么是DStream？</a></h3>
<p>SparkCore =&gt; RDD</p>
<p>SparkSQL =&gt; DataFrame、DataSet</p>
<p>Spark Streaming使用离散化流（Discretized Stream）作为抽象表示，叫作DStream。</p>
<p>DStream是随时间推移而收到的数据的序列。</p>
<p>在DStream内部，每个时间区间收到的数据都作为RDD存在，而DStream是由这些RDD所组成的序列（因此得名“离散化"）。</p>
<p>所以简单来讲，DStream就是对RDD在实时数据处理场景的一种封装。</p>
<h3 id="架构图"><a class="header" href="#架构图">架构图</a></h3>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250115165321.png" alt="" /></p>
<p>整体架构图</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250115165412.png" alt="" /></p>
<p>SparkStreaming架构图</p>
<h3 id="方法vs算子"><a class="header" href="#方法vs算子">方法VS算子</a></h3>
<p>RDD算子是分布式计算，效率更高</p>
<h3 id="sparkstreaming-vs-rdd算子"><a class="header" href="#sparkstreaming-vs-rdd算子">SparkStreaming VS RDD算子</a></h3>
<p>SparkStreaming有三个地方可以写代码，SparkStreaming的方法被称为原语。</p>
<p>DStream 上的操作与 RDD 的类似，分为转换和输出两种，此外转换操作中还有一些比较特殊的原语，如：transform()以及各种Window 相关的原语。</p>
<p>RDD算子只有两个地方可以写代码，Driver端和Executor端</p>
<pre><code class="language-java">//int i= 10;(Driver 1)
        wordCountDS.foreachRDD(
                rdd -&gt; {
                    //int j = 20;(Driver X)
                    rdd.foreach(
                            (num) -&gt; {
                                //int k=30;(Executor N)
                                System.out.println(num);
                            }
                );}
        );
</code></pre>
<h2 id="spark-streaming环境搭建"><a class="header" href="#spark-streaming环境搭建">Spark Streaming环境搭建</a></h2>
<p>Spark在流式数据的处理场景中对核心功能环境进行了封装</p>
<pre><code class="language-java">package com.zzw.bigdata.spark.sparkstreaming;

import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

public class SparkStreamingEnv01 {
    public static void main(String[] args) throws Exception {
        SparkConf conf = new SparkConf();
        conf.setAppName("SparkStreamingEnv01");
        conf.setMaster("local[2]");
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext(conf, new Duration(3 * 1000));
        javaStreamingContext.start();
//        等待数据采集器的结束，如果采集器停止运行，那么main线程会继续执行
        javaStreamingContext.awaitTermination();
//        数据采集器是一个长期执行的任务，所以不能停止，也不能释放资源，只能等待任务结束
//        javaStreamingContext.close();
//        while (true) {
//        }
    }
}
</code></pre>
<p>上面的代码执行会抛出异常<code>Exception in thread "main" java.lang.IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute</code></p>
<p>原因：没有启动行动算子。</p>
<p>Spark Streaming对应的是无界数据流，没有启动行动算子比如print()就会报错。</p>
<p>Socket数据不会保留，不输出就会出问题。</p>
<p>你的代码抛出 <code>java.lang.IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute</code> 的异常是因为在创建 <code>JavaStreamingContext</code> 后没有注册任何输出操作（如接收输入流，处理数据等），Spark Streaming 在没有任何操作的情况下无法执行任务。</p>
<p><strong>解决方案</strong>：</p>
<ol>
<li>需要创建一个输入流，比如通过 <code>socketTextStream</code> 或者其他接收源。</li>
<li>注册一些输出操作，例如调用 <code>print()</code>、<code>saveAsTextFiles()</code> 等方法来处理数据。</li>
</ol>
<p>以下是一个修正后的代码示例，假设使用 <code>socketTextStream</code> 来接收数据：</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.VoidFunction;
import java.util.Arrays;

public class SparkStreamingEnv01 {
    public static void main(String[] args) throws Exception {
        SparkConf conf = new SparkConf();
        conf.setAppName("SparkStreamingEnv01");
        conf.setMaster("local[2]");
        
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext(conf, new Duration(3000));
        
        // 创建一个输入流，假设我们在 localhost:9999 上接收数据
        JavaInputDStream&lt;String&gt; lines = javaStreamingContext.socketTextStream("localhost", 9999);
        
        // 注册输出操作，例如打印接收到的每一行
        lines.foreachRDD(new VoidFunction&lt;JavaRDD&lt;String&gt;&gt;() {
            @Override
            public void call(JavaRDD&lt;String&gt; rdd) throws Exception {
                // 打印每一行
                rdd.foreach(line -&gt; System.out.println(line));
            }
        });
        
        javaStreamingContext.start();
        javaStreamingContext.awaitTermination();
    }
}
</code></pre>
<p>注意事项：</p>
<ol>
<li><strong>输入源</strong>：你需要在运行前确保有一个 socket 服务在 <code>localhost:9999</code> 监听。可以使用 <code>nc -lk 9999</code> 命令在终端开启一个简单的字符发送服务。</li>
<li><strong>依赖</strong>：确保你的项目能找到 Spark 的相关依赖。</li>
</ol>
<p>这段代码创建了一个接收来自 <code>localhost</code> 在 9999 端口的文本流，并且会将接收到的每一行打印到控制台。这种方式可以确保有输出操作被注册，避免抛出上述异常。</p>
<h2 id="socket-vs-kafka"><a class="header" href="#socket-vs-kafka">Socket VS Kafka</a></h2>
<p>版本选型</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250115160549.png" alt="" /></p>
<p>注意：目前spark3.0.0以上版本只有Direct模式。</p>
<h3 id="网络socket数据流处理"><a class="header" href="#网络socket数据流处理">网络（socket）数据流处理</a></h3>
<p>下载<code>netcat</code>工具，执行<code>nc -lp 9999</code>，运行下面的代码：</p>
<pre><code class="language-java">import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

public class SparkStreamingSocket {
    public static void main(String[] args) throws Exception {
        SparkConf conf = new SparkConf();
        conf.setAppName("SparkStreamingEnv01");
        conf.setMaster("local[2]");
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext(conf, new Duration(3 * 1000));

        //通过环境对象对接socket数据源，获取数据模型，进行数据处理
        JavaReceiverInputDStream&lt;String&gt; socketDS = javaStreamingContext.socketTextStream("localhost", 9999);

        socketDS.print();

        javaStreamingContext.start();

        javaStreamingContext.awaitTermination();

    }
}
</code></pre>
<p>输出结果如下所示：</p>
<pre><code>-------------------------------------------
Time: 1736923509000 ms
-------------------------------------------
a
a
a
a
a

-------------------------------------------
Time: 1736923512000 ms
-------------------------------------------
wwww
</code></pre>
<p>调用print()才生成时间戳。</p>
<h3 id="kafka数据流处理"><a class="header" href="#kafka数据流处理">Kafka数据流处理</a></h3>
<p>先启动zookeeper，再启动kafka</p>
<p>使用offset explorer工具</p>
<p>相关代码如下所示：</p>
<pre><code class="language-java">import org.apache.kafka.clients.consumer.ConsumerConfig;  
import org.apache.kafka.clients.consumer.ConsumerRecord;  
import org.apache.spark.api.java.function.Function;  
import org.apache.spark.streaming.Duration;  
import org.apache.spark.streaming.api.java.JavaInputDStream;  
import org.apache.spark.streaming.api.java.JavaStreamingContext;  
import org.apache.spark.streaming.kafka010.ConsumerStrategies;  
import org.apache.spark.streaming.kafka010.KafkaUtils;  
import org.apache.spark.streaming.kafka010.LocationStrategies;  
  
import java.util.ArrayList;  
import java.util.HashMap;  
  
  
public class SparkStreamingKafka {  
    public static void main(String[] args) throws InterruptedException {  
        // 创建流环境  
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext("local[*]", "HelloWorld", Duration.apply(3000));  
  
        // 创建配置参数  
        HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();  
        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "hadoop102:9092,hadoop103:9092,hadoop104:9092");  
        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");  
        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");  
        map.put(ConsumerConfig.GROUP_ID_CONFIG, "atguigu");  
        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");  
  
        // 需要消费的主题  
        ArrayList&lt;String&gt; strings = new ArrayList&lt;&gt;();  
        strings.add("topic_db");  
  
        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.&lt;String, String&gt;Subscribe(strings, map));  
  
        directStream.map(new Function&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() {  
            @Override  
            public String call(ConsumerRecord&lt;String, String&gt; v1) throws Exception {  
                return v1.value();  
            }  
        }).print(100);  
  
        // 执行流的任务  
        javaStreamingContext.start();  
        javaStreamingContext.awaitTermination();  
    }  
}
</code></pre>
<p>更改日志打印级别</p>
<p>如果不希望运行时打印大量日志，可以在resources文件夹中添加<code>log4j2.properties</code>文件，并添加日志配置信息</p>
<pre><code class="language-properties"># Set everything to be logged to the console

rootLogger.level = ERROR

rootLogger.appenderRef.stdout.ref = console

# In the pattern layout configuration below, we specify an explicit `%ex` conversion

# pattern for logging Throwables. If this was omitted, then (by default) Log4J would

# implicitly add an `%xEx` conversion pattern which logs stacktraces with additional

# class packaging information. That extra information can sometimes add a substantial

# performance overhead, so we disable it in our default logging config.

# For more information, see SPARK-39361.

appender.console.type = Console

appender.console.name = console

appender.console.target = SYSTEM_ERR

appender.console.layout.type = PatternLayout

appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n%ex

# Set the default spark-shell/spark-sql log level to WARN. When running the

# spark-shell/spark-sql, the log level for these classes is used to overwrite

# the root logger's log level, so that the user can have different defaults

# for the shell and regular Spark apps.

logger.repl.name = org.apache.spark.repl.Main

logger.repl.level = warn

logger.thriftserver.name = org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver

logger.thriftserver.level = warn

# Settings to quiet third party logs that are too verbose

logger.jetty1.name = org.sparkproject.jetty

logger.jetty1.level = warn

logger.jetty2.name = org.sparkproject.jetty.util.component.AbstractLifeCycle

logger.jetty2.level = error

logger.replexprTyper.name = org.apache.spark.repl.SparkIMain$exprTyper

logger.replexprTyper.level = info

logger.replSparkILoopInterpreter.name = org.apache.spark.repl.SparkILoop$SparkILoopInterpreter

logger.replSparkILoopInterpreter.level = info

logger.parquet1.name = org.apache.parquet

logger.parquet1.level = error

logger.parquet2.name = parquet

logger.parquet2.level = error

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support

logger.RetryingHMSHandler.name = org.apache.hadoop.hive.metastore.RetryingHMSHandler

logger.RetryingHMSHandler.level = fatal

logger.FunctionRegistry.name = org.apache.hadoop.hive.ql.exec.FunctionRegistry

logger.FunctionRegistry.level = error

# For deploying Spark ThriftServer

# SPARK-34128: Suppress undesirable TTransportException warnings involved in THRIFT-4805

appender.console.filter.1.type = RegexFilter

appender.console.filter.1.regex = .*Thrift error occurred during processing of message.*

appender.console.filter.1.onMatch = deny appender.console.filter.1.onMismatch = neutral
</code></pre>
<p>启动生产者生产数据</p>
<pre><code>[atguigu@hadoop102 ~]$ kafka-console-producer.sh --broker-list hadoop102:9092 --topic topicA hello spark
</code></pre>
<p>在IDEA控制台输出如下内容</p>
<pre><code>-------------------------------------------

Time: 1602731772000 ms

-------------------------------------------

hello spark
</code></pre>
<h3 id="案例解析"><a class="header" href="#案例解析">案例解析</a></h3>
<p>DStream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark算子操作后的结果数据流。</p>
<p>在内部实现上，每一批次的数据封装成一个RDD，一系列连续的RDD组成了DStream。对这些RDD的转换是由Spark引擎来计算。</p>
<p>说明：DStream中批次与批次之间计算相互独立。如果批次设置时间小于计算时间会出现计算任务叠加情况，需要多分配资源。通常情况，批次设置时间要大于计算时间。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250115164944.png" alt="" /></p>
<h2 id="dstream转换"><a class="header" href="#dstream转换">DStream转换</a></h2>
<p>DStream上的操作与RDD的类似，分为转换和输出两种，此外转换操作中还有一些比较特殊的原语，如：transform()以及各种Window相关的原语。</p>
<h3 id="无状态转化操作"><a class="header" href="#无状态转化操作">无状态转化操作</a></h3>
<p>无状态转化操作：就是把RDD转化操作应用到DStream每个批次上，每个批次相互独立，自己算自己的。</p>
<p>DStream的部分无状态转化操作列在了下表中，都是DStream自己的API。</p>
<p>注意，只有JavaPairDStream&lt;Key, Value&gt;才能使用xxxByKey()类型的转换算子。</p>
<div class="table-wrapper"><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody>
<tr><td><strong>函数名称</strong></td><td><strong>目的</strong></td><td><strong>函数类型</strong></td></tr>
<tr><td>map()</td><td>对DStream中的每个元素应用给定函数，返回由各元素输出的元素组成的DStream。</td><td>Function&lt;in, out&gt;</td></tr>
<tr><td>flatMap()</td><td>对DStream中的每个元素应用给定函数，返回由各元素输出的迭代器组成的DStream。</td><td>FlatMapFunction&lt;in, out&gt;</td></tr>
<tr><td>filter()</td><td>返回由给定DStream中通过筛选的元素组成的DStream</td><td>Function&lt;in, Boolean&gt;</td></tr>
<tr><td>mapToPair()</td><td>改变DStream的分区数</td><td>PairFunction&lt;in, key, value&gt;</td></tr>
<tr><td>reduceByKey()</td><td>将每个批次中键相同的记录规约。</td><td>Function2&lt;in, in, in&gt;</td></tr>
<tr><td>groupByKey()</td><td>将每个批次中的记录根据键分组。</td><td>ds.groupByKey()</td></tr>
</tbody></table>
</div>
<p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个DStream在内部是由许多RDD批次组成，且无状态转化操作是分别应用到每个RDD(一个批次的数据)上的。</p>
<h3 id="dstream和rdd"><a class="header" href="#dstream和rdd">DStream和RDD</a></h3>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.streaming.Duration;  
import org.apache.spark.streaming.api.java.JavaDStream;  
import org.apache.spark.streaming.api.java.JavaPairDStream;  
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;  
import org.apache.spark.streaming.api.java.JavaStreamingContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
  
public class SparkStreamingFunction {  
    public static void main(String[] args) throws Exception {  
        SparkConf conf = new SparkConf();  
        conf.setAppName("SparkStreamingEnv01");  
        conf.setMaster("local[2]");  
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext(conf, new Duration(3 * 1000));  
  
        //通过环境对象对接socket数据源，获取数据模型，进行数据处理  
        JavaReceiverInputDStream&lt;String&gt; socketDS = javaStreamingContext.socketTextStream("localhost", 9999);  
        JavaDStream&lt;String&gt; flatDS = socketDS.flatMap(  
                line -&gt; Arrays.asList(line.split(" ")).iterator()  
        );  
  
        JavaPairDStream&lt;String, Integer&gt; wordDS = flatDS.mapToPair(  
                word -&gt; new Tuple2&lt;&gt;(word, 1)  
        );  
  
        JavaPairDStream&lt;String, Integer&gt; wordCountDS = wordDS.reduceByKey(Integer::sum);  
  
        //DStream确实就是对RDD的封装，但是不是所有的方法都进行了分装。有些方法不能使用：sortBy，sortByKey  
        //如果特定场合下，就需要使用这些方法，那么就需要将DStream转换为RDD使用  
  
        //wordCountDS.print();  
        wordCountDS.foreachRDD(  
            rdd -&gt; {  
                rdd.sortByKey().collect().forEach(System.out::println);  
            }  
        );  
  
        javaStreamingContext.start();  
  
        javaStreamingContext.awaitTermination();  
  
    }  
}
</code></pre>
<h2 id="窗口操作"><a class="header" href="#窗口操作">窗口操作</a></h2>
<p>生产环境中，窗口操作主要应用于这样的需求：最近N时间，每个M时间的数据变化</p>
<p>需求：最近1个小时，每10分钟，气温的变化趋势</p>
<h3 id="滑动窗口及窗口参数"><a class="header" href="#滑动窗口及窗口参数">滑动窗口及窗口参数</a></h3>
<p>窗口可以移动的称之为移动窗口，但是窗口移动是有幅度的，默认移动幅度就是采集周期</p>
<p>数据窗口范围扩大（6s），但是窗口移动幅度不变（3s），数据可能会有重复</p>
<p>数据窗口范围和窗口移动幅度一致（3s），数据不会有重复</p>
<p>窗口：其实就是数据的范围（时间）</p>
<p>window方法可以改变窗口的数据范围（默认数据范围为采集周期） window方法可以传递2个参数</p>
<p>第一个参数表示窗口的数据范围（时间）</p>
<p>第二个参数表示窗口的移动幅度（时间），可以不用传递，默认使用的就是采集周期</p>
<p>SparkStreaming在窗口移动时计算结果。</p>
<p>执行<code>nc -lp 9999</code></p>
<pre><code class="language-java">import org.apache.spark.SparkConf;  
import org.apache.spark.streaming.Duration;  
import org.apache.spark.streaming.api.java.JavaDStream;  
import org.apache.spark.streaming.api.java.JavaPairDStream;  
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;  
import org.apache.spark.streaming.api.java.JavaStreamingContext;  
import scala.Tuple2;  
  
import java.util.Arrays;  
  
public class SparkStreamingWindows {  
    public static void main(String[] args) throws Exception {  
        SparkConf conf = new SparkConf();  
        conf.setAppName("SparkStreamingEnv01");  
        conf.setMaster("local[2]");  
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext(conf, new Duration(3 * 1000));  
  
        //通过环境对象对接socket数据源，获取数据模型，进行数据处理  
        JavaReceiverInputDStream&lt;String&gt; socketDS = javaStreamingContext.socketTextStream("localhost", 9999);  
        JavaDStream&lt;String&gt; flatDS = socketDS.flatMap(  
                line -&gt; Arrays.asList(line.split(" ")).iterator()  
        );  
  
        JavaPairDStream&lt;String, Integer&gt; wordDS = flatDS.mapToPair(  
                word -&gt; new Tuple2&lt;&gt;(word, 1)  
        );  
  
        JavaPairDStream&lt;String, Integer&gt; windowDS = wordDS.window(new Duration(6 * 1000), new Duration(6 * 1000));  
  
        JavaPairDStream&lt;String, Integer&gt; wordCountDS = windowDS.reduceByKey(Integer::sum);  
  
        wordCountDS.print();  
//        wordCountDS.foreachRDD(  
//                rdd -&gt; {  
//                    rdd.sortByKey().collect().forEach(System.out::println);  
//                }  
//        );  
  
        javaStreamingContext.start();  
  
        javaStreamingContext.awaitTermination();  
  
    }  
}
</code></pre>
<h3 id="windowoperations"><a class="header" href="#windowoperations">WindowOperations</a></h3>
<p>Window Operations可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Streaming的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。</p>
<p>窗口时长：计算内容的时间范围；</p>
<p>滑动步长：隔多久触发一次计算。</p>
<p>注意：这两者都必须为采集批次大小的整数倍。</p>
<p>如下图所示WordCount案例：窗口大小为批次的2倍，滑动步等于批次大小。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250115160633.png" alt="" /></p>
<h3 id="window"><a class="header" href="#window">Window</a></h3>
<p>1）基本语法：</p>
<p>window(windowLength, slideInterval): 基于对源DStream窗口的批次进行计算返回一个新的DStream。</p>
<p>2）需求：</p>
<p>统计WordCount：3秒一个批次，窗口12秒，滑步6秒。</p>
<pre><code class="language-java">import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;

public class Test02_Window {
    public static void main(String[] args) throws InterruptedException {
        // 创建流环境
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext("local[*]", "window", Duration.apply(3000));
        // 创建配置参数
        HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();
        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"hadoop102:9092,hadoop103:9092,hadoop104:9092");
        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.GROUP_ID_CONFIG,"atguigu");
        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"latest");

        // 需要消费的主题
        ArrayList&lt;String&gt; strings = new ArrayList&lt;&gt;();
        strings.add("topicA");

        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.&lt;String, String&gt;Subscribe(strings,map));

        JavaDStream&lt;String&gt; stringJavaDStream = directStream.flatMap(new FlatMapFunction&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() {
            @Override
            public Iterator&lt;String&gt; call(ConsumerRecord&lt;String, String&gt; stringStringConsumerRecord) throws Exception {
                String[] split = stringStringConsumerRecord.value().split(" ");
                return Arrays.asList(split).iterator();
            }
        });

        JavaPairDStream&lt;String, Integer&gt; javaPairDStream = stringJavaDStream.mapToPair(new PairFunction&lt;String, String, Integer&gt;() {
            @Override
            public Tuple2&lt;String, Integer&gt; call(String s) throws Exception {
                return new Tuple2&lt;&gt;(s, 1);
            }
        });

        JavaPairDStream&lt;String, Integer&gt; window = javaPairDStream.window(Duration.apply(12000), Duration.apply(6000));
        window.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() {
            @Override
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1+v2;
            }
        }).print();

        // 执行流的任务
        javaStreamingContext.start();
        javaStreamingContext.awaitTermination();
    }
}
</code></pre>
<p>4）测试</p>
<pre><code>[atguigu@hadoop102 ~]$ kafka-console-producer.sh --broker-list hadoop102:9092 --topic topicA hello spark
</code></pre>
<p>5）如果有多批数据进入窗口，最终也会通过window操作变成统一的RDD处理。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost04/img20250115164720.png" alt="" /></p>
<h3 id="reducebykeyandwindow"><a class="header" href="#reducebykeyandwindow">reduceByKeyAndWindow</a></h3>
<p>1）基本语法</p>
<p><code>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</code>：当在一个(K,V)对的DStream上调用此函数，会返回一个新(K,V)对的DStream，此处通过对滑动窗口中批次数据使用reduce函数来整合每个key的value值。</p>
<p>2）需求：</p>
<p>统计WordCount：3秒一个批次，窗口12秒，滑步6秒。</p>
<p>3）代码编写：</p>
<pre><code class="language-java">import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;

public class Test03_ReduceByKeyAndWindow {
    public static void main(String[] args) throws InterruptedException {
        // 创建流环境
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext("local[*]", "window", Duration.apply(3000L));
        // 创建配置参数
        HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();
        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"hadoop102:9092,hadoop103:9092,hadoop104:9092");
        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.GROUP_ID_CONFIG,"atguigu");
        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"latest");

        // 需要消费的主题
        ArrayList&lt;String&gt; strings = new ArrayList&lt;&gt;();
        strings.add("topicA");

        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.&lt;String, String&gt;Subscribe(strings,map));

        JavaDStream&lt;String&gt; stringJavaDStream = directStream.flatMap(new FlatMapFunction&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() {
            @Override
            public Iterator&lt;String&gt; call(ConsumerRecord&lt;String, String&gt; stringStringConsumerRecord) throws Exception {
                String[] split = stringStringConsumerRecord.value().split(" ");
                return Arrays.asList(split).iterator();
            }
        });

        JavaPairDStream&lt;String, Integer&gt; javaPairDStream = stringJavaDStream.mapToPair(new PairFunction&lt;String, String, Integer&gt;() {
            @Override
            public Tuple2&lt;String, Integer&gt; call(String s) throws Exception {
                return new Tuple2&lt;&gt;(s, 1);
            }
        });

        javaPairDStream.reduceByKeyAndWindow(new Function2&lt;Integer, Integer, Integer&gt;() {
            @Override
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1 + v2;
            }
        },Duration.apply(12000L),Duration.apply(6000L)).print();

        // 执行流的任务
        javaStreamingContext.start();
        javaStreamingContext.awaitTermination();

    }
}
</code></pre>
<p>2）测试</p>
<pre><code>[atguigu@hadoop102 ~]$ kafka-console-producer.sh --broker-list hadoop102:9092 --topic topicA hello spark
</code></pre>
<h2 id="dstream输出"><a class="header" href="#dstream输出">DStream输出</a></h2>
<p>DStream通常将数据输出到外部数据库或屏幕上。</p>
<p>DStream与RDD中的惰性求值类似，如果一个DStream及其派生出的DStream都没有被执行输出操作，那么这些DStream就都不会被求值。如果StreamingContext中没有设定输出操作，整个Context就都不会启动。</p>
<p>1）输出操作API如下：</p>
<p><code>saveAsTextFiles(prefix, [suffix])</code>：以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。“<code>prefix-Time_IN_MS[.suffix]</code>”。</p>
<p>注意：<strong>以上操作都是每一批次写出一次，会产生大量小文件，在生产环境，很少使用。</strong></p>
<p><code>print()</code>：在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。</p>
<p><code>foreachRDD(func)</code>：这是最通用的输出操作，即将函数func用于产生DStream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者写入数据库。</p>
<p>在企业开发中通常采用<code>foreachRDD()</code>，它用来对DStream中的RDD进行任意计算。这和transform()有些类似，都可以让我们访问任意RDD。在foreachRDD()中，可以重用我们在Spark中实现的所有行动操作(action算子)。比如，常见的用例之一是把数据写到如MySQL的外部数据库中。</p>
<pre><code class="language-java">import org.apache.kafka.clients.consumer.ConsumerConfig;

import org.apache.kafka.clients.consumer.ConsumerRecord;

import org.apache.spark.api.java.JavaPairRDD;

import org.apache.spark.api.java.function.FlatMapFunction;

import org.apache.spark.api.java.function.Function2;

import org.apache.spark.api.java.function.PairFunction;

import org.apache.spark.api.java.function.VoidFunction;

import org.apache.spark.streaming.Duration;

import org.apache.spark.streaming.api.java.JavaDStream;

import org.apache.spark.streaming.api.java.JavaInputDStream;

import org.apache.spark.streaming.api.java.JavaPairDStream;

import org.apache.spark.streaming.api.java.JavaStreamingContext;

import org.apache.spark.streaming.kafka010.ConsumerStrategies;

import org.apache.spark.streaming.kafka010.KafkaUtils;

import org.apache.spark.streaming.kafka010.LocationStrategies;

import scala.Tuple2;

import java.util.ArrayList;

import java.util.Arrays;

import java.util.HashMap;

import java.util.Iterator;

/**

 * @author yhm

 * @create 2022-09-01 16:47

 */

public class Test04_Save {

    public static void main(String[] args) throws InterruptedException {

        // 创建流环境

        JavaStreamingContext javaStreamingContext = new JavaStreamingContext("local[*]", "window", Duration.apply(3000L));

        // 创建配置参数

        HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();

        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"hadoop102:9092,hadoop103:9092,hadoop104:9092");

        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");

        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");

        map.put(ConsumerConfig.GROUP_ID_CONFIG,"atguigu");

        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"latest");

        // 需要消费的主题

        ArrayList&lt;String&gt; strings = new ArrayList&lt;&gt;();

        strings.add("topicA");

        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.&lt;String, String&gt;Subscribe(strings,map));

        JavaDStream&lt;String&gt; stringJavaDStream = directStream.flatMap(new FlatMapFunction&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() {

            @Override

            public Iterator&lt;String&gt; call(ConsumerRecord&lt;String, String&gt; stringStringConsumerRecord) throws Exception {

                String[] split = stringStringConsumerRecord.value().split(" ");

                return Arrays.asList(split).iterator();

            }

        });

        JavaPairDStream&lt;String, Integer&gt; javaPairDStream = stringJavaDStream.mapToPair(new PairFunction&lt;String, String, Integer&gt;() {

            @Override

            public Tuple2&lt;String, Integer&gt; call(String s) throws Exception {

                return new Tuple2&lt;&gt;(s, 1);

            }

        });

        JavaPairDStream&lt;String, Integer&gt; resultDStream = javaPairDStream.reduceByKeyAndWindow(new Function2&lt;Integer, Integer, Integer&gt;() {

            @Override

            public Integer call(Integer v1, Integer v2) throws Exception {

                return v1 + v2;

            }

        }, Duration.apply(12000L), Duration.apply(6000L));

        resultDStream.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String, Integer&gt;&gt;() {

            @Override

            public void call(JavaPairRDD&lt;String, Integer&gt; stringIntegerJavaPairRDD) throws Exception {

                // 获取mysql连接

                // 写入到mysql中

                // 关闭连接

            }

        });

        // 执行流的任务

        javaStreamingContext.start();

        javaStreamingContext.awaitTermination();

    }

}
</code></pre>
<h2 id="关闭main线程"><a class="header" href="#关闭main线程">关闭main线程</a></h2>
<h3 id="新增线程方式"><a class="header" href="#新增线程方式">新增线程方式</a></h3>
<pre><code class="language-java">import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

public class SparkStreamingClose {
    public static void main(String[] args) throws Exception {
        SparkConf conf = new SparkConf();
        conf.setAppName("SparkStreamingEnv01");
        conf.setMaster("local[2]");
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext(conf, new Duration(3 * 1000));

        //通过环境对象对接socket数据源，获取数据模型，进行数据处理
        JavaReceiverInputDStream&lt;String&gt; socketDS = javaStreamingContext.socketTextStream("localhost", 9999);

        socketDS.print();

        javaStreamingContext.start();

        //close方法就是用于释放资源，关闭环境，但不能在当前main方法中调用，需要在另外一个线程中调用，否则会导致程序卡死
        new Thread(new Runnable() {
            @Override
            public void run() {
                try {
                    Thread.sleep(3 * 1000);
                    javaStreamingContext.close();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        }).start();

        javaStreamingContext.awaitTermination();

    }
}
</code></pre>
<p>这种方式会抛出异常。</p>
<pre><code>-------------------------------------------
Time: 1736931924000 ms
-------------------------------------------

Exception in thread "receiver-supervisor-future-0" java.lang.Error: java.lang.InterruptedException: sleep interrupted
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
</code></pre>
<h3 id="较为优雅的方式stop"><a class="header" href="#较为优雅的方式stop">较为优雅的方式（stop）</a></h3>
<p><code>javaStreamingContext.stop(true, true);</code></p>
<pre><code class="language-java">package com.zzw.bigdata.spark.sparkstreaming;

import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

public class SparkStreamingClose {
    public static void main(String[] args) throws Exception {
        SparkConf conf = new SparkConf();
        conf.setAppName("SparkStreamingEnv01");
        conf.setMaster("local[2]");
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext(conf, new Duration(3 * 1000));

        //通过环境对象对接socket数据源，获取数据模型，进行数据处理
        JavaReceiverInputDStream&lt;String&gt; socketDS = javaStreamingContext.socketTextStream("localhost", 9999);

        socketDS.print();

        javaStreamingContext.start();

        //close方法就是用于释放资源，关闭环境，但不能在当前main方法中调用，需要在另外一个线程中调用，否则会导致程序卡死
        new Thread(new Runnable() {
            @Override
            public void run() {
                try {
                    Thread.sleep(3 * 1000);
                    javaStreamingContext.stop(true, true);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        }).start();

        javaStreamingContext.awaitTermination();

    }
}

</code></pre>
<p>输出结果如下所示：</p>
<pre><code>-------------------------------------------
Time: 1736932401000 ms
-------------------------------------------

Exception in thread "receiver-supervisor-future-0" java.lang.Error: java.lang.InterruptedException: sleep interrupted
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
-------------------------------------------
Time: 1736932404000 ms
-------------------------------------------

-------------------------------------------
Time: 1736932407000 ms
-------------------------------------------
</code></pre>
<h3 id="使用kafka时优雅关闭"><a class="header" href="#使用kafka时优雅关闭">使用kafka时优雅关闭</a></h3>
<p>流式任务需要7*24小时执行，但是有时涉及到升级代码需要主动停止程序，但是分布式程序没办法做到一个个进程去杀死，所以配置优雅的关闭就显得至关重要了。</p>
<p>关闭方式：使用外部文件系统来控制内部程序关闭。</p>
<p>1）主程序</p>
<pre><code class="language-java">import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.StreamingContextState;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import scala.Tuple2;

import java.net.URI;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;

public class Test05_Close {
    public static void main(String[] args) throws InterruptedException {
        // 创建流环境
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext("local[*]", "window", Duration.apply(3000L));
        // 创建配置参数
        HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();
        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"hadoop102:9092,hadoop103:9092,hadoop104:9092");
        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.GROUP_ID_CONFIG,"atguigu");
        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"latest");

        // 需要消费的主题
        ArrayList&lt;String&gt; strings = new ArrayList&lt;&gt;();
        strings.add("topicA");

        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.&lt;String, String&gt;Subscribe(strings,map));

        JavaDStream&lt;String&gt; stringJavaDStream = directStream.flatMap(new FlatMapFunction&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() {
            @Override
            public Iterator&lt;String&gt; call(ConsumerRecord&lt;String, String&gt; stringStringConsumerRecord) throws Exception {
                String[] split = stringStringConsumerRecord.value().split(" ");
                return Arrays.asList(split).iterator();
            }
        });

        JavaPairDStream&lt;String, Integer&gt; javaPairDStream = stringJavaDStream.mapToPair(new PairFunction&lt;String, String, Integer&gt;() {
            @Override
            public Tuple2&lt;String, Integer&gt; call(String s) throws Exception {
                return new Tuple2&lt;&gt;(s, 1);
            }
        });

        javaPairDStream.reduceByKeyAndWindow(new Function2&lt;Integer, Integer, Integer&gt;() {
            @Override
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1 + v2;
            }
        },Duration.apply(12000L),Duration.apply(6000L)).print();

        // 开启监控程序
        new Thread(new MonitorStop(javaStreamingContext)).start();

        // 执行流的任务
        javaStreamingContext.start();
        javaStreamingContext.awaitTermination();

    }

    public static class MonitorStop implements Runnable {

        JavaStreamingContext javaStreamingContext = null;

        public MonitorStop(JavaStreamingContext javaStreamingContext) {
            this.javaStreamingContext = javaStreamingContext;
        }

        @Override
        public void run() {
            try {
                FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), new Configuration(), "atguigu");
                while (true){
                    Thread.sleep(5000);
                    boolean exists = fs.exists(new Path("hdfs://hadoop102:8020/stopSpark"));
                    if (exists){
                        StreamingContextState state = javaStreamingContext.getState();
                        // 获取当前任务是否正在运行
                        if (state == StreamingContextState.ACTIVE){
                            // 优雅关闭
                            javaStreamingContext.stop(true, true);
                            System.exit(0);
                        }
                    }
                }
            }catch (Exception e){
                e.printStackTrace();
            }
        }
    }
}

</code></pre>
<p>2）测试</p>
<p>（1）发送数据</p>
<pre><code>[atguigu@hadoop102 ~]$ kafka-console-producer.sh --broker-list hadoop102:9092 --topic topicA hello spark
</code></pre>
<p>（2）启动Hadoop集群</p>
<pre><code>[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh

[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /stopSpark
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
